ci_presets:
  # list of names of presets to apply, or a single name, or null if not preset
  to_apply: []

  # dict of variables to apply
  variable_overrides: {}

  # list of names of presets that have been applied
  names: []

  local_config: null # defined locally in variable_overrides.yaml

  mac4:
    secrets.hostname: mac_ai__hostname.mac4
    secrets.username: mac_ai__username.mac4
    secrets.base_work_dir: mac_ai__base_work_dir.mac4

  mac5:
    secrets.hostname: mac_ai__hostname.mac5
    secrets.username: mac_ai__username.mac5
    secrets.base_work_dir: mac_ai__base_work_dir.mac5

  mac-m4:
    secrets.hostname: mac_ai__hostname.mac-m4
    secrets.username: mac_ai__username.mac-m4
    secrets.base_work_dir: mac_ai__base_work_dir.mac-m4

  # ---

secrets:
  dir:
    name: crc-mac-ai-secret
    env_key: CRC_MAC_AI_SECRET_PATH
  private_key_path: mac_ai__private_key
  hostname: mac_ai__hostname.mac-m4
  username: mac_ai__username.mac-m4
  base_work_dir: mac_ai__base_work_dir.mac-m4

remote_host:
  run_locally: false
  private_key_filename: "@secrets.private_key_path" # in the secret dir
  hostname: "*$@secrets.hostname"
  username: "*$@secrets.username"
  port: 22
  base_work_dir: "*$@secrets.base_work_dir"
  ssh_flags:
  - -oStrictHostKeyChecking=no
  - -oUserKnownHostsFile=/dev/null
  - -o LogLevel=ERROR
  system: darwin
  arch: arm64
  python_bin: python3
  podman_bin: podman # only used if not prepare.podman.repo.enabled
  env:
    PATH: "/opt/homebrew/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin"
  home_is_base_work_dir: true
  verbose_ssh_commands: true

prepare:
  # the list of platform environments to prepare
  cleanup_on_exit: false
  prepare_only_inference_server: false

  llm_load_test:
    install_requirements: false

  ramalama:
    repo:
      url: https://github.com/containers/ramalama
      version: v0.7.5

  brew:
    install_dependencies: false
    capture_dependencies: false
    _libomp:
      location: /opt/homebrew/Cellar/libomp/
      version: "20.1.0"
    dependencies:
    # brew tap slp/krunkit
    - krunkit
    # brew link --overwrite molten-vk-krunkit
    - molten-vk-krunkit

    - vulkan-headers
    - libkrun-efi
    - virglrenderer
    - molten-vk
    - cmake
    - shaderc
    - libomp
    - glslang

  podman:
    repo:
      enabled: true
      url: https://github.com/containers/podman
      version: v5.4.0
      darwin:
        file: podman-remote-release-{@remote_host.system}_{@remote_host.arch}.zip
        zip: true
    gvisor:
      repo:
        enabled: true
        url: https://github.com/containers/gvisor-tap-vsock/
        version: v0.8.3
        file: "gvproxy-{@remote_host.system}"

    container:
      name: topsail_mac_ai
      python_bin: python3
      system: linux
      device: /dev/dri

    stop_on_exit: true

    machine:
      enabled: true
      force_configuration: false
      set_default: true
      name: podman-machine-default
      configuration:
        cpus: 4 # in cores
        memory: 5120 # in MiB
      env:
        CONTAINERS_MACHINE_PROVIDER: libkrun
        CONTAINERS_HELPER_BINARY_DIR: /opt/homebrew/bin/

test:
  platform: podman/ramalama

  platforms_to_skip: null

  inference_server:
    port: 11434
    unload_on_exit: true

    benchmark:
      enabled: true

  model:
    name: llama3.2
    size: small

  llm_load_test:
    enabled: true

    args:
      host: localhost
      port: "@test.inference_server.port"
      duration: 60
      concurrency: 1
      plugin: openai_plugin
      interface: http
      streaming: true
      endpoint: "/v1/chat/completions"

    dataset_sizes:
      small:
        max_input_tokens: 2047
        max_output_tokens: 1024
        max_sequence_tokens: 2048
      large:
        max_input_tokens: 3500
        max_output_tokens: 800
        max_sequence_tokens: 3500
  matbenchmarking:
    enabled: false
    stop_on_error: true
    fields:
    - prepare.ramalama.repo.version
    - test.model.name
    - test.llm_load_test.args
  capture_metrics:
    enabled: true
    gpu:
      # needs this in visudo:
      # $USER ALL = (root) NOPASSWD: /usr/bin/pkill powermetrics
      # $USER ALL = (root) NOPASSWD: /usr/bin/powermetrics *
      enabled: true
      sampler: gpu_power
      rate: 1000
    virtgpu:
      enabled: true

cleanup:
  models:
    ramalama: true

  files:
    ramalama: true
    llm-load-test: true
    podman: true

  podman_machine:
    delete: true
    reset: true

export_artifacts:
  enabled: false

matbench:
  enabled: true
  preset: null
  workload: projects.ramalama.visualizations.llm_load_test
  config_file: plots.yaml
  download:
    mode: prefer_cache
    url:
    url_file:
    # if true, copy the results downloaded by `matbench download` into the artifacts directory
    save_to_artifacts: false
  # directory to plot. Set by topsail/testing/visualize.py before launching the visualization
  test_directory: null
  lts:
    generate: false

exec_list:
  _only_: false

  pre_cleanup_ci: null
  prepare_ci: null
  test_ci: null
  post_cleanup_ci: null
  matbench_run: true
  generate_plots_from_pr_args: true
