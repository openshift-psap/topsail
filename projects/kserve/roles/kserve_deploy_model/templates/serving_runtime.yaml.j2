apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: {{ kserve_deploy_model_serving_runtime_name }}
  namespace: {{ kserve_deploy_model_namespace }}
  labels:
    opendatahub.io/dashboard: "true"
  annotations:
    opendatahub.io/template-display-name: "ServingRuntime for {{ kserve_deploy_model_serving_runtime_name }} | Topsail"
spec:
  containers:
{% if kserve_deploy_model_sr_container_flavor == "tgis+caikit" %} # tgis+caikit container flavor
  - name: kserve-container
    image: {{ kserve_deploy_model_sr_kserve_image }}
    command: [bash, -cex]
    args:
    - |
      echo 'Starting kserver (TGIS) {% if kserve_deploy_model_sr_mute_logs %} without {% else %} _with_ {% endif %} stdout logs ...'
      exec text-generation-launcher --model-name=/mnt/models/artifacts/ {% if kserve_deploy_model_sr_mute_logs %} > /tmp/tgis-logs {% endif %}

    env:
    - name: TRANSFORMERS_CACHE
      value: /shared_model_storage/transformers_cache
{% for env_key, env_value in kserve_deploy_model_sr_kserve_extra_env_values.items() %}
    - name: "{{ env_key }}"
      value: "{{ env_value }}"
{% endfor %}

{% if kserve_deploy_model_secret_env_file_name is not none %}
    envFrom:
    - secretRef:
        name: {{ kserve_deploy_model_serving_runtime_name }}-secret
{% endif %}

    resources:
      requests:
        cpu: "{{ kserve_deploy_model_sr_kserve_resource_request.cpu }}"
        memory: "{{ kserve_deploy_model_sr_kserve_resource_request.memory }}Gi"
{% if kserve_deploy_model_sr_kserve_resource_request['nvidia.com/gpu'] %}
        nvidia.com/gpu: "{{ kserve_deploy_model_sr_kserve_resource_request['nvidia.com/gpu'] }}"
{% endif %}
{% if kserve_deploy_model_limits_equals_requests or kserve_deploy_model_sr_kserve_resource_request['nvidia.com/gpu'] %}
      limits:
{% if kserve_deploy_model_limits_equals_requests %}
        cpu: "{{ kserve_deploy_model_sr_kserve_resource_request.cpu }}"
        memory: "{{ kserve_deploy_model_sr_kserve_resource_request.memory }}Gi"
{% endif %}
{% if kserve_deploy_model_sr_kserve_resource_request['nvidia.com/gpu'] %}
        nvidia.com/gpu: "{{ kserve_deploy_model_sr_kserve_resource_request['nvidia.com/gpu'] }}"
{% endif %}
{% endif %}
    volumeMounts:
    - mountPath: /shared_model_storage/transformers_cache
      name: cache-volume
  # ---
  # --- transformer-container
  # ---
  - name: transformer-container
    image: {{ kserve_deploy_model_sr_transformer_image }}
    ports:
    - containerPort: 8085
      name: h2c
      protocol: TCP
    env:
    - name: TRANSFORMERS_CACHE
      value: /shared_model_storage/transformers_cache
{% for env_key, env_value in kserve_deploy_model_sr_transformer_extra_env_values.items() %}
    - name: "{{ env_key }}"
      value: "{{ env_value }}"
{% endfor %}
{% for env_key, env_value in kserve_deploy_model_sr_kserve_extra_env_values.items() %}
    - name: "{{ env_key }}"
      value: "{{ env_value }}"
{% endfor %}
{% if kserve_deploy_model_secret_env_file_name is not none %}
    envFrom:
    - secretRef:
        name: {{ kserve_deploy_model_serving_runtime_name }}-secret
{% endif %}
    resources:
      requests:
        cpu: "{{ kserve_deploy_model_sr_transformer_resource_request.cpu }}"
        memory: "{{ kserve_deploy_model_sr_transformer_resource_request.memory }}Gi"
{% if kserve_deploy_model_limits_equals_requests %}
      limits:
        cpu: "{{ kserve_deploy_model_sr_transformer_resource_request.cpu }}"
        memory: "{{ kserve_deploy_model_sr_transformer_resource_request.memory }}Gi"
{% endif %}
    volumeMounts:
    - name: config
      mountPath: /caikit/config/
      readOnly: true
{% if kserve_deploy_model_sr_shared_memory %}
    - name: shared-memory
      mountPath: /dev/shm
{% endif %}
{% elif kserve_deploy_model_sr_container_flavor == "tgis" %} # tgis container flavor
  - name: kserve-container
    image: {{ kserve_deploy_model_sr_kserve_image }}
    command: ["text-generation-launcher"]
    args:
      - --model-name=/mnt/models/
      - --port=3000
      - --grpc-port=8033
    env:
{% for env_key, env_value in kserve_deploy_model_sr_kserve_extra_env_values.items() %}
    - name: "{{ env_key }}"
      value: "{{ env_value }}"
{% endfor %}
{% for env_key, env_value in kserve_deploy_model_sr_transformer_extra_env_values.items() %}
    - name: "{{ env_key }}"
      value: "{{ env_value }}"
{% endfor %}

{% if kserve_deploy_model_secret_env_file_name is not none %}
    envFrom:
    - secretRef:
        name: {{ kserve_deploy_model_serving_runtime_name }}-secret
{% endif %}

    resources:
      requests:
        cpu: "{{ kserve_deploy_model_sr_kserve_resource_request.cpu }}"
        memory: "{{ kserve_deploy_model_sr_kserve_resource_request.memory }}Gi"
{% if kserve_deploy_model_sr_kserve_resource_request['nvidia.com/gpu'] %}
        nvidia.com/gpu: "{{ kserve_deploy_model_sr_kserve_resource_request['nvidia.com/gpu'] }}"
{% endif %}
{% if kserve_deploy_model_limits_equals_requests or kserve_deploy_model_sr_kserve_resource_request['nvidia.com/gpu'] %}
      limits:
{% if kserve_deploy_model_limits_equals_requests %}
        cpu: "{{ kserve_deploy_model_sr_kserve_resource_request.cpu }}"
        memory: "{{ kserve_deploy_model_sr_kserve_resource_request.memory }}Gi"
{% endif %}
{% if kserve_deploy_model_sr_kserve_resource_request['nvidia.com/gpu'] %}
        nvidia.com/gpu: "{{ kserve_deploy_model_sr_kserve_resource_request['nvidia.com/gpu'] }}"
{% endif %}
{% endif %}
    readinessProbe:
{% if kserve_deploy_model_raw_deployment %}
      httpGet:
        path: /health
        port: 3000
{% else %}
      exec: # Use exec probes instad of httpGet since the probes' port gets rewritten to the containerPort
        command: [curl, --silent, localhost:3000/health]
{% endif %}
    livenessProbe:
{% if kserve_deploy_model_raw_deployment %}
      exec:
        command: [echo, "alive"]
{% else %}
      exec:
        command: [curl, --silent, localhost:3000/health]
{% endif %}
      initialDelaySeconds: 30
    ports:
    - containerPort: 8033
      protocol: TCP

    volumeMounts:
    - name: cache-volume
      mountPath: /shared_model_storage/transformers_cache
{% if kserve_deploy_model_sr_shared_memory %}
    - name: shared-memory
      mountPath: /dev/shm
{% endif %}
{% else %}
{{ "Invalid value for 'kserve_deploy_model_sr_container_flavor'..."/0 }}
{% endif %}
  volumes:
  - name:  cache-volume
    emptyDir:
      sizeLimit: 180Gi
{% if kserve_deploy_model_sr_container_flavor == "tgis+caikit" %} # tgis+caikit container flavor
  - name: config
    configMap:
      name: {{ kserve_deploy_model_serving_runtime_name }}-caikit-tgis-config
{% endif %}
{% if kserve_deploy_model_sr_shared_memory %}
  - name: shared-memory
    emptyDir:
      medium: Memory
      sizeLimit: 2Gi
{% endif %}
  multiModel: false
  supportedModelFormats:
  - autoSelect: true
    name: {{ kserve_deploy_model_inference_service_model_format }}
