# Auto-generated file, do not edit manually ...
# Toolbox generate command: repo generate_ansible_default_settings
# Source component: Kserve.deploy_model

# Parameters
# the namespace in which the model should be deployed
# Mandatory value
kserve_deploy_model_namespace:

# the name to give to the serving runtime
# Mandatory value
kserve_deploy_model_serving_runtime_name:

# the image of the Kserve serving runtime container
# Mandatory value
kserve_deploy_model_sr_kserve_image:

# the resource request of the kserve serving runtime container
# Mandatory value
kserve_deploy_model_sr_kserve_resource_request:

# the image of the Transformer serving runtime container
# Mandatory value
kserve_deploy_model_sr_transformer_image:

# the resource request of the Transformer serving runtime container
# Mandatory value
kserve_deploy_model_sr_transformer_resource_request:

# the name to give to the inference service
# Mandatory value
kserve_deploy_model_inference_service_name:

# [S3] URI where the model is stored
# Mandatory value
kserve_deploy_model_storage_uri:

# name of the service account to use for running the Pod
# Mandatory value
kserve_deploy_model_sa_name:

# extra key/value pairs for the kserve container (will override the values from the secret file)
kserve_deploy_model_sr_kserve_extra_env_values: {}

# extra key/value pairs for the transformer container (will override the values from the secret file)
kserve_deploy_model_sr_transformer_extra_env_values: {}

# if True, deploy 1 container. If False, deploy 2 containers.
kserve_deploy_model_sr_merge_containers: false

# the minimum number of replicas. If none, the field is left unset.
# Type: Int
kserve_deploy_model_inference_service_min_replicas: null

# name of the YAML file containing the secret environment key/values
kserve_deploy_model_secret_env_file_name: null

# key to the secret environment key/values in the secret file
kserve_deploy_model_secret_env_file_key: null

# if True, mute the serving runtime containers logs
kserve_deploy_model_sr_mute_logs: false

# if True, deletes the other serving runtime/inference services of the namespace
kserve_deploy_model_delete_others: true

# if True, sets use the requests values to define the limits. If False, do not define any limits (except for GPU)
kserve_deploy_model_limits_equals_requests: true
