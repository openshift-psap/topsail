---
- name: Create the src directory
  file:
    path: "{{ artifact_extra_logs_dir }}/src"
    state: directory
    mode: '0755'

- name: Create the artifacts directory
  file:
    path: "{{ artifact_extra_logs_dir }}/artifacts"
    state: directory
    mode: '0755'

# Cleanup the namespace
- name: Delete the InferenceServices
  command: oc delete InferenceServices,ServingRuntime --all -n {{ kserve_deploy_model_namespace }}
  when: kserve_deploy_model_delete_others | bool

- name: Wait for the Pods to disappear
  command:
    oc get pods
       --no-headers
       -lcomponent=predictor
       -n {{ kserve_deploy_model_namespace }}
  register: ns_had_predictor_pods_cmd
  retries: 12
  delay: 10
  until: '"No resources found" in ns_had_predictor_pods_cmd.stderr'
  when: kserve_deploy_model_delete_others | bool

# SMMR

- name: Delete the tracking timestamps
  command:
    oc delete cm -ltopsail.time-tracking -n {{ kserve_deploy_model_namespace }}

- name: Save timestamp
  shell: |
    NAME=start-deploy-model
    oc create configmap $NAME -n {{ kserve_deploy_model_namespace }}
    oc label cm/$NAME topsail.time-tracking=yes -n {{ kserve_deploy_model_namespace }}

# Secret

- name: Prepare the secret parameters
  shell: |
    set -o pipefail;
    set -e

    cat "{{ kserve_deploy_model_secret_env_file_name }}" | sha256sum > {{ artifact_extra_logs_dir }}/artifacts/secret_env_file.sha256sum

    secret_data=$(cat "{{ kserve_deploy_model_secret_env_file_name }}" \
     | yq '.["{{ kserve_deploy_model_secret_env_file_key }}"] | to_entries | map( { key: .key, value: .value|tostring|@base64 }) | from_entries')

    if [[ -z "$secret_data" ]]; then
      echo "Could not find key '{{ kserve_deploy_model_secret_env_file_key }}' in file {{ kserve_deploy_model_secret_env_file_name }}. Aborting."
      exit 1
    fi
    oc create secret generic {{ kserve_deploy_model_sr_name }}-secret \
       -n {{ kserve_deploy_model_namespace }} \
       -ojson \
       --dry-run=client \
       | jq --argjson data "$secret_data" '.data = $data' \
       | oc apply -f-

    oc describe secret/{{ kserve_deploy_model_sr_name }}-secret \
       -n {{ kserve_deploy_model_namespace }} \
       > {{ artifact_extra_logs_dir }}/artifacts/env-secrets.desc

  when: kserve_deploy_model_secret_env_file_name != None

# Serving Runtime

- name: Prepare the caikit-tgis-config template
  when: kserve_deploy_model_sr_container_flavor == "tgis+caikit"
  template:
    src: "{{ caikit_tgit_config_template }}"
    dest: "{{ artifact_extra_logs_dir }}/src/caikit_tgit_config.yaml"
    mode: '0400'

- name: Prepare the ServingRuntime template
  template:
    src: "{{ serving_runtime_template }}"
    dest: "{{ artifact_extra_logs_dir }}/src/serving_runtime.yaml"
    mode: '0400'

- name: Create or update the Caikit TGIS config
  when: kserve_deploy_model_sr_container_flavor == "tgis+caikit"
  shell:
    set -o pipefail;

    oc create cm {{ kserve_deploy_model_sr_name }}-caikit-tgis-config
       -n {{ kserve_deploy_model_namespace }}
       --from-file=caikit.yml="{{ artifact_extra_logs_dir }}/src/caikit_tgit_config.yaml"
       --dry-run=client
       -oyaml
       | oc apply -f-


# --server-side is required here because multiple concurrent users
# might try to create the ServingRuntime simultaneously.
# https://github.com/kubernetes/kubernetes/issues/44165
- name: Create the ServingRuntime
  command:
    oc apply
      --server-side
      -f "{{ artifact_extra_logs_dir }}/src/serving_runtime.yaml"


# Inference Service

- name: Prepare the InferenceService template
  template:
    src: "{{ inference_service_template }}"
    dest: "{{ artifact_extra_logs_dir }}/src/inference_service.yaml"
    mode: '0400'

- name: Create the InferenceService
  command:
    oc apply -f "{{ artifact_extra_logs_dir }}/src/inference_service.yaml"

- name: Prepare the InferenceService
  block:
  - name: Wait for the InferenceService Pod to appear
    command:
      oc get pod
      -oname
      -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }}
      -n {{ kserve_deploy_model_namespace }}
    register: inference_service_pod_name
    # wait 15 minutes
    retries: 90
    delay: 10
    until: inference_service_pod_name.stdout | length > 0

  - name: Inform about the next task
    debug:
      msg: |
        The next tasks wait for loading of the InferenceService Pod
        Watch the progress with this command: oc get pods -n {{ kserve_deploy_model_namespace }} -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }}

  - name: Wait for the InferenceService Pod to be scheduled
    command:
      oc get pod
      -ojsonpath={.items[0].spec.nodeName}
      -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }}
      -n {{ kserve_deploy_model_namespace }}
    register: inference_service_pod_nodename
    # wait 1 minutes
    retries: 6
    delay: 10
    until: inference_service_pod_nodename.stdout | length > 0

  - name: Wait for the InferenceService Pod to be fetch the model from S3
    shell: |
      set -e;

      restarted=$(oc get pod \
          -ojsonpath={.items[0].status.initContainerStatuses[0].restartCount} \
          -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }} \
          -n {{ kserve_deploy_model_namespace }})

      if [[ "$restarted" && "$restarted" != 0 ]]; then
          echo "Container restart detected, aborting" >&2
          exit 2
      fi

      terminated=$(oc get pod \
          -ojsonpath={.items[0].status.initContainerStatuses[0].state.terminated} \
          -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }} \
          -n {{ kserve_deploy_model_namespace }})

      if [[ -z "$terminated" ]]; then
          echo "Container running, keep waiting ..." >&2
          exit 3;
      fi;

      echo "Container terminated without reboot, all good :)" >&2

    # wait 60 minutes
    retries: 120
    delay: 30
    register: inference_service_pod_fetching_cmd
    until: inference_service_pod_fetching_cmd.rc != 3
    ignore_errors: true

  - name: Wait for the InferenceService Pod to initialize the model
    shell: |
      set -o pipefail;
      restart_count=$(oc get pods -ojsonpath={.items[*].status.containerStatuses[*]} \
                         -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }} \
                         -n {{ kserve_deploy_model_namespace }} \
                     | jq .restartCount |  python -c "import sys; print(sum(int(l) for l in sys.stdin))");
      if [[ "$restart_count" != 0 ]]; then
        echo "ERROR: restart detected ($restart_count), aborting.";
        exit 2;
      fi;
      oc get -f "{{ artifact_extra_logs_dir }}/src/inference_service.yaml" \
         -ojsonpath={.status.modelStatus.states.targetModelState}
    register: inference_service_state_cmd
    # wait up to 90 minutes
    retries: 180
    delay: 30
    until: inference_service_state_cmd.stdout == "Loaded" or inference_service_state_cmd.stdout == "FailedToLoad" or inference_service_state_cmd.rc != 0
    failed_when: inference_service_state_cmd.stdout == "FailedToLoad" or inference_service_state_cmd.rc != 0

  - name: Capture the state of the InferenceService Pod resource
    shell:
      oc get pod
         -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }}
         -owide

  - name: Save timestamp
    shell: |
      NAME=inference-service-loaded
      oc create configmap $NAME -n {{ kserve_deploy_model_namespace }}
      oc label cm/$NAME topsail.time-tracking=yes -n {{ kserve_deploy_model_namespace }}

  always:
  - name: Capture the state of the InferenceService Pod resource
    shell:
      set -o pipefail;

      oc get pod
         -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }}
         -oyaml
         -n {{ kserve_deploy_model_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/predictor-pod.yaml;
      oc get pod
         -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }}
         -owide
         -n {{ kserve_deploy_model_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/predictor-pod.status;
      oc describe pod
         -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }}
         -n {{ kserve_deploy_model_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/predictor-pod.desc

      oc logs $(oc get pod -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }} -n {{ kserve_deploy_model_namespace }} -oname | head -1)
         -n {{ kserve_deploy_model_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/predictor-pod.log
    ignore_errors: true

  - name: Capture the state of the InferenceService resource
    shell:
      oc get -f "{{ artifact_extra_logs_dir }}/src/inference_service.yaml"
         -oyaml
         > {{ artifact_extra_logs_dir }}/artifacts/inference_service.yaml

  - name: Capture the state of the ServingRuntime resource
    shell:
      oc get -f "{{ artifact_extra_logs_dir }}/src/serving_runtime.yaml"
         -oyaml
         > {{ artifact_extra_logs_dir }}/artifacts/serving_runtime.yaml

  - name: Save the timestamp configmaps
    shell:
      oc get cm -oyaml
         -ltopsail.time-tracking=yes
         -n {{ kserve_deploy_model_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/time_tracking_cm.yaml
