---
- name: Create the src directory
  file:
    path: "{{ artifact_extra_logs_dir }}/src"
    state: directory
    mode: '0755'

- name: Create the artifacts directory
  file:
    path: "{{ artifact_extra_logs_dir }}/artifacts"
    state: directory
    mode: '0755'

# Cleanup the namespace
- name: Delete the InferenceServices
  command: oc delete InferenceServices,ServingRuntime --all -n {{ kserve_deploy_model_namespace }}
  when: kserve_deploy_model_delete_others | bool

- name: Wait for the Pods to disappear
  command:
    oc get pods
       --no-headers
       -lcomponent=predictor
       -n {{ kserve_deploy_model_namespace }}
  register: ns_had_predictor_pods_cmd
  retries: 12
  delay: 10
  until: '"No resources found" in ns_had_predictor_pods_cmd.stderr'
  when: kserve_deploy_model_delete_others | bool

# SMMR

- name: Delete the tracking timestamps
  command:
    oc delete cm -ltopsail.time-tracking -n {{ kserve_deploy_model_namespace }}

- name: Save timestamp
  shell: |
    NAME=start-deploy-model
    oc create configmap $NAME -n {{ kserve_deploy_model_namespace }}
    oc label cm/$NAME topsail.time-tracking=yes -n {{ kserve_deploy_model_namespace }}

# Serving Runtime

# Fail if "files/{{ kserve_deploy_model_runtime }}" don't exist
- name: Check if runtime deployment files exist
  debug:
    msg: "Checking if path {{ role_path }}/files/{{ kserve_deploy_model_runtime }} exists"

- name: Check for runtime
  ansible.builtin.stat:
    path: "{{ role_path }}/files/{{ kserve_deploy_model_runtime }}"
  register: runtime_dir

- name: Fail if doesn't exist
  fail: msg="The files for deploying this runtime do not exist"
  when: not (runtime_dir.stat.isdir is defined and runtime_dir.stat.isdir)

# Fail if "files/{{ kserve_deploy_model_runtime }}/models/{{ kserve_deploy_model_model_name }}" doesn't exist
- name: Check if template files exist
  debug:
    msg: "Checking if path {{ role_path }}/files/{{ kserve_deploy_model_runtime }}/models/{{ kserve_deploy_model_model_name }} exists"

- name: Check for model files
  ansible.builtin.stat: path="{{ role_path }}/files/{{ kserve_deploy_model_runtime }}/models/{{ kserve_deploy_model_model_name }}"
  register: model_dir

- name: Fail if the model files don't exist
  fail: msg="Model files don't exist at files/{{ kserve_deploy_model_runtime }}/models/{{ kserve_deploy_model_model_name }}"
  when: not (model_dir.stat.isdir is defined and model_dir.stat.isdir)

# Fail if "templates/" don't exist
- name: Check if template files exist
  debug:
    msg: "Checking if path {{ role_path }}/templates exists"

- name: Check for runtime templates
  ansible.builtin.stat: path="{{ role_path }}/templates"
  register: runtime_templates_dir

- name: Fail if doesn't exist
  fail: msg="Runtime template files don't exist"
  when: not (runtime_templates_dir.stat.isdir is defined and runtime_templates_dir.stat.isdir)

- name: Copy the kustomize files for this runtime
  copy:
    src: "files/{{ kserve_deploy_model_runtime }}/"
    dest: "{{ artifact_extra_logs_dir }}/src/{{ kserve_deploy_model_runtime }}"
    mode: '0400'
    directory_mode: '0755'

- name: Create the patch directory
  file:
    path: "{{ artifact_extra_logs_dir }}/src/patch/"
    state: directory
    mode: '0755'

- name: Apply the templates for this runtime
  template:
    src: "{{ servingruntime_patch_template }}"
    dest: "{{ artifact_extra_logs_dir }}/src/patch/patch-servingruntime.yaml"
    mode: '0400'

- name: Apply the templates for this inferenceservice
  template:
    src: "{{ inferenceservice_patch_template }}"
    dest: "{{ artifact_extra_logs_dir }}/src/patch/patch-inferenceservice.yaml"
    mode: '0400'

- name: Apply the templates for kustomization.yaml
  template:
    src: "{{ kustomization_patch_template }}"
    dest: "{{ artifact_extra_logs_dir }}/src/patch/kustomization.yaml"
    mode: '0400'

- name: Build the ServingRuntime and InferenceService with kustomize
  shell:
    oc kustomize {{ artifact_extra_logs_dir }}/src/patch/ > {{ artifact_extra_logs_dir }}/src/serving-runtime-and-isvc.yaml

# --server-side is required here because multiple concurrent users
# might try to create the ServingRuntime simultaneously.
# https://github.com/kubernetes/kubernetes/issues/44165
- name: Create the ServingRuntime and InferenceService
  command:
    oc apply
      --server-side
      -f "{{ artifact_extra_logs_dir }}/src/serving-runtime-and-isvc.yaml"


# Wait for the InferenceService Pod to run

- name: Prepare the InferenceService
  block:
  - name: Wait for the InferenceService Pod to appear
    command:
      oc get pod
      -oname
      -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }}
      -n {{ kserve_deploy_model_namespace }}
    register: inference_service_pod_name
    # wait 15 minutes
    retries: 90
    delay: 10
    until: inference_service_pod_name.stdout | length > 0

  - name: Inform about the next task
    debug:
      msg: |
        The next tasks wait for loading of the InferenceService Pod
        Watch the progress with this command: oc get pods -n {{ kserve_deploy_model_namespace }} -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }}

  - name: Wait for the InferenceService Pod to be scheduled
    command:
      oc get pod
      -ojsonpath={.items[0].spec.nodeName}
      -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }}
      -n {{ kserve_deploy_model_namespace }}
    register: inference_service_pod_nodename
    # wait 1 minutes
    retries: 6
    delay: 10
    until: inference_service_pod_nodename.stdout | length > 0

  - name: Wait for the InferenceService Pod to fetch the model from S3
    shell: |
      set -e;

      restarted=$(oc get pod \
          -ojsonpath={.items[0].status.initContainerStatuses[0].restartCount} \
          -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }} \
          -n {{ kserve_deploy_model_namespace }})

      if [[ "$restarted" && "$restarted" != 0 ]]; then
          echo "Container restart detected, aborting" >&2
          exit 2
      fi

      terminated=$(oc get pod \
          -ojsonpath={.items[0].status.initContainerStatuses[0].state.terminated} \
          -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }} \
          -n {{ kserve_deploy_model_namespace }})

      if [[ -z "$terminated" ]]; then
          echo "Container running, keep waiting ..." >&2
          exit 3;
      fi;

      echo "initContainer terminated without reboot, all good :)" >&2

    # wait up to 60 minutes
    retries: 120
    delay: 30
    register: inference_service_pod_fetching_cmd
    until: inference_service_pod_fetching_cmd.rc != 3
    ignore_errors: true

  - name: Wait for the InferenceService Pod to initialize the model
    shell: |
      set -o pipefail;
      restart_count=$(oc get pods -ojsonpath={.items[*].status.containerStatuses[*]} \
                         -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }} \
                         -n {{ kserve_deploy_model_namespace }} \
                     | jq .restartCount |  python -c "import sys; print(sum(int(l) for l in sys.stdin))");
      if [[ "$restart_count" != 0 ]]; then
        echo "ERROR: restart detected ($restart_count), aborting.";
        exit 2;
      fi;
      oc get inferenceservice/{{ kserve_deploy_model_inference_service_name }} \
         -ojsonpath={.status.modelStatus.states.targetModelState} \
         -n {{ kserve_deploy_model_namespace }}
    register: inference_service_state_cmd
    # wait up to 90 minutes
    retries: 180
    delay: 30
    until: inference_service_state_cmd.stdout == "Loaded" or inference_service_state_cmd.stdout == "FailedToLoad" or inference_service_state_cmd.rc == 2
    failed_when: inference_service_state_cmd.stdout != "Loaded" or inference_service_state_cmd.rc != 0

  - name: Capture the state of the InferenceService Pod resource
    shell:
      oc get pod
         -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }}
         -owide

  - name: Save timestamp
    shell: |
      NAME=inference-service-loaded
      oc create configmap $NAME -n {{ kserve_deploy_model_namespace }}
      oc label cm/$NAME topsail.time-tracking=yes -n {{ kserve_deploy_model_namespace }}

  always:
  - name: Capture the state of the InferenceService Pod resource
    shell:
      set -o pipefail;

      oc get pod
         -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }}
         -oyaml
         -n {{ kserve_deploy_model_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/predictor-pod.yaml;
      oc get pod
         -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }}
         -owide
         -n {{ kserve_deploy_model_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/predictor-pod.status;
      oc describe pod
         -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }}
         -n {{ kserve_deploy_model_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/predictor-pod.desc

      oc logs $(oc get pod -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }} -n {{ kserve_deploy_model_namespace }} -oname | head -1)
         -n {{ kserve_deploy_model_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/predictor-pod.log
    ignore_errors: true

  - name: Capture the state of the InferenceService resource
    shell:
      oc get inferenceservice/{{ kserve_deploy_model_inference_service_name }}
         -n {{ kserve_deploy_model_namespace }}
         -oyaml
         > {{ artifact_extra_logs_dir }}/artifacts/inference_service.yaml

  - name: Capture the state of the ServingRuntime resource
    shell:
      oc get servingruntime/{{ kserve_deploy_model_sr_name }}
         -n {{ kserve_deploy_model_namespace }}
         -oyaml
         > {{ artifact_extra_logs_dir }}/artifacts/serving_runtime.yaml

  - name: Save the timestamp configmaps
    shell:
      oc get cm -oyaml
         -ltopsail.time-tracking=yes
         -n {{ kserve_deploy_model_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/time_tracking_cm.yaml
