apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: vllm-servingruntime # Overwritten by templates
  labels:
    opendatahub.io/dashboard: "true"
  annotations:
    opendatahub.io/template-display-name: "ServingRuntime for vLLM | Topsail"
spec:
  annotations:
    serving.knative.dev/progress-deadline: 60m
  builtInAdapter:
    modelLoadingTimeoutMillis: 90000
  containers:
  - args: # Merged with args in inferenceservice
    - --model=/mnt/models/
    - --download-dir=/models-cache
    - --port=8080
    image: quay.io/opendatahub/vllm:fast-ibm-nightly-2024-05-01 # Overwritten by templates
    name: kserve-container
    ports:
    - containerPort: 8080
      name: http1
      protocol: TCP
    volumeMounts:
      - name: home
        mountPath: /home/vllm
      - name: cache
        mountPath: /.cache
      - name: config
        mountPath: /.config
  volumes:
    - name: home
      emptyDir: {}
    - name: cache
      emptyDir: {}
    - name: config
      emptyDir: {}
  multiModel: false
  supportedModelFormats:
  - autoSelect: true
    name: pytorch
