{% set secrets_location = false | or_env(secrets.dir.env_key) %}
{% if not secrets_location %}
  {{ ("ERROR: secrets_location must be defined (secrets.dir.name="+ secrets.dir.name|string +" or env(secrets.dir.env_key=" + secrets.dir.env_key|string + ")) ") | log_warning }}
  {% set secrets_location = "/secret/locatation/not/set" %}
{% endif %}
{% set s3_ldap_password_location = secrets_location + "/" + secrets.s3_ldap_password_file %}

# ---

sutest/cluster set_scale:
  name: {{ clusters.sutest.compute.machineset.name }}
  instance_type: {{ clusters.sutest.compute.machineset.type }}
{% if clusters.sutest.compute.dedicated %}
  taint: {{ clusters.sutest.compute.machineset.taint.key }}={{ clusters.sutest.compute.machineset.taint.value }}:{{ clusters.sutest.compute.machineset.taint.effect }}
{% endif %}
  disk_size: {{ clusters.sutest.compute.machineset.disk_size }}
  spot: {{ clusters.sutest.compute.machineset.spot }}
  scale: SET_AT_RUNTIME

#
# Scale test - deploy RHODS
#

rhods deploy_ods:
  catalog_image: {{ rhods.catalog.image }}
  tag: {{ rhods.catalog.tag }}
  channel: {{ rhods.catalog.channel }}
  version: {{ rhods.catalog.version }}
  opendatahub: {{ rhods.catalog.opendatahub }}
  managed_rhoai: {{ rhods.catalog.managed_rhoai }}

#
# Scale test - Run one
#

sutest/cluster set_project_annotation/scale_test_node_selector:
  key: openshift.io/node-selector
  value: "{{ clusters.sutest.compute.machineset.taint.key }}={{ clusters.sutest.compute.machineset.taint.value }}"

sutest/cluster set_project_annotation/scale_test_toleration:
  key: scheduler.alpha.kubernetes.io/defaultTolerations
  value: '[{\"operator\": \"Exists\", \"effect\": \"{{ clusters.sutest.compute.machineset.taint.effect }}\", \"key\": \"{{ clusters.sutest.compute.machineset.taint.key }}\"}]'

#
# Scale test - Prepare RHOAI KServe
#

sutest/cluster preload_image/kserve-runtime:
{% if tests.mode == "scale" %}
  namespace: {{ tests.scale.namespace.name }}
{% else %}
  namespace: {{ tests.e2e.namespace }}
{% endif %}
  name: SET_AT_RUNTIME
  image: SET_AT_RUNTIME

  node_selector_key: {{ clusters.sutest.compute.machineset.taint.key }}
  node_selector_value: "{{ clusters.sutest.compute.machineset.taint.value }}"
  pod_toleration_effect: {{ clusters.sutest.compute.machineset.taint.effect }}
  pod_toleration_key: {{ clusters.sutest.compute.machineset.taint.key }}

#
# Scale Test - Prepare GPU
#

gpu_operator enable_time_sharing:
  replicas: {{ gpu.time_sharing.replicas }}

#
# Scale test - Prepare User Pods
#

driver/cluster set_scale:
  instance_type: {{ clusters.driver.compute.machineset.type }}
  name: {{ clusters.driver.compute.machineset.name }}
{% if clusters.driver.compute.dedicated %}
  taint: {{ clusters.driver.compute.machineset.taint.key }}={{ clusters.driver.compute.machineset.taint.value }}:{{ clusters.driver.compute.machineset.taint.effect }}
{% endif %}
  spot: {{ clusters.driver.compute.machineset.spot }}
  scale: SET_AT_RUNTIME

driver/cluster set_project_annotation/test_node_selector:
  key: openshift.io/node-selector
  value: "{{ clusters.driver.compute.machineset.taint.key }}={{ clusters.driver.compute.machineset.taint.value }}"
  project: {{ base_image.namespace }}

driver/cluster set_project_annotation/test_toleration:
  key: scheduler.alpha.kubernetes.io/defaultTolerations
  value: '[{\"operator\": \"Exists\", \"effect\": \"{{ clusters.driver.compute.machineset.taint.effect }}\", \"key\": \"{{ clusters.driver.compute.machineset.taint.key }}\"}]'
  project: {{ base_image.namespace }}

base_image/cluster build_push_image:
  namespace: "{{ base_image.namespace }}"
  image_local_name: "{{ base_image.imagestream }}"
  tag: "{{ base_image.repo.tag }}"
  _istag: "{{ base_image.imagestream }}:{{ base_image.repo.tag }}"

  git_repo: "{{ base_image.repo.url }}"
  git_ref: "{{ base_image.repo.ref }}" # may be overwritten at runtime with the PR ref
  dockerfile_path: "{{ base_image.repo.dockerfile_path }}"

extended_image/cluster build_push_image:
  namespace: "{{ base_image.namespace }}"
  image_local_name: "{{ base_image.imagestream }}"
  tag: "{{ base_image.extend.tag }}"
  _istag: "{{ base_image.imagestream }}:{{ base_image.extend.tag }}"

  dockerfile_path: "{{ base_image.extend.local_dockerfile_path }}"
  from_imagetag: "{{ base_image.imagestream }}:{{ base_image.repo.tag }}"

cluster deploy_redis_server:
{% set redis_internal_address = "redis."+base_image.namespace+".svc" %}
  namespace: "{{ base_image.namespace }}"

cluster deploy_minio_s3_server:
  namespace: "{{ base_image.namespace }}"
  secret_properties_file: {{ s3_ldap_password_location }}
  bucket_name: {{ base_image.minio.bucket_name }}

cluster reset_prometheus_db/uwm:
  label: app.kubernetes.io/instance=user-workload,app.kubernetes.io/component=prometheus
  namespace: openshift-user-workload-monitoring

cluster dump_prometheus_db/uwm:
  label: app.kubernetes.io/instance=user-workload,app.kubernetes.io/component=prometheus
  namespace: openshift-user-workload-monitoring

#
# Test RHOAI KServe scale
#

local_ci run_multi/scale:
  user_count: "{{ tests.scale.namespace.replicas }}"
  namespace: "{{ base_image.namespace }}"
  job_name: scale-test-run-one

  istag: "{{ base_image.imagestream }}:{{ base_image.extend.tag }}"
  service_account: "{{ base_image.user.service_account }}"
  state_signal_redis_server: {{ redis_internal_address }}

  secret_name: "{{ secrets.dir.name }}"
  secret_env_key: "{{ secrets.dir.env_key }}"

  ci_command: "kserve test run_one"

  retrieve_artifacts: true
  minio_bucket_name: "{{ base_image.minio.bucket_name }}"
  minio_namespace: "{{ base_image.namespace }}"
  minio_secret_key_key: s3_ldap.passwords

  sleep_factor: {{ tests.scale.sleep_factor }}
  user_batch_size: 1

  git_pull: null #refs/pull/716/merge
  capture_prom_db: "{{ tests.capture_prom }}"

{% if tests.mode == "scale" and tests.scale.model.consolidated %}

#
# Test RHOAI KServe scale: run one
#

kserve deploy_model:
  namespace: {{ tests.scale.namespace.name }}
  sa_name: {{ kserve.sa_name }}

  sr_name: {{ tests.scale.model.name.split("/")[-1] }}

  sr_kserve_image: {{ kserve.model.serving_runtime.kserve.image }}
  sr_kserve_resource_request: {{ tests.scale.model.serving_runtime.kserve.resource_request }}
  inference_service_model_format: {{ tests.scale.model.inference_service.model_format }}
  sr_transformer_image: {{ kserve.model.serving_runtime.transformer.image }}
  sr_transformer_resource_request: {{ tests.scale.model.serving_runtime.transformer.resource_request }}
  sr_mute_logs: {{ kserve.model.serving_runtime.mute_logs }}
  sr_container_flavor: {{ tests.scale.model.serving_runtime.container_flavor }}
  inference_service_name: {{ tests.scale.model.name }}
  storage_uri: {{ tests.scale.model.inference_service.storage_uri }}

  delete_others: false
  limits_equals_requests: {{ tests.e2e.limits_equals_requests }}

kserve validate_model:
  namespace: {{ tests.scale.namespace.name }}
  inference_service_names: [{{ tests.scale.model.name }}]

  query_count: {{ kserve.inference_service.validation.query_count }}
{% endif %}

#
# Test RHOAI KServe e2e
#
__e2e_aliases:
  local_ci_run_multi_e2e_common: &local_ci_run_multi_e2e
    namespace: "{{ base_image.namespace }}"
    istag: "{{ base_image.imagestream }}:{{ base_image.extend.tag }}"
    service_account: "{{ base_image.user.service_account }}"
    state_signal_redis_server: {{ redis_internal_address }}

    secret_name: "{{ secrets.dir.name }}"
    secret_env_key: "{{ secrets.dir.env_key }}"
    user_count: "{{ tests.e2e.models | length }}"

    retrieve_artifacts: true
    minio_bucket_name: "{{ base_image.minio.bucket_name }}"
    minio_namespace: "{{ base_image.namespace }}"
    minio_secret_key_key: s3_ldap.passwords

    user_batch_size: 1
    capture_prom_db: false # Prom DB is captured manually for the e2e test
    git_pull: null #refs/pull/716/merge

    use_local_config: true
    need_all_success: true

local_ci run_multi/deploy_concurrently:
  <<: *local_ci_run_multi_e2e
  ci_command: "kserve test_e2e deploy_one_model --use-job-index"
  sleep_factor: 0
  job_name: deploy-one-model

local_ci run_multi/test_sequentially:
  <<: *local_ci_run_multi_e2e
  ci_command: "kserve test_e2e multi_model_test_sequentially --locally"
  sleep_factor: 0
  user_count: 1
  job_name: test-models-seq

local_ci run_multi/test_concurrently:
  <<: *local_ci_run_multi_e2e
  ci_command: "kserve test_e2e test_one_model --use-job-index"
  sleep_factor: 0
  job_name: test-one-model

local_ci run_multi/deploy_and_test_sequentially:
  <<: *local_ci_run_multi_e2e
  ci_command: "kserve test_e2e single_model_deploy_and_test_sequentially --locally"
  sleep_factor: 0
  user_count: 1
  job_name: deploy-and-test-models
