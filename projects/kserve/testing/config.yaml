ci_presets:
  # name of the presets to apply, or null if no preset
  name: null
  # list of names of presets to apply, or a single name, or null if not preset
  names: null
  single:
    clusters.create.type: single

  keep:
    extends: [spot_cluster]

    clusters.create.keep: true
    clusters.create.ocp.tags.Project: PSAP/Watsonx/serving/scale/home-dev
    clusters.create.ocp.tags.TicketId: null

  light_cluster:
    clusters.create.ocp.deploy_cluster.target: cluster_light

  light:
    extends: [light_cluster]

  no_gpu:
    gpu.prepare_cluster: false
    clusters.sutest.compute.machineset.type: m6i.2xlarge

  gpu:
    gpu.prepare_cluster: true

  cluster_dgx:
    clusters.sutest.compute.machineset.type: "DGX A100-40GB"

  cluster_icelake:
    clusters.sutest.compute.machineset.type: "Icelake"

  cluster_a30:
    clusters.sutest.compute.machineset.type: "PSAP A30 node"

  cluster_2xa100:
    clusters.sutest.compute.machineset.type: "Beaker A100-80GB"

  metal:
    clusters.sutest.is_metal: true
    clusters.driver.is_metal: true
    clusters.sutest.compute.dedicated: false
    clusters.driver.compute.dedicated: false
    clusters.sutest.compute.machineset.type: "bare-metal node"

  not_metal:
    clusters.sutest.is_metal: false
    clusters.driver.is_metal: false

  spot_compute:
    clusters.sutest.compute.machineset.spot: true
    clusters.driver.compute.machineset.spot: true

  spot_cluster:
    extends: [spot_compute]
    clusters.create.ocp.workers.spot: true
    clusters.create.ocp.region: us-east-2

  cleanup:
    clusters.cleanup_on_exit: true

  # ---

  e2e_perf_strict_limits:
    extends: [e2e_perf]
    tests.e2e.limits_equals_requests: true

  e2e_perf:
    extends: [e2e_gpu]
    tests.e2e.mode: single
    tests.e2e.limits_equals_requests: false
    matbench.workload: projects.kserve.visualizations.kserve-llm
    gpu.time_sharing.replicas: 1
    clusters.sutest.compute.machineset.count: 1

  e2e:
    tests.mode: e2e
    gpu.time_sharing.replicas: 1

  e2e_multi:
    extends: [e2e_gpu]
    tests.e2e.mode: multi
    gpu.time_sharing.replicas: 1
    matbench.lts.generate: false

  e2e_gpu:
    extends: [e2e]
    gpu.prepare_cluster: true
    clusters.sutest.compute.machineset.spot: false
    clusters.sutest.compute.machineset.type: g5.2xlarge

  raw:
    kserve.raw_deployment.enabled: true

  quick_raw:
    extends: [raw, no_gpu]

    tests.e2e.models:
    - name: flan-t5-small-1
      model: flan-t5-small
      testing:
        size: small
        max_concurrency: 2
    - name: flan-t5-small-2
      model: flan-t5-small
      testing:
        size: small
        max_concurrency: 2
    tests.e2e.llm_load_test.args.concurrency: 2

  single_raw:
    extends: [quick_raw]
    tests.e2e.models:
    - name: flan-t5-small
      testing:
        size: small
        max_concurrency: 2

  # ---

  3min:
    tests.e2e.llm_load_test.args.duration: 180

  5min:
    tests.e2e.llm_load_test.args.duration: 300

  10min:
    tests.e2e.llm_load_test.args.duration: 600

  vllm:
    kserve.model.runtime: vllm
    # IBM image hardcoded until RHOAI includes an image in the release
    kserve.model.serving_runtime.kserve.image: "quay.io/modh/vllm:rhoai-2.10-2cf4d15"
    tests.e2e.llm_load_test.args.plugin: "openai_plugin"
    tests.e2e.llm_load_test.args.streaming: true
    tests.e2e.llm_load_test.args.interface: "http"
      # tests.e2e.llm_load_test.args.model_name: "/mnt/models/"
      # tests.e2e.llm_load_test.args.host: "http://localhost:8033"
      # tests.e2e.llm_load_test.args.endpoint: "/v1/chat/completions"

  # ---

  scale_test:
    tests.mode: scale
    matbench.workload: projects.kserve.visualizations.kserve-scale
    tests.prom_plot_workload: null
    kserve.model.serving_runtime.mute_logs: false

  # ---

  customize_smcp:
    kserve.customize.serverless.enabled: true
    kserve.customize.serverless.egress.limits.memory: 4Gi
    kserve.customize.serverless.ingress.limits.memory: 4Gi

  # ---

  gating:
    matbench.lts.opensearch.index_prefix: "psap-rhoai."
    matbench.lts.opensearch.instance: intlab
    matbench.lts.opensearch.export.enabled: true

  gating_rehearsal:
    matbench.lts.opensearch.index_prefix: "psap-rhoai.rehearsal."
    matbench.lts.opensearch.instance: intlab

  # ---
  # Get models from integration lab minio instead of AWS S3
  use_intlab_minio:
    secrets.model_s3_cred: intlab-miniocred
    kserve.storage_config.region: "us-east-1"
    kserve.storage_config.endpoint: "minio.app.intlab.redhat.com"
    kserve.storage_config.use_https: "1"
    kserve.storage_config.verify_ssl: "0"

  skip_opensearch_export:
    matbench.lts.opensearch.export.enabled: false

  # ---
  # single-model
  # ---

  cpt_single_model_gating_light:
    extends: [cpt_single_model_gating, gating_rehearsal, use_intlab_minio]
    tests.e2e.models:
    - name: flan-t5-small
      testing:
        size: small
        max_concurrency: 8
    tests.e2e.matbenchmark.enabled: true
    tests.e2e.llm_load_test.args.concurrency: [4, 8]
    matbench.lts.horreum.test_name: null
    matbench.lts.opensearch.index: rhoai-kserve-single-light

  cpt_single_model_gating:
    extends: [e2e_perf, gating, 3min, raw, use_intlab_minio]
    # kserve.model.directory_prefix: cpt
    tests.e2e.models:
    - name: flan-t5-xxl
      testing:
        size: small
        max_concurrency: 32
    - name: llama-3-8b-instruct
      testing:
        size: small
        max_concurrency: 32
    - name: llama-2-13b-chat-hf
      testing:
        size: small
        max_concurrency: 32
    - name: mpt-7b-instruct2
      testing:
        size: small
        max_concurrency: 32
    - name: mistral-7b-instruct-v02
      testing:
        size: small
        max_concurrency: 32
    - name: llama-3-70b-instruct
      testing:
        size: large
    - name: codellama-34b-instruct-hf
      testing:
        size: large
    - name: granite-3b-code-instruct
      testing:
        size: small
    - name: granite-8b-code-instruct
      testing:
        size: small
        max_concurrency: 64
    - name: granite-20b-code-instruct
      testing:
        size: large
    matbench.lts.horreum.test_name: rhoai-kserve-single
    matbench.lts.opensearch.index: rhoai-kserve-single

    tests.e2e.matbenchmark.enabled: true
    # small models will stop at 32
    tests.e2e.llm_load_test.args.concurrency: [1, 2, 4, 8, 16, 32, 64, 96, 128]
    export_artifacts.enabled: true

  cpt_single_model_gating_mistral:
    extends: [e2e_perf, gating, 3min, raw, use_intlab_minio]
    # kserve.model.directory_prefix: cpt
    tests.e2e.models:
    - name: mistral-7b-instruct-v02
      testing:
        size: small
        max_concurrency: 32
    matbench.lts.horreum.test_name: rhoai-kserve-single
    matbench.lts.opensearch.index: rhoai-kserve-single

    tests.e2e.matbenchmark.enabled: true
    # small models will stop at 32
    tests.e2e.llm_load_test.args.concurrency: [1, 2, 4, 8, 16, 32, 64, 96, 128]
    export_artifacts.enabled: true

  # --
  # single-model vLLM
  # --
  vllm_cpt_single_model_gating_light:
    extends: [raw, e2e_gpu, vllm]
    tests.e2e.models:
    - name: phi-2
      testing:
        size: small
        max_concurrency: 64
    tests.e2e.matbenchmark.enabled: true
    tests.e2e.llm_load_test.args.concurrency: [1, 2, 4, 8, 16, 32, 64, 96, 128]

  vllm_cpt_single_model_gating:
    extends: [e2e_perf, gating, 3min, raw, use_intlab_minio, vllm]
    tests.e2e.models:
    - name: llama-3-8b-instruct
      testing:
       size: small
       max_concurrency: 64
    - name: llama-2-13b-chat-hf
      testing:
        size: small
        max_concurrency: 32
    - name: mpt-7b-instruct2
      testing:
        size: small
        max_concurrency: 64
    - name: llama-3-70b-instruct
      testing:
        size: large
    - name: codellama-34b-instruct-hf
      testing:
        size: large
    - name: mistral-7b-instruct-v02
      testing:
        size: small
        max_concurrency: 64
    - name: mixtral-8x7b
      testing:
        size: large
    - name: granite-3b-code-instruct
      testing:
        size: small
    - name: granite-8b-code-instruct
      testing:
        size: small
        max_concurrency: 64
    - name: granite-20b-code-instruct
      testing:
        size: large
    matbench.lts.horreum.test_name: rhoai-kserve-single
    matbench.lts.opensearch.index: rhoai-kserve-single

    tests.e2e.matbenchmark.enabled: true
    # small models will stop at 32
    tests.e2e.llm_load_test.args.concurrency: [1, 2, 4, 8, 16, 32, 64, 96, 128]
    export_artifacts.enabled: true

  # ---
  # multi-model
  # ---

  cpt_multi_model_gating:
    extends: [gating, 5min, e2e_gpu, e2e_multi, raw, cpt_multi_model_models]
    matbench.lts.opensearch.index: rhoai-kserve-multi
    matbench.lts.horreum.test_name: rhoai-kserve-multi
    tests.e2e.limits_equals_requests: false
    tests.e2e.llm_load_test.args.concurrency: 16

  cpt_multi_model_models:
    extends: [tgis, raw, use_intlab_minio]
    tests.e2e.models:
    - name: mpt-7b-instruct2
    - name: mpt-7b-instruct2
    - name: japanese-llama-2-7b-instruct
    - name: llama-2-13b-chat-hf
    - name: llama-2-13b-chat-hf
    - name: flan-t5-xl
      namespace: kserve-e2e-flan-grouped
      serving_runtime:
        name: kserve-flan-grouped
    - name: flan-t5-xl
      namespace: kserve-e2e-flan-grouped
      serving_runtime:
        name: kserve-flan-grouped
    - name: flan-t5-xxl
      namespace: kserve-e2e-flan-grouped
      serving_runtime:
        name: kserve-flan-grouped

  # multi-model non-cpt

  multi_model_gating_light:
    extends: [cpt_multi_model_gating, gating_rehearsal, 3min, no_gpu]
    tests.e2e.models:
    - name: flan-t5-small
    - name: flan-t5-small
    - name: flan-t5-small-1
      model: flan-t5-small
      namespace: kserve-e2e-grouped
      serving_runtime:
        name: flan-t5-small-hf-cpu
    - name: flan-t5-small-2
      model: flan-t5-small
      namespace: kserve-e2e-grouped
      serving_runtime:
        name: flan-t5-small-hf-cpu


  # ---
  # longevity
  # ---

  cpt_longevity_gating:
    extends: [cpt_multi_model_models, multi_day_longevity, raw]
    tests.e2e.limits_equals_requests: false
    tests.e2e.llm_load_test.args.concurrency: 16

  longevity:
    extends: [10min]
    tests.e2e.mode: longevity

  cpt_longevity:
    extends: [longevity]

    matbench.lts.horreum.test_name: rhoai-kserve-longevity
    matbench.lts.opensearch.index: rhoai-kserve-longevity

  multi_model_a30:
    extends: [raw, use_intlab_minio, tgis]

    tests.e2e.models:
    - name: flan-t5-xl
    - name: flan-t5-xl
    - name: japanese-llama-2-7b-instruct

  hour_longevity:
    extends: [longevity]

    tests.e2e.longevity.sleep_interval: 15 min
    tests.e2e.longevity.total_duration: 1 hour

  multi_hour_longevity:
    extends: [longevity]
    tests.e2e.longevity.sleep_interval: 20 min
    tests.e2e.longevity.total_duration: 24 hours

  multi_day_longevity:
    extends: [longevity]

    tests.e2e.longevity.sleep_interval: 12 hours
    tests.e2e.longevity.total_duration: 3 days

  longevity_large_model:
    extends: [multi_hour_longevity, raw, tgis, use_intlab_minio]
    tests.e2e.models:
    - name: codellama-34b-instruct-hf
    - name: flan-t5-xl

secrets:
  dir:
    name: psap-ods-secret
    env_key: PSAP_ODS_SECRET_PATH
  # name of the file containing the properties of LDAP secrets
  s3_ldap_password_file: s3_ldap.passwords
  keep_cluster_password_file: get_cluster.password
  brew_registry_redhat_io_token_file: brew.registry.redhat.io.token
  model_s3_cred: .awscred
  kserve_model_secret_settings: watsonx-models.yaml
  opensearch_instances: opensearch.yaml
  aws_credentials: .awscred

clusters:
  metal_profiles:
    p42-h03-dgx.rdu3.labs.perfscale.redhat.com: cluster_dgx
    e26-h23-000-r650: cluster_icelake
    bb37-h13-000-r750.rdu3.labs.perfscale.redhat.com: cluster_a30
    nvd-srv-02.nvidia.eng.rdu2.redhat.com: cluster_2xa100
  create:
    type: single # can be: single, ocp, managed
    keep: false
    name_prefix: kserve-ci
    ocp:
      # list of tags to apply to the machineset when creating the cluster
      tags:
        TicketId:
        Project: PSAP/Watsonx/serving/scale/ci-dev
      deploy_cluster:
        target: cluster
      base_domain: psap.aws.rhperfscale.org
      version: 4.13.9
      region: us-west-2
      control_plane:
        type: m6a.xlarge
      workers:
        type: m6a.2xlarge
        count: 2
        spot: false

  sutest:
    is_metal: false
    lab:
      name: null
    compute:
      dedicated: true
      machineset:
        name: workload-pods
        type: g5.2xlarge
        count: null
        disk_size: 400 # GB for the root partition
        spot: false
        taint:
          key: only-workload-pods
          value: "yes"
          effect: NoSchedule

  driver:
    is_metal: false
    compute:
      dedicated: true
      machineset:
        name: test-pods
        type: m6i.2xlarge
        count: null
        spot: false
        taint:
          key: only-test-pods
          value: "yes"
          effect: NoSchedule
  cleanup_on_exit: false

rhods:
  catalog:
    image: brew.registry.redhat.io/rh-osbs/iib
    tag: 741931
    channel: stable
    version: 2.10.0
    version_name: rc1
    opendatahub: false
    managed_rhoai: false
  operator:
    # set to true to stop the RHODS operator
    stop: false
    customize:
      kserve:
        enabled: false
        cpu: 500m
        memory: 500Mi

prepare:
  enabled: true
  operators:
  - name: serverless-operator
    catalog: redhat-operators
    namespace: all
    cleanup:
      crds:
      - knativeservings.operator.knative.dev
      namespaces:
      - knative-eventing
      - knative-serving
  cleanup:
    enabled: true
    crds:
    - InferenceService
    - ServingRuntime

base_image:
  namespace: kserve-user-test-driver
  imagestream: topsail
  repo:
    url: https://github.com/openshift-psap/topsail/
    tag: main
    ref: main
    ref_prefer_pr: true
    dockerfile_path: build/Dockerfile
  extend:
    enabled: true
    local_dockerfile_path: projects/kserve/testing/images/Containerfile.e2e_test_user
    tag: e2e-test-user
  user:
    service_account: ci-artifacts
    role: cluster-admin
  minio:
    bucket_name: kserve-test-bucket

kserve:
  sa_name: sa
  raw_deployment:
    enabled: false
  storage_config:
    name: storage-config
    region: us-east-1
    endpoint: s3.amazonaws.com
    use_https: 1
    verify_ssl: 1

  inference_service: # TODO get rid of caikit validation and add vLLM validation
    validation:
      query_count: 10
      method: fmaas.GenerationService/Generate
      proto: projects/kserve/testing/protos/tgis_generation.proto
  model: # Applied to all models if set
    runtime: standalone-tgis
    serving_runtime:
      update_image: true
      kserve:
        image: quay.io/opendatahub/text-generation-inference:fast
  customize:
    serverless:
      enabled: false
      egress:
        limits:
          memory: 4Gi
      ingress:
        limits:
          memory: 4Gi
gpu:
  prepare_cluster: true
  time_sharing:
    replicas: 1

tests:
  mode: e2e

  dry_mode: false
  visualize: true
  capture_prom: true
  capture_prom_uwm: false
  prom_plot_workload: projects.kserve.visualizations.kserve-prom
  prom_plot_index_suffix: -prom
  scale:
    sleep_factor: 1
    namespace:
      name: kserve-scale-test
      label: topsail.scale-test=true
      replicas: 2

    model:
      consolidated: false
      name: flan-t5-small
      runtime: standalone-tgis
      replicas: 2
  e2e:
    namespace: kserve-e2e
    mode: single # single, longevity or multi
    models:
    - name: flan-t5-small
      testing:
        size: small
        max_concurrency: 16
    - name: flan-t5-small
      testing:
        size: small
        max_concurrency: 16
    consolidated_models: {} # will be filled at runtime
    limits_equals_requests: false
    delete_others: true
    validate_model: true
    capture_state: true
    matbenchmark:
      enabled: false
      stop_on_error: true
    llm_load_test:
      enabled: true
      args:
        src_path: projects/llm_load_test/subprojects/llm-load-test/

        duration: 60
        concurrency: 16
        plugin: tgis_grpc_plugin
        interface: grpc
        streaming: true
        endpoint: null # only used with openai plugin
      dataset_size:
        small:
          max_input_tokens: 2047
          max_output_tokens: 1024
          max_sequence_tokens: 2048
        large:
          max_input_tokens: 3500
          max_output_tokens: 800
          max_sequence_tokens: 3500
    longevity:
      sleep_interval: 60 sec
      # https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.html
      total_duration: 15 min
      test_on_finish: true

matbench:
  preset: null
  workload: projects.kserve.visualizations.kserve-llm
  config_file: plots.yaml
  download:
    mode: prefer_cache
    url:
    url_file:
    # if true, copy the results downloaded by `matbench download` into the artifacts directory
    save_to_artifacts: false
  ignore_exit_code: true
  # directory to plot. Set by topsail/testing/visualize.py before launching the visualization
  test_directory: null
  lts:
    generate: true
    horreum:
      test_name: null
    opensearch:
      export:
        enabled: false
        enabled_on_replot: false
        fail_test_on_fail: true
      instance: smoke
      index: rhoai-kserve-single
      index_prefix: ""

    regression_analyses:
      enabled: false
      enabled_on_replot: false
      # if the regression analyses fail, mark the test as failed
      fail_test_on_regression: true
export_artifacts:
  enabled: false
  bucket: rhoai-cpt-artifacts
  path_prefix: cpt/kserve
  dest: null # will be set by the export code
