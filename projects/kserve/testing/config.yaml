ci_presets:
  # name of the presets to apply, or null if no preset
  name: null
  # list of names of the presets to apply, or a single name, or null if no preset
  names: null

  single:
    clusters.create.type: single

  keep:
    extends: [spot_cluster]

    clusters.create.keep: true
    clusters.create.ocp.tags.Project: PSAP/Watsonx/serving/scale/home-dev
    clusters.create.ocp.tags.TicketId: 229

  light_cluster:
    clusters.create.ocp.deploy_cluster.target: cluster_light

  light:
    extends: [light_cluster]

  quick:
    extends: [no_gpu]
    tests.e2e.models:
    - flan-t5-small-cpu
    - flan-t5-small-cpu

  no_gpu:
    gpu.prepare_cluster: false
    clusters.sutest.compute.machineset.type: m6i.2xlarge

  dgx:
    clusters.sutest.compute.machineset.type: "DGX A100-40GB"

  icelake:
    clusters.sutest.compute.machineset.type: "Icelake"

  metal:
    clusters.sutest.is_metal: true
    clusters.driver.is_metal: true
    clusters.sutest.compute.dedicated: false
    clusters.driver.compute.dedicated: false

  not_metal:
    clusters.sutest.is_metal: false
    clusters.driver.is_metal: false

  spot_compute:
    clusters.sutest.compute.machineset.spot: true
    clusters.driver.compute.machineset.spot: true

  spot_cluster:
    extends: [spot_compute]
    clusters.create.ocp.workers.spot: true
    clusters.create.ocp.region: us-east-2

  cleanup:
    clusters.cleanup_on_exit: true

  # ---

  e2e_perf_strict_limits:
    extends: [e2e_perf]
    tests.e2e.limits_equals_requests: true

  e2e_perf:
    extends: [e2e_gpu]
    tests.e2e.mode: single
    tests.e2e.limits_equals_requests: false
    matbench.workload: projects.kserve.visualizations.kserve-llm
    gpu.time_sharing.replicas: 1
    clusters.sutest.compute.machineset.count: 1

  e2e:
    tests.mode: e2e
    gpu.time_sharing.replicas: 1

  e2e_multi:
    extends: [e2e_gpu]
    tests.e2e.mode: multi
    gpu.time_sharing.replicas: 1
    matbench.lts.generate: false

  e2e_gpu:
    extends: [e2e]
    gpu.prepare_cluster: true
    clusters.sutest.compute.machineset.spot: false
    clusters.sutest.compute.machineset.type: g4dn.2xlarge

  e2e_all_models:
    tests.e2e.models:
    - flan-t5-small-cpu
    - flan-t5-small-gpu
    - flan-t5-large-gpu
    - bloom-560m
    - mpt-7b-instruct2

  raw:
    kserve.raw_deployment.enabled: true
    rhods.operator.stop: true # mandatory in 2.7

  tgis:
    kserve.inference_service.validation.method: fmaas.GenerationService/Generate
    kserve.inference_service.validation.proto: projects/kserve/testing/protos/tgis_generation.proto
    tests.e2e.llm_load_test.plugin: tgis_grpc_plugin

  quick_raw:
    extends: [raw, no_gpu, tgis]

    tests.e2e.models: [flan-t5-small-hf-cpu, flan-t5-small-hf-cpu]
    tests.e2e.llm_load_test.concurrency: 2

  single_raw:
    extends: [quick_raw]
    tests.e2e.models: [flan-t5-small-hf-cpu]

  # ---

  prepare_only:
    tests.mode: prepare_only

  cleanup_only:
    tests.mode: cleanup_only

  test_only:
    prepare.enabled: false
    prepare.cleanup.enabled: false

  gating_concurrency:
    tests.e2e.matbenchmark.enabled: true
    tests.e2e.llm_load_test.concurrency: [1, 2, 16, 32, 64, 96]

  gating_concurrency_light:
    tests.e2e.matbenchmark.enabled: true
    tests.e2e.llm_load_test.concurrency: [8, 16, 32]

  # ---

  3min:
    tests.e2e.llm_load_test.duration: 180

  5min:
    tests.e2e.llm_load_test.duration: 300

  10min:
    tests.e2e.llm_load_test.duration: 600

  # ---

  scale_test:
    tests.mode: scale
    matbench.workload: projects.kserve.visualizations.kserve-scale
    tests.prom_plot_workload: null

  scale_gpu:
    extends: [scale_test]
    gpu.prepare_cluster: true
    clusters.sutest.compute.machineset.spot: false
    clusters.sutest.compute.machineset.type: g4dn.2xlarge
    tests.scale.model.name: flan-t5-small-gpu
    gpu.time_sharing.replicas: 17

  # ---

  customize_smcp:
    kserve.customize.serverless.enabled: true
    kserve.customize.serverless.egress.limits.memory: 4Gi
    kserve.customize.serverless.ingress.limits.memory: 4Gi

  # ---

  use_intlab_os:
    matbench.lts.opensearch.index_prefix: "psap-rhoai."
    matbench.lts.opensearch.instance: intlab

  use_smoke_os:
    matbench.lts.opensearch.instance: smoke

  gating:
    # using the 'smoke' instance for the time being, the intlab instance ('use_intlab_os') is full
    # 'Validation Failed: 1: this action would add [8] total shards, but this cluster currently has [6000]/[6000] maximum shards open;')
    extends: [use_smoke_os]

  gating_rehearsal:
    extends: [use_smoke_os]

  # ---
  # Get models from integration lab minio instead of AWS S3
  use_intlab_minio:
    secrets.model_s3_cred: intlab-miniocred
    kserve.storage_config.region: "us-east-1"
    kserve.storage_config.endpoint: "minio.app.intlab.redhat.com"
    kserve.storage_config.use_https: "1"
    kserve.storage_config.verify_ssl: "0"

  # ---

  # single-model

  watsonx_single_model_gating_light:
    extends: [watsonx_single_model_gating, gating_rehearsal, use_intlab_minio]
    tests.e2e.models:
    - watsonx-japanese-llama-2-7b-instruct
    - watsonx-flan-t5-xl
    matbench.lts.horreum.test_name: null
    matbench.lts.opensearch.index: rhoai-kserve-single-light

  watsonx_single_model_gating:
    extends: [e2e_perf, gating, 3min, metal, tgis, raw, use_intlab_minio]
    tests.e2e.models:
    - watsonx-japanese-llama-2-7b-instruct
    - watsonx-flan-t5-xl
    - watsonx-flan-t5-xxl
    - watsonx-llama-2-13b-chat-hf
    - watsonx-llama-2-70b-chat-hf
    - watsonx-mpt-7b-instruct2
    - watsonx-codellama-34b-instruct-hf
    matbench.lts.horreum.test_name: RHOAI-kserve-single
    matbench.lts.opensearch.index: rhoai-kserve-single

  single_model_gating:
    extends: [gating, 10min, e2e_gpu, e2e_perf]
    tests.e2e.models: [flan-t5-small-gpu, flan-t5-large-gpu, bloom-560m, mpt-7b-instruct2]
    matbench.lts.horreum.test_name: RHOAI-kserve-single
    matbench.lts.opensearch.index: rhoai-kserve-single

  single_model_gating_light:
    extends: [single_model_gating, 5min, gating_rehearsal]

    tests.e2e.models: [flan-t5-small-gpu, flan-t5-large-gpu]
    matbench.lts.horreum.test_name: null
    matbench.lts.opensearch.index: rhoai-kserve-single-light

  # multi-model

  watsonx_multi_model_gating:
    extends: [gating, 5min, e2e_gpu, e2e_multi, tgis, raw]
    matbench.lts.horreum.test_name: RHOAI-watsonx-multi
    matbench.lts.opensearch.index: rhoai-watsonx-multi
    tests.e2e.models:
    - watsonx-starcoder
    - watsonx-mt0-xxl
    - watsonx-japanese-Llama-2-7b-instruct
    - watsonx-flan-t5-xl-1:
        name: watsonx-flan-t5-xl
        namespace: watsonx-e2e-flan-grouped
        serving_runtime:
          name: watsonx-flan-grouped
    - watsonx-flan-t5-xl-2:
        name: watsonx-flan-t5-xl
        namespace: watsonx-e2e-flan-grouped
        serving_runtime:
          name: watsonx-flan-grouped
    - watsonx-flan-t5-xxl:
        name: watsonx-flan-t5-xl
        namespace: watsonx-e2e-flan-grouped
        serving_runtime:
          name: watsonx-flan-grouped
    - watsonx-llama-2-13b-chat-hf
    - watsonx-llama-2-13b-chat-hf

  watsonx_multi_model_gating_light:
    extends: [watsonx_multi_model_gating, gating_rehearsal, 3min, no_gpu]
    tests.e2e.models:
    - flan-t5-small-hf-cpu
    - flan-t5-small-hf-cpu
    - flan-t5-small-hf-cpu-1:
        name: flan-t5-small-hf-cpu
        namespace: watsonx-e2e-grouped
        serving_runtime:
          name: flan-t5-small-hf-cpu
    - flan-t5-small-hf-cpu-2:
        name: flan-t5-small-hf-cpu
        namespace: watsonx-e2e-grouped
        serving_runtime:
          name: flan-t5-small-hf-cpu

  multi_model_gating:
    extends: [gating, 10min, e2e_gpu, e2e_multi]
    matbench.lts.horreum.test_name: RHOAI-kserve-multi
    tests.e2e.models:
    - flan-t5-large-gpu-1:
        name: flan-t5-large-gpu
    - flan-t5-small-gpu-1:
        name: flan-t5-small-gpu
    - bloom-560m-1:
        name: bloom-560m
    - mpt-7b-instruct2-1:
        name: mpt-7b-instruct2

    - flan-t5-large-gpu-2:
        name: flan-t5-large-gpu
    - flan-t5-small-gpu-2:
        name: flan-t5-small-gpu
    - bloom-560m-2:
        name: bloom-560m
    - mpt-7b-instruct2-2:
        name: mpt-7b-instruct2
    matbench.lts.opensearch.index: rhoai-kserve-multi

  multi_model_gating_light:
    extends: [multi_model_gating, 5min, gating_rehearsal]

    tests.e2e.models: [flan-t5-small-gpu, flan-t5-small-cpu]
    matbench.lts.horreum.test_name: null
    matbench.lts.opensearch.index: rhoai-kserve-multi-light

  longevity:
    tests.e2e.mode: longevity
    tests.e2e.llm_load_test.concurrency: 2

  multi_model_longevity:
    extends: [multi_model_gating, longevity]

  multi_day_longevity:
    extends: [multi_model_longevity]
    tests.e2e.longevity.repeat: 3
    tests.e2e.longevity.delay: 86400 # 24 hours

  # many-model

  many_model_gating:
    extends: [gating, scale_test, customize_smcp]
    matbench.lts.horreum.test_name: RHOAI-kserve-many

    tests.scale.model.replicas: 17
    tests.scale.namespace.replicas: 12
    gpu.time_sharing.replicas: 32

    tests.scale.model.name: flan-t5-small-gpu-light
    kserve.model.serving_runtime.mute_logs: true
    tests.scale.sleep_factor: 60
    tests.e2e.limits_equals_requests: false
    matbench.lts.opensearch.index: rhoai-kserve-many

  many_model_gating_light:
    extends: [many_model_gating, gating_rehearsal]

    tests.scale.sleep_factor: 1
    tests.scale.model.replicas: 2
    tests.scale.namespace.replicas: 2
    gpu.time_sharing.replicas: 10
    matbench.lts.horreum.test_name: null
    matbench.lts.opensearch.index: rhoai-kserve-many-light

secrets:
  dir:
    name: psap-ods-secret
    env_key: PSAP_ODS_SECRET_PATH
  # name of the file containing the properties of LDAP secrets
  s3_ldap_password_file: s3_ldap.passwords
  keep_cluster_password_file: get_cluster.password
  brew_registry_redhat_io_token_file: brew.registry.redhat.io.token
  model_s3_cred: .awscred
  kserve_model_secret_settings: watsonx-models.yaml
  opensearch_instances: opensearch.yaml
  aws_credentials: .awscred

clusters:
  create:
    type: single # can be: single, ocp, managed
    keep: false
    name_prefix: kserve-ci
    ocp:
      # list of tags to apply to the machineset when creating the cluster
      tags:
        TicketId: 229
        Project: PSAP/Watsonx/serving/scale/ci-dev
      deploy_cluster:
        target: cluster
      base_domain: psap.aws.rhperfscale.org
      version: 4.13.9
      region: us-west-2
      control_plane:
        type: m6a.xlarge
      workers:
        type: m6a.2xlarge
        count: 2
        spot: false

  sutest:
    is_metal: false
    lab:
      name: null
    compute:
      dedicated: true
      machineset:
        name: workload-pods
        type: g4dn.2xlarge
        count: null
        disk_size: 400 # GB for the root partition
        spot: false
        taint:
          key: only-workload-pods
          value: "yes"
          effect: NoSchedule

  driver:
    is_metal: false
    compute:
      dedicated: true
      machineset:
        name: test-pods
        type: m6i.2xlarge
        count: null
        spot: false
        taint:
          key: only-test-pods
          value: "yes"
          effect: NoSchedule
  cleanup_on_exit: false

rhods:
  catalog:
    image: brew.registry.redhat.io/rh-osbs/iib
    tag: 682686
    channel: fast
    version: 2.8.0
    version_name: rc1
  operator:
    # set to true to stop the RHODS operator
    stop: false
    customize:
      kserve:
        enabled: false
        cpu: 500m
        memory: 500Mi


prepare:
  enabled: true
  operators:
  - name: serverless-operator
    catalog: redhat-operators
    namespace: all
    cleanup:
      crds:
      - knativeservings.operator.knative.dev
      namespaces:
      - knative-eventing
      - knative-serving
  - name: servicemeshoperator
    catalog: redhat-operators
    namespace: all
    cleanup:
      crds:
      - servicemeshmemberrolls.maistra.io
      - servicemeshcontrolplanes.maistra.io
      namespaces:
      - istio-system
  cleanup:
    enabled: true
    crds:
    - InferenceService
    - ServingRuntime

base_image:
  namespace: kserve-user-test-driver
  imagestream: topsail
  repo:
    url: https://github.com/openshift-psap/topsail/
    tag: main
    ref: main
    ref_prefer_pr: true
    dockerfile_path: build/Dockerfile
  extend:
    enabled: true
    local_dockerfile_path: projects/kserve/testing/images/Containerfile.e2e_test_user
    tag: e2e-test-user
  user:
    service_account: ci-artifacts
    role: cluster-admin
  minio:
    bucket_name: kserve-test-bucket

kserve:
  sa_name: sa
  raw_deployment:
    enabled: false
  storage_config:
    name: storage-config
    region: us-east-1
    endpoint: s3.amazonaws.com
    use_https: 1
    verify_ssl: 1

  inference_service:
    validation:
      dataset: projects/llm_load_test/llm-load-test/datasets/openorca_large_subset_010.jsonl
      query_count: 10
      method: caikit.runtime.Nlp.NlpService/TextGenerationTaskPredict
      proto: null
  model:
    serving_runtime:
      mute_logs: true
      kserve:
        image: quay.io/opendatahub/text-generation-inference:fast
      transformer:
        image: quay.io/opendatahub/caikit-tgis-serving:fast
  customize:
    serverless:
      enabled: false
      egress:
        limits:
          memory: 4Gi
      ingress:
        limits:
          memory: 4Gi
gpu:
  prepare_cluster: true
  time_sharing:
    replicas: 1

tests:
  mode: e2e

  dry_mode: false
  visualize: true
  capture_prom: true
  prom_plot_workload: projects.kserve.visualizations.kserve-prom
  scale:
    sleep_factor: 1
    namespace:
      name: watsonx-scale-test
      label: topsail.scale-test=true
      replicas: 2

    model:
      consolidated: false
      name: flan-t5-small-cpu
      format: caikit
      replicas: 2
  e2e:
    namespace: watsonx-e2e
    mode: single # single, longevity or multi
    models:
    - flan-t5-small-gpu
    - flan-t5-small-gpu
    consolidated_models: {} # will be filled at runtime
    limits_equals_requests: false
    delete_others: true
    validate_model: true
    capture_state: true
    matbenchmark:
      enabled: false
      stop_on_error: true
    llm_load_test:
      enabled: true
      src_path: projects/llm_load_test/llm-load-test/
      duration: 60
      concurrency: 16
      plugin: caikit_client_plugin
      interface: grpc
    longevity:
      repeat: 2
      delay: 30 # seconds

matbench:
  preset: null
  workload: projects.kserve.visualizations.kserve-llm
  config_file: plots.yaml
  download:
    mode: prefer_cache
    url:
    url_file:
    # if true, copy the results downloaded by `matbench download` into the artifacts directory
    save_to_artifacts: false
  ignore_exit_code: true
  # directory to plot. Set by topsail/testing/visualize.py before launching the visualization
  test_directory: null
  lts:
    generate: true
    horreum:
      test_name: null
    opensearch:
      export: true
      fail_test_on_fail: false
      instance: smoke
      index: rhoai-kserve-single
      index_prefix: ""
    regression_analyses:
      enabled: false
      # if the regression analyses fail, mark the test as failed
      fail_test_on_regression: true
export_artifacts:
  enabled: false
  bucket: rhoai-cpt-artifacts
  path_prefix: cpt/kserve
  dest: null # will be set by the export code
