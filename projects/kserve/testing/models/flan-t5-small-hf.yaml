extends: base

serving_runtime:
  single_container: true
  kserve:
    resource_request:
      cpu: 1
      memory: 2 # in Gi
      nvidia.com/gpu: 1
      nvidia.com/gpu_memory: 1 # in Gi of GPU memory
    extra_env: {}
inference_service:
  storage_uri: "s3://psap-hf-models/flan-t5-small/flan-t5-small"
  model_format: pytorch
