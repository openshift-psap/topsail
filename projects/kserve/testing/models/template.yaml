extends: base
secret_key: <key in watsonx-models.yaml>

serving_runtime:
  kserve:
    resource_request:
      cpu: 0
      memory: 0 # in Gi
      nvidia.com/gpu: 0
      nvidia.com/gpu_memory: 0 # in Gi of GPU memory
    extra_env:
      key: value
inference_service:
  model_format: caikit/pytorch/...
  storage_uri: "s3://<bucket>/<path>"
  min_replicas: 1
testing:
  size: small # see the keys of tests.e2e.llm_load_test.dataset_size
  max_concurrency: 32
