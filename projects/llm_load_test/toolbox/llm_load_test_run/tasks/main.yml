---
- name: Create the src directory
  file:
    path: "{{ artifact_extra_logs_dir }}/src"
    state: directory
    mode: '0755'

- name: Create the output directory
  file:
    path: "{{ artifact_extra_logs_dir }}/output"
    state: directory
    mode: '0755'

- name: Create the logs directory
  file:
    path: "{{ artifact_extra_logs_dir }}/logs"
    state: directory
    mode: '0755'

- name: Apply the llm-load-test config.yaml template
  template:
    src: "{{ llm_load_test_config_template }}"
    dest: "{{ artifact_extra_logs_dir }}/src/llm_load_test.config.yaml"
    mode: '0400'

- name: Show the start time
  command:
    date +%H:%M:%S
  register: start_time_cmd

- name: Inform | Next task runs the load test
  debug:
    msg: |
      Next task runs the load test.
      It takes {{ llm_load_test_run_duration }}s to complete.
      Starting at {{ start_time_cmd.stdout }}.
      Artifacts will be saved into '{{ artifact_extra_logs_dir }}/output'.
      Configuration file is {{ artifact_extra_logs_dir }}/src/llm_load_test.config.yaml

- name: Run llm-load-test and update the response
  block:
  - name: Run llm-load-test
    shell: |
      set -e
      cd "{{ llm_load_test_run_src_path }}"
      export OPENBLAS_NUM_THREADS=1 # https://github.com/OpenMathLib/OpenBLAS/wiki/Faq#how-can-i-use-openblas-in-multi-threaded-applications

      if ! which timeout &>/dev/null; then
        echo "no timeout on mac ..."
        timeout=""
      else
        timeout="timeout $(({{ llm_load_test_run_duration }} + 25*60))"
      fi

      time $timeout {{ llm_load_test_run_python_cmd }} load_test.py --config "{{ artifact_extra_logs_dir }}/src/llm_load_test.config.yaml" &> {{ artifact_extra_logs_dir }}/logs/run.log
    register: llm_load_test_cmd
  always:
  - name: Check if llm-load-test run timed-out
    when: llm_load_test_cmd.rc == 124
    command:
      touch {{ artifact_extra_logs_dir }}/TIMEOUT

  - name: Ensure that some content has been generated
    shell:
      set -o pipefail;
      cat {{ artifact_extra_logs_dir }}/output/output.json | grep . --quiet
    register: llm_load_test_has_content
    failed_when: false

  - name: Fail if llm-load-test did not generate content
    fail: msg="llm-load-test did not generate content. See logs in '{{ artifact_extra_logs_dir }}/logs/run.log'"
    when: llm_load_test_has_content.rc == 1

  - name: Get the number of requests that succeeded
    shell:
      set -o pipefail;
      cat {{ artifact_extra_logs_dir }}/output/output.json | jq '.summary.total_failures == .summary.total_requests'
    register: llm_load_test_no_success

  - name: Fail if no request succeeded
    fail: msg="llm-load-test calls did not succeed. See logs in '{{ artifact_extra_logs_dir }}/logs/run.log'"
    when: llm_load_test_no_success.stdout == "true"
