{% set secrets_location = false | or_env(secrets.dir.env_key) %}
{% if not secrets_location %}
  {{ ("ERROR: secrets_location must be defined (secrets.dir.name="+ secrets.dir.name|string +" or env(secrets.dir.env_key=" + secrets.dir.env_key|string + ")) ") | raise_exception }}
{% endif %}
{% set s3_ldap_password_location = secrets_location + "/" + secrets.s3_ldap_password_file %}

# ---

sutest/cluster set_scale:
  name: {{ clusters.sutest.compute.machineset.name }}
  instance_type: {{ clusters.sutest.compute.machineset.type }}
{% if clusters.sutest.compute.dedicated %}
  taint: {{ clusters.sutest.compute.machineset.taint.key }}={{ clusters.sutest.compute.machineset.taint.value }}:{{ clusters.sutest.compute.machineset.taint.effect }}
{% endif %}
  disk_size: {{ clusters.sutest.compute.machineset.disk_size }}
  spot: {{ clusters.sutest.compute.machineset.spot }}
  scale: SET_AT_RUNTIME


gpu_operator enable_time_sharing:
  replicas: {{ gpu.time_sharing.replicas }}

sutest/cluster preload_image/kserve-runtime:
  namespace: "{{ tests.fine_tuning.namespace }}"
  name: SET_AT_RUNTIME
  image: SET_AT_RUNTIME

  node_selector_key: {{ clusters.sutest.compute.machineset.taint.key }}
  node_selector_value: "{{ clusters.sutest.compute.machineset.taint.value }}"
  pod_toleration_effect: {{ clusters.sutest.compute.machineset.taint.effect }}
  pod_toleration_key: {{ clusters.sutest.compute.machineset.taint.key }}

#
# deploy RHODS
#

rhods deploy_ods:
  catalog_image: {{ rhods.catalog.image }}
  tag: {{ rhods.catalog.tag }}
  channel: {{ rhods.catalog.channel }}
  version: {{ rhods.catalog.version }}
  opendatahub: {{ rhods.catalog.opendatahub }}
  managed_rhoai: {{ rhods.catalog.managed_rhoai }}

# ---

sutest/cluster set_project_annotation/scale_test_node_selector:
  key: openshift.io/node-selector
  value: "{{ clusters.sutest.compute.machineset.taint.key }}={{ clusters.sutest.compute.machineset.taint.value }}"

sutest/cluster set_project_annotation/scale_test_toleration:
  key: scheduler.alpha.kubernetes.io/defaultTolerations
  value: '[{\"operator\": \"Exists\", \"effect\": \"{{ clusters.sutest.compute.machineset.taint.effect }}\", \"key\": \"{{ clusters.sutest.compute.machineset.taint.key }}\"}]'

fine_tuning run_fine_tuning_job:
  name: {{ tests.fine_tuning.test_settings.job_name }}
  namespace: "{{ tests.fine_tuning.namespace }}"
  pvc_name: "{{ fine_tuning.pvc.name }}"
  container_image: "{{ fine_tuning.image }}"

  model_name: {{ tests.fine_tuning.test_settings.model_name }}
  dataset_name: {{ tests.fine_tuning.test_settings.dataset_name }}
  dataset_replication: {{ tests.fine_tuning.test_settings.dataset_replication }}

fine_tuning run_quality_evaluation:
  name: {{ tests.fine_tuning.test_settings.job_name }}
  namespace: "{{ tests.fine_tuning.namespace }}"
  pvc_name: "{{ fine_tuning.pvc.name }}"
  container_image: "{{ fine_tuning.image }}"

  model_name: {{ tests.fine_tuning.test_settings.model_name }}

cluster download_to_pvc:
  name: SET_AT_RUNTIME

  pvc_name: "{{ fine_tuning.pvc.name }}"
  pvc_access_mode: "{{ fine_tuning.pvc.access_mode }}"
  pvc_size: "{{ fine_tuning.pvc.size }}"
  namespace: "{{ tests.fine_tuning.namespace }}"

  source: SET_AT_RUNTIME
  storage_dir: SET_AT_RUNTIME


cluster reset_prometheus_db/uwm:
  label: app.kubernetes.io/instance=user-workload,app.kubernetes.io/component=prometheus
  namespace: openshift-user-workload-monitoring

cluster dump_prometheus_db/uwm:
  label: app.kubernetes.io/instance=user-workload,app.kubernetes.io/component=prometheus
  namespace: openshift-user-workload-monitoring

# many model test

scheduler generate_load:
  namespace: "{{ tests.fine_tuning.namespace }}"
  base_name: "many-model-fine-tuning"
  mode: kueue
  count: {{ tests.fine_tuning.many_model.count }}
  timespan: {{ tests.fine_tuning.many_model.timespan }}
  pod_requests: null
  pod_runtime: null
  pod_count: null
  resource_kind: pytorchjob
