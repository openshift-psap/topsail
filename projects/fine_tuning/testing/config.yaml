ci_presets:
  # name of the presets to apply, or null if no preset
  name: null
  # list of names of the presets to apply, or a single name, or null if no preset
  names: null

  single:
    clusters.create.type: single

  keep:
    clusters.create.keep: true
    clusters.create.ocp.tags.Project: PSAP/Project/FineTuning
    # clusters.create.ocp.tags.TicketId:

  light_cluster:
    clusters.create.ocp.deploy_cluster.target: cluster_light

  light:
    extends: [light_cluster]
    tests.fine_tuning.matbenchmarking.enabled: false
    tests.fine_tuning.test_settings.gpu: null
    tests.fine_tuning.test_settings.dataset_replication: 1
    tests.fine_tuning.test_settings.dataset_name: twitter_complaints_small.json
    tests.fine_tuning.test_settings.model_name: bigscience/bloom-560m@hf

  gpu:
    gpu.prepare_cluster: true
    clusters.sutest.compute.machineset.type: g4dn.2xlarge
  # ---

  dgx_single_model_multi_dataset:
    extends: [dgx_single_model]
    tests.fine_tuning.matbenchmarking.enabled: true
    tests.fine_tuning.test_settings.gpu: 1
    tests.fine_tuning.test_settings.dataset_replication: [1, 2, 4, 8]

  dgx_single_full:
    tests.fine_tuning.test_settings.model_name: bigcode/gpt_bigcode-santacoder@hf
    tests.fine_tuning.test_settings.dataset_name: alpaca_data.json
    tests.fine_tuning.test_settings.gpu: [1, 2, 4, 8]
    tests.fine_tuning.test_settings.hyper_parameters.per_device_train_batch_size: [8, 10, 12, 14, 16, 18]
    tests.fine_tuning.test_settings.hyper_parameters.max_seq_length: [128, 256, 512]
    tests.fine_tuning.test_settings.hyper_parameters.num_train_epochs: 2
    tests.fine_tuning.matbenchmarking.enabled: true
    tests.fine_tuning.matbenchmarking.stop_on_error: false

  dgx_hyper:
    tests.fine_tuning.test_settings.hyper_parameters:
      use_flash_attn: false

  dgx_single_model_multi_gpu:
    extends: [dgx_single_model]
    tests.fine_tuning.matbenchmarking.enabled: true
    tests.fine_tuning.test_settings.gpu: [1, 2, 4, 6, 8]

  dgx_single_model:
    tests.fine_tuning.test_settings.model_name: bigcode/gpt_bigcode-santacoder@hf
    tests.fine_tuning.test_settings.dataset_name: alpaca_data.json
    tests.fine_tuning.test_settings.gpu: 1

  multi_model:
    tests.fine_tuning.multi_model.enabled: true
    tests.fine_tuning.test_settings.model_name: null

  ibm_release_comparison:
    tests.fine_tuning.test_settings.container_image:
    - quay.io/modh/fms-hf-tuning:release-ec50c3d7dc09f50d9885f25efc3d2fc98a379709 # RHOAI-2.12
    - quay.io/modh/fms-hf-tuning:release-5e4e9441febdb5b2beb21eaecdda1103abd1db05 # RHOAI-2.11
    - quay.io/modh/fms-hf-tuning:release-7a8ff0f4114ba43398d34fd976f6b17bb1f665f3 # RHOAI-2.10

  hf_evaluation:
    fine_tuning.pvc.size: 2000Gi
    tests.fine_tuning.matbenchmarking.enabled: true
    fine_tuning.model_registry: hf
    tests.fine_tuning.test_settings.dataset_name: alpaca_data.json
    tests.fine_tuning.test_settings.dataset_replication: 0.1
    tests.fine_tuning.matbenchmarking.stop_on_error: false

  ibm_regression:
    extends: [hf_evaluation, dgx_small_footprint]
    tests.fine_tuning.test_settings.model_name:
    - ibm-granite/granite-3b-code-instruct
    - ibm/merlinite-7b
    - meta-llama/Llama-2-13b-chat-hf
    - ibm-granite/granite-20b-code-instruct
    - ibm-granite/granite-34b-code-instruct
    tests.fine_tuning.test_settings.gpu: [2, 4, 8]
    tests.fine_tuning.test_settings.container_image:
    - quay.io/modh/fms-hf-tuning:main-5e965e4676bff71f0c4e8219a59aba37ce542083 # 4.42
    - quay.io/modh/fms-hf-tuning:main-abbb2e2dfac0f92a34714147f3bd4696758037c6 # 4.41
    - quay.io/modh/fms-hf-tuning:release-5e4e9441febdb5b2beb21eaecdda1103abd1db05 # RHOAI 2.11 image

  dgx_small_footprint:
    tests.fine_tuning.test_settings.gpu: 8
    # -- #
    tests.fine_tuning.test_settings.hyper_parameters.per_device_train_batch_size: 1
    tests.fine_tuning.test_settings.hyper_parameters.gradient_accumulation_steps: 1
    tests.fine_tuning.test_settings.hyper_parameters.peft_method: "none"
    tests.fine_tuning.test_settings.hyper_parameters.max_seq_length: 1024
    tests.fine_tuning.test_settings.hyper_parameters.use_flash_attn: true

  ibm_40gb_models:
    extends: [hf_evaluation, dgx_small_footprint]
    tests.fine_tuning.test_extra_settings:
    - name: granite-family
      model_name:
      - ibm-granite/granite-3b-code-instruct
      - instructlab/granite-7b-lab
      - ibm-granite/granite-8b-code-base
      - ibm-granite/granite-8b-code-instruct
      hyper_parameters:
        per_device_train_batch_size: 1
    - name: llama-family
      model_name:
      - meta-llama/Meta-Llama-3-8B-Instruct
      - meta-llama/llama-2-7b-hf
      - meta-llama/Llama-2-13b-chat-hf
      hyper_parameters:
        per_device_train_batch_size: [1, 2]

  ibm_80gb_models:
    extends: [hf_evaluation, dgx_small_footprint]
    tests.fine_tuning.test_settings.model_name:
    - ibm-granite/granite-34b-code-instruct
    - mistralai/Mixtral-8x7B-Instruct-v0.1

  ibm_lora_models:
    extends: [hf_evaluation, dgx_small_footprint]
    tests.fine_tuning.test_settings.model_name: null # for consistency. Set in 'test_extra_settings'
    tests.fine_tuning.test_settings.dataset_name: news-tokens-16384plus-entries-4096.jsonl
    tests.fine_tuning.test_settings.dataset_replication: 1
    tests.fine_tuning.test_settings.gpu: [2, 4]
    tests.fine_tuning.test_settings.hyper_parameters:
      peft_method: "lora"
      torch_dtype: bfloat16
      use_flash_attn: true
      max_steps: -1
      per_device_train_batch_size: [2, 16]
      max_seq_length: [512, 2048]
      gradient_accumulation_steps: 4
      save_strategy: "no"
      warmup_ratio: 0.03
      num_train_epochs: 1
      gradient_checkpointing: true
      packing: false
      r: 4
      lora_alpha: 16
    tests.fine_tuning.test_extra_settings:
    - name: llama-13b
      model_name:
      - meta-llama/Llama-2-13b-hf
      hyper_parameters:
        raw_lists: {target_modules: ["q_proj", "k_proj"]}
    - name: granite-20b-v2
      model_name:
      - ibm-granite/granite-20b-code-base
      hyper_parameters:
        raw_lists: {target_modules: ["c_attn", "c_proj"]}

  dgx_multi_model_8:
    extends: [multi_model]
    fine_tuning.model_registry: hf
    tests.fine_tuning.multi_model.models:
    - name: bigcode/gpt_bigcode-santacoder
      replicas: 8
    tests.fine_tuning.test_settings.gpu: 1
    tests.fine_tuning.test_settings.dataset_name: alpaca_data.json

  dgx_multi_model_4:
    extends: [dgx_multi_model_8]
    tests.fine_tuning.multi_model.models:
    - name: bigcode/gpt_bigcode-santacoder
      replicas: 4
    tests.fine_tuning.test_settings.gpu: 2

  dgx_multi_model_2:
    extends: [dgx_multi_model_8]
    tests.fine_tuning.multi_model.models:
    - name: bigcode/gpt_bigcode-santacoder
      replicas: 2
    tests.fine_tuning.test_settings.gpu: 4

  many_model:
    tests.prom_plot_workload: null
    tests.fine_tuning.many_model.enabled: true
    matbench.workload: projects.scheduler.visualizations.schedulers

  dgx_many_model:
    extends: [many_model]

    tests.fine_tuning.test_settings.gpu: 2
    tests.fine_tuning.test_settings.dataset_name: alpaca_data.json
    fine_tuning.model_registry: hf
    tests.fine_tuning.test_settings.model_name: bigcode/gpt_bigcode-santacoder
    tests.fine_tuning.many_model.count: 64
    tests.fine_tuning.many_model.timespan: 64

  # ---

  multi_node_training:
    tests.fine_tuning.test_settings.worker_replicas: 1
    fine_tuning.pvc.access_mode: ReadWriteMany

  # ---

  metal:
    clusters.sutest.is_metal: true
    clusters.driver.is_metal: true
    clusters.sutest.compute.dedicated: false
    clusters.driver.compute.dedicated: false

  not_metal:
    clusters.sutest.is_metal: false
    clusters.driver.is_metal: false

  use_intlab_os:
    matbench.lts.opensearch.index_prefix: "psap-rhoai."
    matbench.lts.opensearch.instance: intlab

  use_smoke_os:
    matbench.lts.opensearch.instance: smoke

  # ---

  quality_evaluation:
    tests.fine_tuning.quality_evaluation.enabled: true
    tests.capture_prom: false
    matbench.workload: projects.fine_tuning.visualizations.quality_evaluation
    matbench.lts.generate: false
    tests.capture_state: false
    tests.fine_tuning.test_settings.name: quality-evaluation
  # ---

  cluster_ibm_dgx:
    extends: ["metal"]
    clusters.sutest.compute.machineset.type: "IBM-DGX A100-80GB"
    tests.fine_tuning.namespace: rhoai-tuning-test
    fine_tuning.pvc.name: rhoai-tuning-test
    tests.capture_prom: with-queries

  cluster_dgx:
    clusters.sutest.compute.machineset.type: "DGX A100-40GB"

  cluster_icelake:
    clusters.sutest.compute.machineset.type: "Icelake"

  cluster_a30:
    clusters.sutest.compute.machineset.type: "Cluster A30-24GB"
secrets:
  dir:
    name: psap-ods-secret
    env_key: PSAP_ODS_SECRET_PATH
  # name of the file containing the properties of LDAP secrets
  s3_ldap_password_file: s3_ldap.passwords
  keep_cluster_password_file: get_cluster.password
  brew_registry_redhat_io_token_file: brew.registry.redhat.io.token
  opensearch_instances: opensearch.yaml
  aws_credentials: .awscred
  git_credentials: git-credentials
clusters:
  metal_profiles:
    p42-h03-dgx.rdu3.labs.perfscale.redhat.com: cluster_dgx
    e26-h23-000-r650: cluster_icelake
    cc37-h13-000-r750.rdu3.labs.perfscale.redhat.com: cluster_a30
    fmaas-devstage-backen-9cjgj-worker-a100-il-rdma-3-nc7c8: cluster_ibm_dgx
  create:
    type: single # can be: single, ocp, managed
    keep: false
    name_prefix: fine-tuning-ci
    ocp:
      # list of tags to apply to the machineset when creating the cluster
      tags:
        # TicketId: "..."
        Project: PSAP/Project/FineTuning
      deploy_cluster:
        target: cluster
      base_domain: psap.aws.rhperfscale.org
      version: 4.15.9
      region: us-west-2
      control_plane:
        type: m6a.xlarge
      workers:
        type: m6a.2xlarge
        count: 2

  sutest:
    is_metal: false
    lab:
      name: null
    compute:
      dedicated: true
      machineset:
        name: workload-pods
        type: m6i.2xlarge
        count: null
        taint:
          key: only-workload-pods
          value: "yes"
          effect: NoSchedule
  driver:
    is_metal: false
    compute:
      dedicated: true
      machineset:
        name: test-pods
        count: null
        type: m6i.2xlarge
        taint:
          key: only-test-pods
          value: "yes"
          effect: NoSchedule
  cleanup_on_exit: false

rhods:
  catalog:
    image: brew.registry.redhat.io/rh-osbs/iib
    tag: 757482
    channel: fast
    version: 2.11.0
    version_name: rc3
    opendatahub: false
    managed_rhoi: true
  operator:
    # set to true to stop the RHODS operator
    stop: false

gpu:
  prepare_cluster: false
  time_sharing:
    replicas: 1

fine_tuning:
  image: "quay.io/modh/fms-hf-tuning:release-ec50c3d7dc09f50d9885f25efc3d2fc98a379709" # RHOAI 2.12 image
  pvc:
    name: fine-tuning-storage
    access_mode: ReadWriteOnce
    size: 80Gi
  model_registry: null # if set to a fine_tuning_sources.* model registry, all the lookups will be done in this registry
  sources:
    hf:
      type: model-registry
      registry_type: model
      source_dir: 'https://huggingface.co/'
      secret_key: secrets.git_credentials

    twitter_complaints_small.json:
      type: dataset
      source_dir: 'https://raw.githubusercontent.com/foundation-model-stack/fms-hf-tuning/b48249fab3df124d6b85cc8ce59b9e5a66ea6dcb/tests/data'

    synthetic_dataset.txt:
      type: dataset
      source_dir: https://raw.githubusercontent.com/openshift-psap/topsail/main/projects/fine_tuning/roles/fine_tuning_run_fine_tuning_job/files/entrypoint/
      transform: generate_synthetic_dataset.py

    news-tokens-16384plus-entries-4096.jsonl:
      type: dataset
      source_dir: /manually/populated
      response_template: "\n### Response:"

    alpaca_data.json:
      type: dataset
      source_dir: 'https://raw.githubusercontent.com/gururise/AlpacaDataCleaned/main'
      transform: convert_alpaca.py

    bloom-560m:
      type: model
      source_dir: 's3://psap-watsonx-models/fine-tuning/bigscience'
      secret_key: secrets.aws_credentials
tests:
  capture_prom: true
  capture_prom_uwm: false
  capture_state: true
  prom_plot_workload: projects.fine_tuning.visualizations.fine_tuning_prom
  visualize: true
  dry_mode: false
  fine_tuning:
    namespace: fine-tuning-testing
    matbenchmarking:
      enabled: false
      visu_file: plots.yaml
      stop_on_error: true
    multi_model:
      enabled: false
      models:
      - name: bloom-560m
        replicas: 2
    test_settings:
      name: fine-tuning
      model_name: bigscience/bloom-560m@hf
      dataset_name: twitter_complaints_small.json
      gpu: 1
      dataset_replication: 1
      worker_replicas: null
      container_image: null
      # ---
      # https://huggingface.co/transformers/v3.0.2/main_classes/trainer.html
      hyper_parameters:
        # hyper-parameters need to be listed here to allow setting their value in the presets
        per_device_train_batch_size: null
        gradient_accumulation_steps: null
        num_train_epochs: null
        max_seq_length: null
        peft_method: null
        use_flash_attn: null
        # LoRA hyper-parameters:
        r: null
        lora_dropout: null
        lora_alpha: null
        target_modules: null
    test_extra_settings: {} # fine-grain configuration when matbenchmarking
    many_model:
      enabled: false
      count: 20
      kueue_name: local-queue
      timespan: 0
    quality_evaluation:
      enabled: false
      image: registry.redhat.io/ubi9
matbench:
  preset: null
  workload: projects.fine_tuning.visualizations.fine_tuning
  config_file: plots.yaml
  download:
    mode: prefer_cache
    url:
    url_file:
    # if true, copy the results downloaded by `matbench download` into the artifacts directory
    save_to_artifacts: false
  ignore_exit_code: true
  # directory to plot. Set by testing/common/visualize.py before launching the visualization
  test_directory: null
  lts:
    generate: true
    horreum:
      test_name: null
    opensearch:
      export:
        enabled: false
        enabled_on_replot: false
        fail_test_on_fail: true
      fail_test_on_fail: false
      instance: smoke
      index: topsail-fine-tuning
      index_prefix: ""
    regression_analyses:
      enabled: false
      # if the regression analyses fail, mark the test as failed
      fail_test_on_regression: true
export_artifacts:
  enabled: false
  bucket: rhoai-cpt-artifacts
  path_prefix: cpt/fine-tuning
  dest: null # will be set by the export code
