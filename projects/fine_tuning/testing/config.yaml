ci_presets:
  # name of the presets to apply, or null if no preset
  name: null
  # list of names of the presets to apply, or a single name, or null if no preset
  names: null

  single:
    clusters.create.type: single

  keep:
    clusters.create.keep: true
    clusters.create.ocp.tags.Project: PSAP/Project/FineTuning
    # clusters.create.ocp.tags.TicketId:

  light_cluster:
    clusters.create.ocp.deploy_cluster.target: cluster_light

  light:
    extends: [light_cluster]
    clusters.sutest.compute.dedicated: false
    tests.fine_tuning.matbenchmarking.enabled: false
    tests.fine_tuning.test_settings.gpu: null
    tests.fine_tuning.test_settings.dataset_replication: 1
    tests.fine_tuning.test_settings.dataset_name: twitter_complaints_small.json
    tests.fine_tuning.test_settings.model_name: bigscience/bloom-560m@hf

  gpu:
    gpu.prepare_cluster: true
    clusters.sutest.compute.machineset.type: g4dn.2xlarge
    clusters.sutest.compute.machineset.count: 1
    tests.fine_tuning.test_settings.gpu: 1

  # ---

  dgx_single_model_multi_dataset:
    extends: [fms, dgx_single_model]
    tests.fine_tuning.matbenchmarking.enabled: true
    tests.fine_tuning.test_settings.gpu: 1
    tests.fine_tuning.test_settings.dataset_replication: [1, 2, 4, 8]

  dgx_single_full:
    extends: [fms]
    tests.fine_tuning.test_settings.model_name: bigcode/gpt_bigcode-santacoder@hf
    tests.fine_tuning.test_settings.dataset_name: alpaca_data.json
    tests.fine_tuning.test_settings.gpu: [1, 2, 4, 8]
    tests.fine_tuning.test_settings.hyper_parameters.per_device_train_batch_size: [8, 10, 12, 14, 16, 18]
    tests.fine_tuning.test_settings.hyper_parameters.max_seq_length: [128, 256, 512]
    tests.fine_tuning.test_settings.hyper_parameters.num_train_epochs: 2
    tests.fine_tuning.matbenchmarking.enabled: true
    tests.fine_tuning.matbenchmarking.stop_on_error: false

  dgx_single_model_multi_gpu:
    extends: [dgx_single_model]
    tests.fine_tuning.matbenchmarking.enabled: true
    tests.fine_tuning.test_settings.gpu: [1, 2, 4, 6, 8]

  dgx_single_model:
    tests.fine_tuning.test_settings.model_name: bigcode/gpt_bigcode-santacoder@hf
    tests.fine_tuning.test_settings.dataset_name: alpaca_data.json
    tests.fine_tuning.test_settings.gpu: 1

  multi_model:
    tests.fine_tuning.multi_model.enabled: true
    tests.fine_tuning.test_settings.model_name: null

  hf_evaluation:
    fine_tuning.pvc.size: 2000Gi
    tests.fine_tuning.matbenchmarking.enabled: true
    fine_tuning.model_registry: hf
    tests.fine_tuning.test_settings.dataset_name: alpaca_data.json
    tests.fine_tuning.test_settings.dataset_replication: 0.1
    tests.fine_tuning.matbenchmarking.stop_on_error: false

  dgx_small_footprint:
    extends: [fms]

    tests.fine_tuning.test_settings.gpu: 8
    # -- #
    tests.fine_tuning.test_settings.hyper_parameters.per_device_train_batch_size: 1
    tests.fine_tuning.test_settings.hyper_parameters.gradient_accumulation_steps: 1
    tests.fine_tuning.test_settings.hyper_parameters.peft_method: "none"
    tests.fine_tuning.test_settings.hyper_parameters.max_seq_length: 1024
    tests.fine_tuning.test_settings.hyper_parameters.use_flash_attn: true

  gating_dgx40gb_full:
    extends: [fms, gating]

    fine_tuning.pvc.size: 2000Gi
    fine_tuning.model_registry: hf

    tests.fine_tuning.matbenchmarking.enabled: true
    tests.fine_tuning.matbenchmarking.stop_on_error: false

    tests.fine_tuning.test_settings.dataset_name: alpaca_data.json
    tests.fine_tuning.test_settings.dataset_replication: 0.2

    tests.fine_tuning.test_settings.gpu: 4

    tests.fine_tuning.test_settings.hyper_parameters.gradient_accumulation_steps: 1
    tests.fine_tuning.test_settings.hyper_parameters.peft_method: "none"
    tests.fine_tuning.test_settings.hyper_parameters.max_seq_length: 512
    tests.fine_tuning.test_settings.hyper_parameters.per_device_train_batch_size: 1
    tests.fine_tuning.test_settings.hyper_parameters.use_flash_attn: true

    tests.fine_tuning.test_extra_settings:
    - model_name:
      - meta-llama/Llama-2-13b-hf

    - model_name:
      - meta-llama/Meta-Llama-3.1-70B
      hyper_parameters.skip_if: {peft_method: "none"}

    - model_name:
      - ibm-granite/granite-3b-code-instruct
      - instructlab/granite-7b-lab
      - ibm-granite/granite-8b-code-base

      - meta-llama/Meta-Llama-3.1-8B

      - mistralai/Mistral-7B-v0.3

    - model_name:
      - mistralai/Mixtral-8x7B-v0.1
      hyper_parameters.skip_if: {peft_method: "none"}

  ibm_80gb_models:
    extends: [fms, hf_evaluation, dgx_small_footprint]
    tests.fine_tuning.test_settings.model_name:
    - ibm-granite/granite-34b-code-instruct
    - mistralai/Mixtral-8x7B-Instruct-v0.1

  gating_dgx40gb_lora:
    extends: [fms, gating_dgx40gb_full]

    tests.fine_tuning.test_settings.hyper_parameters.peft_method: "lora"
    tests.fine_tuning.test_settings.hyper_parameters.r: 4
    tests.fine_tuning.test_settings.hyper_parameters.lora_alpha: 16
    tests.fine_tuning.test_settings.hyper_parameters.raw_lists: {target_modules: ["q_proj", "k_proj"]}


  ibm_lora_qlora_models:
    extends: [fms]

    fine_tuning.pvc.size: 2000Gi
    tests.fine_tuning.matbenchmarking.enabled: true
    tests.fine_tuning.matbenchmarking.stop_on_error: false
    tests.fine_tuning.test_settings.name: qlora
    tests.fine_tuning.test_settings.model_name: null
    tests.fine_tuning.test_settings.dataset_name: alpaca_data.json
    tests.fine_tuning.test_settings.dataset_replication: 0.5
    tests.fine_tuning.test_settings.gpu: 4

    tests.fine_tuning.test_settings.hyper_parameters:
      peft_method: "lora"
      use_flash_attn: true
      max_steps: -1
      per_device_train_batch_size: [16, 24, 32, 40, 48]
      max_seq_length: 1024
      gradient_accumulation_steps: 4
      warmup_ratio: 0.03
      num_train_epochs: 1
      gradient_checkpointing: true
      packing: false
      r: 4
      lora_alpha: 16

    tests.fine_tuning.test_extra_settings:
    # QLoRA
    - model_name: rhoai/mistral-7b-v0.3-gptq@dmf
      hyper_parameters.raw_lists:
        auto_gptq: ["triton_v2"]
        target_modules: ["all-linear"]
      hyper_parameters.torch_dtype: "float16"
      hyper_parameters.fp16: true

    # LoRA
    - model_name: mistralai/Mistral-7B-v0.3@hf
      hyper_parameters.raw_lists:
        target_modules: ["all-linear"]

  gating_dgx40gb_qlora:
    extends: [fms, gating]
    fine_tuning.pvc.size: 2000Gi
    tests.fine_tuning.matbenchmarking.enabled: true
    tests.fine_tuning.matbenchmarking.stop_on_error: false
    tests.fine_tuning.test_settings.name: qlora
    tests.fine_tuning.test_settings.model_name:
    - rhoai/mistral-7b-v0.3-gptq
    - rhoai/granite-8b-code-instruct-gptq
    - rhoai/allam-beta-13b-chat-gptq
    - rhoai/granite-34b-code-base-gptq
    - rhoai/mixtral-8x7b-instruct-v0.1-gptq
    # - rhoai/llama-3.1-405b-gptq
    tests.fine_tuning.test_settings.dataset_name: alpaca_data.json
    tests.fine_tuning.test_settings.dataset_replication: 0.2
    tests.fine_tuning.test_settings.gpu: 4
    fine_tuning.model_registry: dmf

    tests.fine_tuning.test_settings.hyper_parameters:
      peft_method: "lora"
      use_flash_attn: true
      max_steps: -1
      per_device_train_batch_size: 1
      max_seq_length: 512
      gradient_accumulation_steps: 4
      warmup_ratio: 0.03
      num_train_epochs: 1
      gradient_checkpointing: true
      packing: false
      r: 4
      lora_alpha: 16
      # qlora arguments: https://github.com/foundation-model-stack/fms-hf-tuning?tab=readme-ov-file#fms-acceleration
      torch_dtype: "float16"
      fp16: true
      raw_lists:
        auto_gptq: ["triton_v2"]
        target_modules: ["all-linear"]


  dgx_multi_model_8:
    extends: [multi_model]
    fine_tuning.model_registry: hf
    tests.fine_tuning.multi_model.models:
    - name: bigcode/gpt_bigcode-santacoder
      replicas: 8
    tests.fine_tuning.test_settings.gpu: 1
    tests.fine_tuning.test_settings.dataset_name: alpaca_data.json

  many_model:
    extends: [fms]

    matbench.prom_workload: null
    tests.fine_tuning.many_model.enabled: true
    matbench.workload: projects.scheduler.visualizations.schedulers

  dgx_many_model:
    extends: [fms, many_model]

    tests.fine_tuning.test_settings.gpu: 2
    tests.fine_tuning.test_settings.dataset_name: alpaca_data.json
    fine_tuning.model_registry: hf
    tests.fine_tuning.test_settings.model_name: bigcode/gpt_bigcode-santacoder
    tests.fine_tuning.many_model.count: 64
    tests.fine_tuning.many_model.timespan: 64

  # ---

  multi_node_training:
    tests.fine_tuning.test_settings.pod_count: 1
    fine_tuning.pvc.access_mode: ReadWriteMany

  # ---

  metal:
    clusters.sutest.is_metal: true
    clusters.sutest.compute.dedicated: false

  not_metal:
    clusters.sutest.is_metal: false

  use_intlab_os:
    matbench.lts.opensearch.index_prefix: "psap-rhoai."
    matbench.lts.opensearch.instance: intlab

  use_smoke_os:
    matbench.lts.opensearch.instance: smoke

  gating_smoke:
    extends: [gpu, use_smoke_os]
    matbench.lts.opensearch.export.enabled: true
    matbench.lts.regression_analyses.enabled: true
    'ci_presets.light["tests.fine_tuning.test_settings.gpu"]': 1

  no_model:
    fine_tuning.pvc.name: null
    tests.fine_tuning.test_settings.dataset_name: null
    tests.fine_tuning.test_settings.model_name: null
    tests.fine_tuning.test_settings.dataset_replication: null

  # ---

  fms:
    tests.fine_tuning.fms.enabled: true

    matbench.workload: projects.fine_tuning.visualizations.fms_hf_tuning
    matbench.prom_workload: projects.fine_tuning.visualizations.fms_prom
    matbench.config_file: fms.yaml

    fine_tuning.default_response_template: "\n### Label:"
    tests.fine_tuning.test_settings.hyper_parameters:
      # hyper-parameters need to be listed here to allow setting their value in the presets
      per_device_train_batch_size: null
      gradient_accumulation_steps: null
      num_train_epochs: null
      max_seq_length: null
      peft_method: null
      auto_gptq: null
      torch_dtype: null
      fp16: null
      use_flash_attn: null
      # LoRA hyper-parameters:
      r: null
      lora_dropout: null
      lora_alpha: null
      target_modules: null
      #
      raw_lists: null
      skip_if: {}

  ray:
    tests.fine_tuning.ray.enabled: true
    tests.capture_prom: false # not needed for the time being
    tests.fine_tuning.test_settings.hyper_parameters: {}
    matbench.lts.generate: false
    tests.fine_tuning.test_settings.name: ray
    matbench.prom_workload: projects.fine_tuning.visualizations.ray_prom

  ray_bench_base:
    extends: [ray, no_model]
    matbench.config_file: ray_benchmark.yaml
    matbench.workload: projects.fine_tuning.visualizations.ray_benchmark

    tests.fine_tuning.ray.workload: ray-benchmark
    tests.fine_tuning.test_settings.gpu: 0
    tests.fine_tuning.test_settings.node_selector_key: nvidia.com/gpu.present
    tests.fine_tuning.test_settings.node_selector_value: "true"

  ray_bench__network_overhead:
    extends: [ray_bench_base]
    tests.fine_tuning.test_settings.hyper_parameters:
      flavor: network_overhead
      num_samples: 10

  ray_bench__iperf:
    extends: [ray_bench_base]

    tests.fine_tuning.test_settings.hyper_parameters:
      flavor: iperf

    tests.fine_tuning.test_settings.pod_count: 2
    tests.fine_tuning.test_settings.use_secondary_nic: null
    tests.fine_tuning.matbenchmarking.enabled: false
    tests.fine_tuning.matbenchmarking.stop_on_error: false

  ray_bench_scale:
    extends: [ray_bench]
    tests.fine_tuning.matbenchmarking.enabled: true
    tests.fine_tuning.matbenchmarking.stop_on_error: true
    tests.fine_tuning.test_settings.pod_count: [16, 32, 64, 128, 256, 512]
    tests.fine_tuning.test_settings.hyper_parameters.num_samples: [20, 75, 150]
    tests.fine_tuning.test_settings.gpu: null

  # ---

  ilab:
    extends: [gpu]
    ci_presets.light["tests.fine_tuning.test_settings.dataset_name"]: ilab_skills_data.jsonl
    ci_presets.light["tests.fine_tuning.test_settings.model_name"]: ibm-granite/granite-3b-code-instruct@hf

    tests.fine_tuning.ilab.enabled: true
    tests.fine_tuning.test_settings.name: ilab
    tests.fine_tuning.test_settings.dataset_name: ilab_skills_data.jsonl
    tests.fine_tuning.test_settings.model_name: ibm-granite/granite-3.0-8b-instruct@hf
    tests.fine_tuning.test_settings.dataset_replication: null
    matbench.workload: projects.fine_tuning.visualizations.ilab_training
    matbench.prom_workload: projects.fine_tuning.visualizations.ilab_prom
    matbench.config_file: ilab_training.yaml
    matbench.lts.generate: false
    tests.fine_tuning.test_settings.shared_memory: 20
    tests.fine_tuning.test_settings.hyper_parameters:
      num_epochs: null
      max_batch_len: null
      NCCL_SOCKET_NTHREADS: null
      cpu_offload_optimizer: null
      cpu_offload_params: null

  ilab_scale:
    extends: [ilab]

    tests.fine_tuning.test_settings.model_name: ibm-granite/granite-3.0-8b-instruct@hf
    tests.fine_tuning.test_settings.dataset_name: [ilab_large_10000samples_skills_data.jsonl, ilab_large_knowledge_data.jsonl]

    tests.fine_tuning.test_settings.pod_count: [1, 2, 4]
    tests.fine_tuning.test_settings.gpu: 2
    clusters.sutest.compute.machineset.count: 4
    tests.fine_tuning.test_settings.hyper_parameters.num_epochs: 1
    tests.fine_tuning.matbenchmarking.enabled: true
    tests.fine_tuning.matbenchmarking.stop_on_error: true

  ilab_ib_scale:
    extends: [ilab_scale]
    tests.fine_tuning.test_settings.pod_count: [1, 2]
    tests.fine_tuning.test_settings.gpu: [1, 2]
    clusters.sutest.compute.machineset.count: null
    tests.fine_tuning.matbenchmarking.stop_on_error: false

  ilab_ipi_scale:
    extends: [ilab_scale]
    tests.fine_tuning.test_settings.pod_count: [1, 2, 4]
    tests.fine_tuning.test_settings.gpu: [2]
    clusters.sutest.compute.machineset.count: null
    tests.fine_tuning.matbenchmarking.stop_on_error: false

  ilab_secondary_nic:
    tests.fine_tuning.test_settings.secondary_nic_prefix: "network-port-"
    tests.fine_tuning.test_settings.secondary_nic_count: [2, 4, 6, 8]

  ilab_aws:
    extends: [gpu, ilab]
    ci_presets.light["tests.fine_tuning.test_settings.gpu"]: 1
    ci_presets.gpu["clusters.sutest.compute.machineset.type"]: g5.2xlarge
    clusters.sutest.compute.machineset.type: g5.2xlarge

    fine_tuning.pvc.access_mode: ReadWriteMany
    fine_tuning.pvc.size: 200Gi
    fine_tuning.pvc.storage_class_name: nfs-provisioner
    nfs_provisioner.enabled: true

    tests.fine_tuning.node_count_equal_pod_count: true
    tests.fine_tuning.matbenchmarking.enabled: true
    tests.fine_tuning.matbenchmarking.stop_on_error: true
    tests.fine_tuning.test_settings.hyper_parameters.num_epochs: 1
    tests.fine_tuning.test_settings.ephemeral_output_pvc_size: 100Gi

  ilab_aws_scale:
    extends: [ilab_aws]

    tests.fine_tuning.matbenchmarking.enabled: true
    tests.fine_tuning.matbenchmarking.stop_on_error: false

    clusters.sutest.compute.machineset.spot: true
    clusters.sutest.compute.machineset.type: g6e.12xlarge

    tests.fine_tuning.test_settings.pod_count: [2, 1]
    tests.fine_tuning.test_settings.model_name: ibm-granite/granite-7b-base@hf
    tests.fine_tuning.test_settings.dataset_name: ilab_large_10000samples_skills_data.jsonl
    tests.fine_tuning.test_settings.gpu: 2

    tests.fine_tuning.test_settings.hyper_parameters.num_epochs: 1
    tests.fine_tuning.node_count_equal_pod_count: true

  ilab_2x8xh100_secondary_nic:
    extends: [ilab]

    tests.fine_tuning.matbenchmarking.enabled: true
    tests.fine_tuning.matbenchmarking.stop_on_error: false

    tests.fine_tuning.test_settings.model_name: ibm-granite/granite-3.0-8b-instruct@hf
    tests.fine_tuning.test_settings.dataset_name: ilab_large_10000samples_skills_data.jsonl

    tests.fine_tuning.test_settings.gpu: 8
    tests.fine_tuning.test_settings.pod_count: 2

    tests.fine_tuning.test_settings.secondary_nic_prefix: "network-port-"
    tests.fine_tuning.test_settings.secondary_nic_count: [8, 6, 4, 2, 1]

    # tests.fine_tuning.test_settings.hyper_parameters.max_batch_len: [60000, 70000, 80000, 85000, 90000, 95000]
    tests.fine_tuning.test_settings.hyper_parameters.max_batch_len: 85000
    tests.fine_tuning.test_settings.hyper_parameters.num_epochs: 1

  ilab_2x8xh100_pod_network:
    extends: [ilab]

    tests.fine_tuning.matbenchmarking.enabled: true
    tests.fine_tuning.matbenchmarking.stop_on_error: false

    tests.fine_tuning.test_settings.model_name: ibm-granite/granite-3.0-8b-instruct@hf
    tests.fine_tuning.test_settings.dataset_name: ilab_large_10000samples_skills_data.jsonl

    tests.fine_tuning.test_settings.gpu: 8
    tests.fine_tuning.test_settings.pod_count: 2

    tests.fine_tuning.test_settings.hyper_parameters.max_batch_len: 85000
    tests.fine_tuning.test_settings.hyper_parameters.num_epochs: 1

  ilab_2x8xh100_sriov:
    extends: [ilab]

    tests.fine_tuning.matbenchmarking.enabled: true
    tests.fine_tuning.matbenchmarking.stop_on_error: false

    tests.fine_tuning.test_settings.model_name: ibm-granite/granite-3.0-8b-instruct@hf
    tests.fine_tuning.test_settings.dataset_name: ilab_large_10000samples_skills_data.jsonl

    tests.fine_tuning.test_settings.gpu: 8
    tests.fine_tuning.test_settings.pod_count: 2

    tests.fine_tuning.test_settings.secondary_nic_prefix: "subnet-port-"
    tests.fine_tuning.test_settings.secondary_nic_count: [8, 6, 4, 2, 1]

    # tests.fine_tuning.test_settings.hyper_parameters.max_batch_len: [60000, 70000, 80000, 85000, 90000, 95000]
    tests.fine_tuning.test_settings.hyper_parameters.max_batch_len: 85000
    tests.fine_tuning.test_settings.hyper_parameters.num_epochs: 1

  ilab_2x8xh100_sriov_rdma:
    extends: [ilab]

    tests.fine_tuning.matbenchmarking.enabled: true
    tests.fine_tuning.matbenchmarking.stop_on_error: false

    tests.fine_tuning.test_settings.model_name: ibm-granite/granite-3.0-8b-instruct@hf
    tests.fine_tuning.test_settings.dataset_name: ilab_large_10000samples_skills_data.jsonl

    tests.fine_tuning.test_settings.gpu: 8
    tests.fine_tuning.test_settings.pod_count: 2

    tests.fine_tuning.test_settings.secondary_nic_prefix: "subnet-rdma-port-"
    tests.fine_tuning.test_settings.secondary_nic_count: [8, 6, 4, 2, 1]

    # tests.fine_tuning.test_settings.hyper_parameters.max_batch_len: [60000, 70000, 80000, 85000, 90000, 95000]
    tests.fine_tuning.test_settings.hyper_parameters.max_batch_len: 85000
    tests.fine_tuning.test_settings.hyper_parameters.num_epochs: 1

  ilab_2x8xh100_sriov_rdma_25000:
    extends: [ilab_2x8xh100_sriov_rdma]

    tests.fine_tuning.test_settings.dataset_name: ilab_large_25000samples_skills_data.jsonl

  ilab_2x8xh100_test_epochs:
    extends: [ilab]

    tests.fine_tuning.matbenchmarking.enabled: true
    tests.fine_tuning.matbenchmarking.stop_on_error: false

    tests.fine_tuning.test_settings.model_name: ibm-granite/granite-3.0-8b-instruct@hf
    tests.fine_tuning.test_settings.dataset_name: ilab_large_10000samples_skills_data.jsonl

    tests.fine_tuning.test_settings.gpu: 8
    tests.fine_tuning.test_settings.pod_count: 2

    tests.fine_tuning.test_settings.secondary_nic_prefix: "subnet-rdma-port-"
    tests.fine_tuning.test_settings.secondary_nic_count: [1]

    # tests.fine_tuning.test_settings.hyper_parameters.max_batch_len: [60000, 70000, 80000, 85000, 90000, 95000]
    tests.fine_tuning.test_settings.hyper_parameters.max_batch_len: 85000
    tests.fine_tuning.test_settings.hyper_parameters.num_epochs: 4

  ilab_1x8xh100_single_node:
    extends: [ilab]

    tests.fine_tuning.matbenchmarking.enabled: true
    tests.fine_tuning.matbenchmarking.stop_on_error: false

    tests.fine_tuning.test_settings.model_name: ibm-granite/granite-3.0-8b-instruct@hf
    tests.fine_tuning.test_settings.dataset_name: ilab_large_10000samples_skills_data.jsonl

    tests.fine_tuning.test_settings.gpu: 8
    tests.fine_tuning.test_settings.pod_count: 1

    tests.fine_tuning.test_settings.hyper_parameters.max_batch_len: [20000, 60000, 70000]
#    tests.fine_tuning.test_settings.hyper_parameters.max_batch_len: 85000
    tests.fine_tuning.test_settings.hyper_parameters.num_epochs: 1

  ilab_l40s_scale:
    extends: [ilab]

    tests.fine_tuning.matbenchmarking.enabled: true
    tests.fine_tuning.matbenchmarking.stop_on_error: false

    tests.fine_tuning.test_settings.model_name: ibm-granite/granite-3.0-8b-instruct@hf
    tests.fine_tuning.test_settings.dataset_name: ilab_large_10000samples_skills_data.jsonl

    tests.fine_tuning.test_settings.gpu: 2

    tests.fine_tuning.test_settings.hyper_parameters.num_epochs: 1
    tests.fine_tuning.test_settings.hyper_parameters.cpu_offload_optimizer: true
    tests.fine_tuning.test_settings.hyper_parameters.cpu_offload_params: true
    tests.fine_tuning.test_extra_settings:
    - pod_count: 1
      hyper_parameters.max_batch_len: [35000, 40000]

  # ---

  cluster_instructlab:
    fine_tuning.pvc.storage_class_name: nfs-csi
    fine_tuning.pvc.access_mode: ReadWriteMany
    fine_tuning.pvc.size: 500Gi
    clusters.sutest.is_metal: true
    clusters.sutest.compute.machineset.type: gx3-48x240x2l40s
    clusters.sutest.compute.machineset.name: instructlab-standalon-6rjg8-worker-1
    clusters.sutest.compute.machineset.taint: null
    clusters.sutest.compute.machineset.rest_count: 1
    tests.fine_tuning.test_settings.ephemeral_output_pvc_size: 500Gi

  cluster_ibm_dgx:
    clusters.sutest.compute.machineset.type: "IBM-DGX A100-80GB"
    tests.fine_tuning.namespace: rhoai-tuning-test
    fine_tuning.pvc.name: rhoai-tuning-test
    tests.capture_prom: with-queries

  cluster_dgx:
    clusters.sutest.compute.machineset.type: "DGX A100-40GB"

  cluster_icelake:
    clusters.sutest.compute.machineset.type: "Icelake"

  cluster_a30:
    clusters.sutest.compute.machineset.type: "Cluster A30-24GB"

  cluster_kubecon_roce:
    clusters.sutest.compute.machineset.type: "Cluster Kubecon-RoCE"
    clusters.sutest.is_metal: true
    fine_tuning.pvc.storage_class_name: trident-sc
    tests.fine_tuning.test_settings.use_secondary_nic: "network-port-1"

  cluster_h100:
    extends: [metal]
    fine_tuning.pvc.storage_class_name: nfs-provisioner
    nfs_provisioner.enabled: true
    nfs_provisioner.storage_size: 6500Gi
    nfs_provisioner.pvc_sc: lvms-vg1
    fine_tuning.pvc.access_mode: ReadWriteMany
    fine_tuning.pvc.size: 2000Gi
    clusters.sutest.compute.machineset.type: "gx3d-160x1792x8h100"
    tests.fine_tuning.test_settings.ephemeral_output_pvc_size: 500Gi

  cluster_rhoai_h100:
    clusters.sutest.compute.machineset.type: "4xH100-80GB"

  gating:
    extends: [use_intlab_os]
    matbench.lts.opensearch.export.enabled: true
    matbench.lts.regression_analyses.enabled: true
    export_artifacts.enabled: true

secrets:
  dir:
    name: psap-ods-secret
    env_key: PSAP_ODS_SECRET_PATH
  # name of the file containing the properties of LDAP secrets
  s3_ldap_password_file: s3_ldap.passwords
  keep_cluster_password_file: get_cluster.password
  rhoai_token_file: quay.io.token
  opensearch_instances: opensearch.yaml
  aws_credentials: .awscred
  git_credentials: git-credentials
  dmf_token: dmf.token
clusters:
  metal_profiles:
    p42-h03-dgx.rdu3.labs.perfscale.redhat.com: cluster_dgx
    e26-h23-000-r650: cluster_icelake
    cc37-h13-000-r750.rdu3.labs.perfscale.redhat.com: cluster_a30
    fmaas-devstage-backen-9cjgj-worker-a100-il-rdma-3-nc7c8: cluster_ibm_dgx
    instructlab-standalon-6rjg8-master-0: cluster_instructlab
    intel-perf-27.perf.eng.bos2.dc.redhat.com: cluster_kubecon_roce
    tosokin-ilab-cluster-556dc-master-0: cluster_h100
    dagray-psap-ext-7kr6c-master-0: cluster_h100
    l42-h06-000-xe8640.rdu3.labs.perfscale.redhat.com: cluster_rhoai_h100
  create:
    type: single # can be: single, ocp, managed
    keep: false
    name_prefix: fine-tuning-ci
    ocp:
      # list of tags to apply to the machineset when creating the cluster
      tags:
        TicketId: null
        Project: PSAP/Project/FineTuning
      deploy_cluster:
        target: cluster
      base_domain: psap.aws.rhperfscale.org
      version: 4.15.9
      region: us-west-2
      control_plane:
        type: m6a.xlarge
      workers:
        type: m6a.2xlarge
        count: 2

  sutest:
    is_metal: false
    lab:
      name: null
    compute:
      dedicated: true
      machineset:
        name: workload-pods
        type: m6i.2xlarge
        count: null
        spot: false
        rest_count: 0
        taint:
          key: only-workload-pods
          value: "yes"
          effect: NoSchedule
  cleanup_on_exit: false

rhods:
  catalog:
    image: quay.io/rhoai/rhoai-fbc-fragment@sha256
    tag: 5f2d0a88ef533cb4a95840051c8d4b91d517b2c3dad340daebfc405416d3205d
    channel: fast
    version: 2.17.0
    version_name: rc2
    opendatahub: false
    managed_rhoi: true
  operator:
    # set to true to stop the RHODS operator
    stop: false

nfs_provisioner:
  enabled: false
  storage_size: 250Gi
  enforce_enabled_when_sc_used: true
  pvc_sc:

gpu:
  prepare_cluster: false
  time_sharing:
    replicas: 1

fine_tuning:
  pvc:
    name: fine-tuning-storage
    access_mode: ReadWriteOnce
    size: 80Gi
    storage_class_name: null
  model_registry: null # if set to a fine_tuning_sources.* model registry, all the lookups will be done in this registry
  default_response_template: null

  sources:
    dmf:
      type: model-registry
      registry_type: model
      source_dir: "dmf://"
      secret_key: "secrets.dmf_token"
      download_pod_image_key: "tests.fine_tuning.fms.image"
    hf:
      type: model-registry
      registry_type: model
      source_dir: 'https://huggingface.co/'
      secret_key: secrets.git_credentials

    twitter_complaints_small.json:
      type: dataset
      source_dir: 'https://raw.githubusercontent.com/foundation-model-stack/fms-hf-tuning/b48249fab3df124d6b85cc8ce59b9e5a66ea6dcb/tests/data'

    synthetic_dataset.txt:
      type: dataset
      source_dir: https://raw.githubusercontent.com/openshift-psap/topsail/main/projects/fine_tuning/toolbox/fine_tuning_run_fine_tuning_job/files/entrypoint/
      transform: generate_synthetic_dataset.py

    news-tokens-16384plus-entries-4096.jsonl:
      type: dataset
      source_dir: /manually/populated
      response_template: "\n### Response:"

    # ---

    ilab_skills_data.jsonl:
      type: dataset
      source_dir: 's3://instructlab-standalone/data'
      secret_key: secrets.aws_credentials

    ilab_knowledge_data.jsonl:
      type: dataset
      source_dir: 's3://instructlab-standalone/data'
      secret_key: secrets.aws_credentials

    ilab_large_skills_data.jsonl:
      type: dataset
      source_dir: 's3://instructlab-standalone/data'
      secret_key: secrets.aws_credentials

    ilab_large_10000samples_skills_data.jsonl:
      type: dataset
      source_dir: 's3://instructlab-standalone/data'
      secret_key: secrets.aws_credentials

    ilab_large_25000samples_skills_data.jsonl:
      type: dataset
      source_dir: 's3://instructlab-standalone/data'
      secret_key: secrets.aws_credentials

    ilab_large_knowledge_data.jsonl:
      type: dataset
      source_dir: 's3://instructlab-standalone/data'
      secret_key: secrets.aws_credentials

    # ---

    alpaca_data.json:
      type: dataset
      source_dir: 'https://raw.githubusercontent.com/gururise/AlpacaDataCleaned/main'
      transform: convert_alpaca.py

    bloom-560m:
      type: model
      source_dir: 's3://psap-watsonx-models/fine-tuning/bigscience'
      secret_key: secrets.aws_credentials
tests:
  capture_prom: true
  capture_prom_uwm: false
  capture_state: true
  visualize: true
  dry_mode: false
  fine_tuning:
    namespace: fine-tuning-testing
    node_count_equal_pod_count: false
    matbenchmarking:
      enabled: false
      visu_file:
      stop_on_error: true
    multi_model:
      enabled: false
      models:
      - name: bloom-560m
        replicas: 2
    test_settings:
      name: fine-tuning
      pod_count: 1
      model_name: bigscience/bloom-560m@hf
      dataset_name: twitter_complaints_small.json
      gpu: 1
      shared_memory: null
      dataset_replication: 1
      container_image: null
      use_secondary_nic: null
      use_primary_nic: null
      secondary_nic_prefix: null
      secondary_nic_count: null
      ephemeral_output_pvc_size: null
      # Ray-specific:
      node_selector_key: null
      node_selector_value: null
      # ---
      # https://huggingface.co/transformers/v3.0.2/main_classes/trainer.html
      hyper_parameters: {}

    test_extra_settings: {} # fine-grain configuration when matbenchmarking
    many_model:
      enabled: false
      count: 20
      kueue_name: local-queue
      timespan: 0
    fms:
      enabled: false
      image: quay.io/modh/fms-hf-tuning:v2.5.0
    ray:
      enabled: false
      workload: ray-benchmark
      image: quay.io/rhoai/ray:2.35.0-py311-cu121-torch24-fa26
    ilab:
      enabled: false
      image: registry.redhat.io/rhelai1/instructlab-nvidia-rhel9@sha256:525ab53de3829cac1a9aabb73194f49e22da8fdcf12a01c56ece961300cdab0d
      # instructlab 1.3
matbench:
  preset: null
  workload: projects.fine_tuning.visualizations.fine_tuning # actual workload must be a symlink to this dir
  prom_workload: projects.fine_tuning.visualizations.fine_tuning_prom # actual workload must be a symlink to this dir
  config_file: null
  download:
    mode: prefer_cache
    url:
    url_file:
    # if true, copy the results downloaded by `matbench download` into the artifacts directory
    save_to_artifacts: false
  # directory to plot. Set by testing/common/visualize.py before launching the visualization
  test_directory: null
  lts:
    generate: true
    horreum:
      test_name: null
    opensearch:
      export:
        enabled: false
        enabled_on_replot: false
        fail_test_on_fail: true
      instance: smoke
      index: topsail-fine-tuning
      index_prefix: ""
      prom_index_suffix: -prom
    regression_analyses:
      enabled: false
      enabled_on_replot: true
      # if the regression analyses fail, mark the test as failed
      fail_test_on_regression: true
export_artifacts:
  enabled: false
  bucket: rhoai-cpt-artifacts
  path_prefix: cpt/fine-tuning
  dest: null # will be set by the export code
exec_list:
  _only_: false

  pre_cleanup_ci: null
  prepare_ci: null
  test_ci: null
  post_cleanup_ci: null
