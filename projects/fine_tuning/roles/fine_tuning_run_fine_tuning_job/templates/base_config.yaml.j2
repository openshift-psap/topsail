---
# DATASET_SOURCE:      {{ fine_tuning_run_fine_tuning_job_dataset_name }}
# DATASET_TRANSFORM:   {{ fine_tuning_run_fine_tuning_job_dataset_transform }}
# DATASET_REPLICATION: {{ fine_tuning_run_fine_tuning_job_dataset_replication }}

training_data_path: "/mnt/output/dataset.json" # aka DATASET_DEST
model_name_or_path: "/mnt/storage/model/{{ fine_tuning_run_fine_tuning_job_model_name }}"
tokenizer_name_or_path: "/mnt/storage/model/{{ fine_tuning_run_fine_tuning_job_model_name }}"

response_template: "{{ fine_tuning_run_fine_tuning_job_dataset_response_template }}"

output_dir: "/mnt/output/fine-tuning"

accelerate_launch_args:
  num_processes: {{ fine_tuning_run_fine_tuning_job_gpu or 1 }}
  num_machines: {{ fine_tuning_run_fine_tuning_job_worker_replicas + 1 }}
  mixed_precision: "no"
  dynamo_backend: "no"

  downcast_bf16: 'no'
  main_training_function: main
  rdzv_backend: static
  same_network: true
  tpu_use_sudo: false
{% if (fine_tuning_run_fine_tuning_job_gpu or 1) > 1 %}
  use_fsdp: true
{% endif %}
# below are the *default* values. They might be overwritten when 'config_final.json' is generated.

# https://huggingface.co/transformers/v3.0.2/main_classes/trainer.html

num_train_epochs: 1
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
gradient_accumulation_steps: 4
evaluation_strategy: 'no'
save_strategy: epoch
learning_rate: 1.0e-05
weight_decay: 0
lr_scheduler_type: cosine
max_seq_length: 4096

include_tokens_per_second: true
dataset_text_field: output
use_flash_attn: false
log_level: debug
