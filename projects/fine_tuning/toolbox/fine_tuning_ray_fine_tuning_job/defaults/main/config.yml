# Auto-generated file, do not edit manually ...
# Toolbox generate command: repo generate_ansible_default_settings
# Source component: Fine_Tuning.ray_fine_tuning_job

# Parameters
# the name of the fine-tuning job to create
# Mandatory value
fine_tuning_ray_fine_tuning_job_name:

# the name of the namespace where the scheduler load will be generated
# Mandatory value
fine_tuning_ray_fine_tuning_job_namespace:

# the name of the PVC where the model and dataset are stored
fine_tuning_ray_fine_tuning_job_pvc_name: null

# the name of the model to use inside the /dataset directory of the PVC
fine_tuning_ray_fine_tuning_job_model_name: null

# the name of the workload job to run (see the role's workload directory)
fine_tuning_ray_fine_tuning_job_workload: ray-finetune-llm-deepspeed

# the name of the dataset to use inside the /model directory of the PVC
fine_tuning_ray_fine_tuning_job_dataset_name: null

# number of replications of the dataset to use, to artificially extend or reduce the fine-tuning effort
fine_tuning_ray_fine_tuning_job_dataset_replication: 1

# name of the transformation to apply to the dataset
fine_tuning_ray_fine_tuning_job_dataset_transform: null

# if True, and the dataset has to be transformed/duplicated, save and/or load it from the PVC
fine_tuning_ray_fine_tuning_job_dataset_prefer_cache: true

# if True, only prepare the dataset cache file and do not run the fine-tuning.
fine_tuning_ray_fine_tuning_job_dataset_prepare_cache_only: false

# the image to use for the fine-tuning container
fine_tuning_ray_fine_tuning_job_container_image: quay.io/rhoai/ray:2.35.0-py39-cu121-torch24-fa26

# the version identifier passed to the RayCluster object
fine_tuning_ray_fine_tuning_job_ray_version: 2.35.0

# the number of GPUs to request for the fine-tuning job
fine_tuning_ray_fine_tuning_job_gpu: 1

# the number of RAM gigs to request for to the fine-tuning job (in Gigs)
fine_tuning_ray_fine_tuning_job_memory: 10

# the number of CPU cores to request for the fine-tuning job (in cores)
fine_tuning_ray_fine_tuning_job_cpu: 1

# if True, sets the 'limits' of the job with the same value as the request.
fine_tuning_ray_fine_tuning_job_request_equals_limits: false

# if True, only prepare the environment but do not run the fine-tuning job.
fine_tuning_ray_fine_tuning_job_prepare_only: false

# if True, delete the other PyTorchJobs before running
fine_tuning_ray_fine_tuning_job_delete_other: false

# number of worker replicas to deploy
fine_tuning_ray_fine_tuning_job_worker_replicas: 2

# dictionnary of hyper-parameters to pass to sft-trainer
fine_tuning_ray_fine_tuning_job_hyper_parameters: {}

# if true, sleeps forever instead of running the fine-tuning command.
fine_tuning_ray_fine_tuning_job_sleep_forever: false

# if enabled, captures the artifacts that will help post-mortem analyses
fine_tuning_ray_fine_tuning_job_capture_artifacts: true
