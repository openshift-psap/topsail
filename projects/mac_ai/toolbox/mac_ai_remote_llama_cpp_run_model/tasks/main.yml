---
- name: Create the artifacts directory
  file:
    path: "{{ artifact_extra_logs_dir }}/artifacts"
    state: directory
    mode: '0755'

- name: Try to capture the build logs
  shell: |
    {{ mac_ai_remote_llama_cpp_run_model_prefix }} bash -c 'ls $(dirname "{{ mac_ai_remote_llama_cpp_run_model_path }}")/../*.log'
  ignore_errors: true
  register: build_logs_cmd

- name: Capture the build logs
  when: build_logs_cmd.rc == 0
  shell: |
    {{ mac_ai_remote_llama_cpp_run_model_prefix }} cat "{{ item }}" > "{{ artifact_extra_logs_dir }}/artifacts/$(basename '{{ item }}')"
  with_items: "{{ build_logs_cmd.stdout_lines }}"

- name: Execute and retrieve the artifacts
  block:
  - name: Start serving the llama_cpp model
    shell:
      cd "{{ mac_ai_remote_llama_cpp_run_model_base_work_dir }}";
      nohup {{ mac_ai_remote_llama_cpp_run_model_prefix }} {{ mac_ai_remote_llama_cpp_run_model_path }} \
        --model {{ mac_ai_remote_llama_cpp_run_model_name }} \
        --host 0.0.0.0 \
        --port {{ mac_ai_remote_llama_cpp_run_model_port }} \
        --n-gpu-layers {{ mac_ai_remote_llama_cpp_run_model_ngl }} \
        &> {{ artifact_extra_logs_dir }}/artifacts/llama_cpp.log \
        &
    environment:
      HOME: "{{ mac_ai_remote_llama_cpp_run_model_base_work_dir }}"

  - name: Wait for llama_cpp to start responding correctly
    command:
      curl -sSf localhost:{{ mac_ai_remote_llama_cpp_run_model_port }}/v1/models
    register: ollama_running_cmd
    until: ollama_running_cmd.rc == 0
    retries: 2
    delay: 5

  rescue:
  - name: Show the logs of the server
    command:
      cat {{ artifact_extra_logs_dir }}/artifacts/llama_cpp.log

  - name: Fail
    fail: msg="Failed to load the model"

  always:
  - name: Retrieve the artifact files locally
    include_role:
      name: remote_retrieve
    vars:
      remote_retrieve_path: "{{ artifact_extra_logs_dir }}"
      remote_retrieve_dest: "{{ artifact_extra_logs_dir }}"
