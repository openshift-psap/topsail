# Auto-generated file, do not edit manually ...
# Toolbox generate command: repo generate_ansible_default_settings
# Source component: Mac_Ai.remote_llama_cpp_run_model

# Parameters
# the base directory where to store things
# Mandatory value
mac_ai_remote_llama_cpp_run_model_base_work_dir:

# the path to the llama-server binary
# Mandatory value
mac_ai_remote_llama_cpp_run_model_path:

# the port number on which llama-cpp should listen
# Mandatory value
mac_ai_remote_llama_cpp_run_model_port:

# the name of the model to run
# Mandatory value
mac_ai_remote_llama_cpp_run_model_name:

# the prefix to get the llama-server running
mac_ai_remote_llama_cpp_run_model_prefix:

# number of layers to store in VRAM
mac_ai_remote_llama_cpp_run_model_ngl: 99

# Default Ansible variables
# Default value for ansible_os_family to ensure role remains standalone
ansible_os_family: Linux
