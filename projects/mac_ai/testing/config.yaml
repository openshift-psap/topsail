__platforms: &all_platforms
  - darwin/metal
  - darwin/vulkan
  - podman/vulkan

ci_presets:
  # list of names of presets to apply, or a single name, or null if not preset
  to_apply: []

  # dict of variables to apply
  variable_overrides: {}

  # list of names of presets that have been applied
  names: []

  local_config: null # defined locally in variable_overrides.yaml

  benchmark:
    test.matbenchmarking.enabled: true
    test.platform: *all_platforms
    test.model.name: llama3.2

    test.llm_load_test.matbenchmarking: false
    test.llm_load_test.args.concurrency: 1
    test.llm_load_test.args.duration: 300

    prepare.brew.capture_dependencies: true

  keep_running:
    test.inference_server.unload_on_exit: false
    test.inference_server.stop_on_exit: false
    prepare.podman.stop_on_exit: false

secrets:
  dir:
    name: crc-mac-ai-secret
    env_key: CRC_MAC_AI_SECRET_PATH
  private_key_path: mac_ai__private_key
  hostname: mac_ai__hostname
  username: mac_ai__username
  base_work_dir: mac_ai__base_work_dir

remote_host:
  run_locally: false
  private_key_filename: "@secrets.private_key_path" # in the secret dir
  hostname: "*$@secrets.hostname"
  username: "*$@secrets.username"
  port: 22
  base_work_dir: "*$@secrets.base_work_dir"
  ssh_flags:
  - -oStrictHostKeyChecking=no
  - -oUserKnownHostsFile=/dev/null
  - -o LogLevel=ERROR
  system: darwin
  arch: arm64
  python_bin: python3
  podman_bin: podman # only used if not prepare.podman.repo.enabled
  env:
    PATH: "/opt/homebrew/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin"
    HOME: "/Users/podmanqe/topsail_kpouget"

prepare:
  cleanup_on_exit: false
  llama_cpp:
    repo:
      url: https://github.com/ggml-org/llama.cpp
      version: b4870
      source:
        cmake:
          parallel: 5
          openmp:
            enabled: true
            flags: -DGGML_OPENMP=ON -DOpenMP_C_FLAGS="-I{@prepare.brew._libomp.location}/{@prepare.brew._libomp.version}/include/" -DOpenMP_C_LIB_NAMES=omp -DOpenMP_omp_LIBRARY={@prepare.brew._libomp.location}/{@prepare.brew._libomp.version}/lib/libomp.dylib -DOpenMP_CXX_FLAGS="-I{@prepare.brew._libomp.location}/{@prepare.brew._libomp.version}/include/" -DOpenMP_CXX_LIB_NAMES=omp
          common: -DGGML_CPU_ARM_ARCH=native
          flavors:
            metal: -DGGML_NATIVE=ON -DGGML_VULKAN=OFF
            vulkan: -DGGML_VULKAN=ON -DGGML_NATIVE=OFF -DGGML_METAL=OFF -DVulkan_INCLUDE_DIR=/opt/homebrew/include/ -DVulkan_LIBRARY=/opt/homebrew/lib/libMoltenVK.dylib
      darwin:
        upstream:
          file: llama-{@prepare.llama_cpp.repo.version}-bin-macos-{@remote_host.arch}.zip
          tarball: true
      podman:
        build_from: local_container_file # desktop_playground
        command: # will be set in prepare_test

        local_container_file:
          path: images/llama_cpp.containerfile
          command: /app/llama.cpp/build/bin/llama-server
          build_args:
            LLAMA_CPP_VERSION: "@prepare.llama_cpp.repo.version"
            LLAMA_CPP_CMAKE_BUILD_FLAGS: # set at runtime with --parallel
            # ..flavors.<name> will be appended to this
            LLAMA_CPP_CMAKE_FLAGS:
          flavors:
            kompute: -DGGML_KOMPUTE=ON -DGGML_NATIVE=OFF -DGGML_METAL=OFF
            vulkan: -DGGML_VULKAN=ON -DGGML_NATIVE=OFF -DGGML_METAL=OFF
        desktop_playground:
          url: https://github.com/slp/podman-desktop-extension-ai-lab-playground-images
          # url: https://github.com/containers/podman-desktop-extension-ai-lab-playground-images
          ref: 188866acbdd7efcdd438f21c09139ae378820a00
          root_directory: chat
          prepare_script: setup.sh
          container_file: vulkan/arm64/Containerfile
          image: llama_cpp
          tag: # use the commit first chars
          command: env PYTHONPATH=/locallm python -m llama_cpp.server

  ollama:
    repo:
      url: https://github.com/ollama/ollama/
      version: v0.5.7
      darwin:
        file: ollama-darwin
        executable: true
      podman/linux:
        file: ollama-linux-{@remote_host.arch}.tgz
        tarball: true
  brew:
    install_dependencies: true
    capture_dependencies: false
    _libomp:
      location: /opt/homebrew/Cellar/libomp/
      version: "20.1.0"
    dependencies:
    - krunkit
    - vulkan-headers
    - libkrun-efi
    - virglrenderer
    - molten-vk
    - molten-vk-krunkit
    - cmake
    - shaderc
    - libomp
    - glslang

  # the list of platform environments to prepare
  platforms: *all_platforms

  native_platform: darwin/upstream

  podman:
    repo:
      enabled: true
      url: https://github.com/containers/podman
      version: v5.4.0
      darwin:
        file: podman-remote-release-{@remote_host.system}_{@remote_host.arch}.zip
        zip: true
    gvisor:
      repo:
        enabled: true
        url: https://github.com/containers/gvisor-tap-vsock/
        version: v0.8.3
        file: "gvproxy-{@remote_host.system}"

    container:
      name: topsail_mac_ai

      image: quay.io/slopezpa/fedora-vgpu:latest # arm64 image
      python_bin: python3
      system: linux
      device: /dev/dri

    stop_on_exit: true

    machine:
      enabled: true
      force_configuration: false
      set_default: true
      name: podman-machine-default
      configuration:
        cpus: 4 # in cores
        memory: 5120 # in MiB
      env:
        CONTAINERS_MACHINE_PROVIDER: libkrun
        CONTAINERS_HELPER_BINARY_DIR: /opt/homebrew/bin/

test:
  platform: podman/vulkan
  platforms_to_skip:
  - darwin/upstream # always built, but no need to test it every time
  - podman/no-gpu/kompute # slow, not interesting atm

  inference_server:
    name: llama_cpp
    port: 11434
    unload_on_exit: true

    benchmark:
      enabled: true

  model:
    name: llama3.2
    size: small
    cache_dir: models

  llm_load_test:
    enabled: true
    matbenchmarking: false

    args:
      host: localhost
      port: "@test.inference_server.port"
      duration: 60
      concurrency: 1
      plugin: openai_plugin
      interface: http
      streaming: true
      endpoint: "/v1/chat/completions"

    dataset_sizes:
      small:
        max_input_tokens: 2047
        max_output_tokens: 1024
        max_sequence_tokens: 2048
      large:
        max_input_tokens: 3500
        max_output_tokens: 800
        max_sequence_tokens: 3500
  matbenchmarking:
    enabled: false
    stop_on_error: true

  capture_metrics:
    enabled: true
    gpu:
      enabled: true
      sampler: gpu_power
      rate: 1000
    virtgpu:
      enabled: true

cleanup:
  llm-load-test: true
  llama_cpp:
    image: true
    files: true
  models: true
  podman_machine:
    delete: true
    reset: true
  podman: true

export_artifacts:
  enabled: false

matbench:
  enabled: true
  preset: null
  workload: projects.mac_ai.visualizations.llm_load_test
  config_file: plots.yaml
  download:
    mode: prefer_cache
    url:
    url_file:
    # if true, copy the results downloaded by `matbench download` into the artifacts directory
    save_to_artifacts: false
  # directory to plot. Set by topsail/testing/visualize.py before launching the visualization
  test_directory: null
  lts:
    generate: false

exec_list:
  _only_: false

  pre_cleanup_ci: null
  prepare_ci: null
  test_ci: null
  post_cleanup_ci: null
  matbench_run_with_deploy: true
  matbench_run_without_deploy: true
