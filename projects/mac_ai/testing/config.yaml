__platforms: &all_platforms
  - macos/llama_cpp/metal
  - macos/llama_cpp/vulkan
  # - macos/llama_cpp/upstream_bin
  - podman/llama_cpp/vulkan
  # - macos/ollama
  - podman/ramalama

ci_presets:
  # list of names of presets to apply, or a single name, or null if not preset
  to_apply: []

  # dict of variables to apply
  variable_overrides: {}

  # list of names of presets that have been applied
  names: []

  local_config: null # defined locally in variable_overrides.yaml

  mac4:
    secrets.hostname: mac_ai__hostname.mac4
    secrets.username: mac_ai__username.mac4
    secrets.base_work_dir: mac_ai__base_work_dir.mac4
    remote_host.description: "crc-lab-mac4"

  mac5:
    secrets.hostname: mac_ai__hostname.mac5
    secrets.username: mac_ai__username.mac5
    secrets.base_work_dir: mac_ai__base_work_dir.mac5
    remote_host.description: "crc-lab-mac5"
    prepare.cleanup_on_exit: true

  mac-m4:
    secrets.hostname: mac_ai__hostname.mac-m4
    secrets.username: mac_ai__username.mac-m4
    secrets.base_work_dir: mac_ai__base_work_dir.mac-m4
    remote_host.description: "kpouget's MacBook M4 Pro"

  kpouget-atuona:
    secrets.hostname: mac_ai__hostname.kpouget-atuona
    secrets.username: mac_ai__username.kpouget-atuona
    secrets.base_work_dir: mac_ai__base_work_dir.kpouget-atuona
    remote_host.description: "kpouget's Atuona system"
    remote_host.system: linux
    remote_host.arch: x86_64
    prepare.podman.repo.enabled: false # no linux binary in GH ...
    prepare.podman.machine.enabled: false

  kpouget-laptop:
    secrets.hostname: mac_ai__hostname.kpouget-laptop
    secrets.username: mac_ai__username.kpouget-laptop
    secrets.base_work_dir: mac_ai__base_work_dir.kpouget-laptop
    remote_host.description: "kpouget's laptop system"
    remote_host.system: linux
    remote_host.arch: x86_64
    prepare.podman.repo.enabled: false # no linux binary in GH ...
    prepare.podman.machine.enabled: false

  # ---

  use_intlab_os:
    matbench.lts.opensearch.index_prefix: "psap-rhoai."
    matbench.lts.opensearch.instance: intlab

  export_kpis:
    matbench.lts.generate: true
    matbench.lts.opensearch.export.enabled: true

  analyze_kpis:
    matbench.lts.generate: true
    matbench.lts.regression_analyses.enabled: true

  # -*-

  cpt_nightly:
    extends: [cpt_all, mac5]
    matbench.lts.regression_analyses.notification.title: "Mac-AI: Ramalama & llama.cpp on Mac5"

  cpt_nightly_lightspeed:
    extends: [cpt_lightspeed, mac5]
    matbench.lts.regression_analyses.notification.title: "Mac-AI: Lightspeed on Mac5"

  cpt_all:
    extends: [cpt_ramalama, cpt_llama_cpp]
    matbench.lts.regression_analyses.notification.title: "Mac-AI: Ramalama & llama.cpp"
    test.model.name: ollama://llama3.2
    test.platform:
    - podman/ramalama
    - podman/ramalama/no-gpu
    - macos/llama_cpp/vulkan
    # - podman/llama_cpp/vulkan
    - macos/llama_cpp/metal
    test.llm_load_test.args.concurrency: 1
    test.llm_load_test.args.duration: 300

  cpt_ramalama:
    extends: [use_intlab_os, analyze_kpis, export_kpis]
    prepare.ramalama.repo.version: main
    prepare.ramalama.build_image.enabled: true

    test.platform:
    - podman/ramalama/no-gpu
    - podman/ramalama

    test.model.name: ollama://llama3.2
    matbench.lts.generate: true
    test.inference_server.benchmark.enabled: true

  cpt_llama_cpp:
    extends: [use_intlab_os, analyze_kpis, export_kpis]
    test.platform:
    - macos/llama_cpp/vulkan
    - macos/llama_cpp/metal
    - podman/llama_cpp/vulkan

    test.model.name: ollama://llama3.2
    prepare.llama_cpp.source.repo.version: latest
    matbench.lts.generate: true
    test.inference_server.benchmark.enabled: true

  cpt_lightspeed:
    extends: [lightspeed, use_intlab_os, analyze_kpis, export_kpis]
    test.llm_load_test.args.concurrency: 1
    test.llm_load_test.args.duration: 300
    secrets.image_registry: lightspeed-authfile.json
  # ---

  remoting:
    prepare.podman.machine.remoting_env.enabled: true
    prepare.virglrenderer.enabled: true
    prepare.llama_cpp.source.repo.url: https://github.com/crc-org/llama.cpp
    prepare.llama_cpp.source.repo.version: b7003-remoting-0.2.1
    prepare.virglrenderer.repo.branch: v1.2.0-remoting-0.1.6
    prepare.ramalama.build_image.enabled: true
    prepare.ramalama.build_image.name: remoting
    prepare.ramalama.repo.url: https://github.com/kpouget/ramalama
    prepare.ramalama.repo.version: v0.15.0-apir.0.1.2
    prepare.remoting.podman_desktop_extension.repo.version: v0.1.3

  remoting_wip:
    extends: []

  remoting_linux:
    extends: [remoting]
    prepare.virglrenderer.repo.branch: v1.2.0-remoting-0.1.6-linux
    prepare.llama_cpp.source.cmake.openmp.enabled: false
    prepare.platforms.model_pullers.llama_cpp: linux/llama_cpp/upstream_bin

    prepare.podman.machine.remoting_env.env.APIR_LLAMA_CPP_GGML_LIBRARY_REG: "ggml_backend_vk_reg"
    prepare.podman.machine.remoting_env.env.APIR_LLAMA_CPP_GGML_LIBRARY_INIT: "ggml_backend_vk_init"
    prepare.podman.machine.remoting_env.ggml_libs[0]: "libggml-remotingbackend.so"
    prepare.podman.machine.remoting_env.ggml_libs[1]: "libggml-vulkan.so"
    test.capture_metrics.enabled: false
    test.platform:
    - linux/llama_cpp/vulkan
    - podman/ramalama/remoting
    - podman/llama_cpp/remoting
    - podman/llama_cpp/vulkan

  remoting_mac:
    extends: [remoting]
    test.platform:
    - podman/ramalama/remoting
    - podman/llama_cpp/remoting
    - podman/llama_cpp/vulkan
    - macos/llama_cpp/metal
    - macos/llama_cpp/vulkan


  remoting_bench:
    extends: [remoting, benchmark]
    test.platform:
    - podman/ramalama/remoting
    - podman/llama_cpp/remoting
    - podman/llama_cpp/vulkan
    - macos/llama_cpp/metal

    test.platforms_to_skip:
    - podman/llama_cpp/remoting # build it but don't test it

  remoting_publish:
    extends: [remoting, use_intlab_os]
    prepare.remoting.publish: true
    prepare.ramalama.build_image.publish.enabled: true
    prepare.ramalama.build_image.registry_path: quay.io/crcont
    prepare.remoting.podman_desktop_extension.enabled: true
    prepare.remoting.podman_desktop_extension.image.publish.enabled: true

    test.platform:
    - podman/ramalama/remoting
    - podman/llama_cpp/remoting
    - podman/llama_cpp/vulkan
    - macos/llama_cpp/metal
    - macos/llama_cpp/vulkan
    - linux/llama_cpp/vulkan

  debug:
    prepare.llama_cpp.source.cmake.debug.enabled: true
    prepare.virglrenderer.debug.enabled: true
    prepare.ramalama.build_image.debug: true

  remoting_testing:
    extends: [remoting]
    test.platform:
    - podman/llama_cpp/remoting
    - macos/llama_cpp/metal
    - podman/llama_cpp/vulkan

  # ---

  lightspeed:
    test.platform: podman/lightspeed
    test.model.name: /models/Phi-4-mini-instruct-Q4_K_M.gguf
    test.inference_server.port: 8888
    test.inference_server.benchmark.enabled: true
    test.matbenchmarking.enabled: false
    test.llm_load_test.enabled: true

  llama_cpp:
    test.platform:
    - macos/llama_cpp/metal
    - macos/llama_cpp/vulkan
    - podman/llama_cpp/vulkan

  ramalama:
    test.model.name: ollama://llama3.1:8b
    test.platform:
    - podman/ramalama

  multi-servers:
    test.matbenchmarking.enabled: true
    test.model.name: ollama://llama3.1:8b
    test.platform:
    - podman/ramalama
    - macos/llama_cpp/metal
    - macos/llama_cpp/vulkan
    prepare.platforms.to_build: null # use test.platform

  vulkan-regression:
    extends: [inference_only]
    test.platform:
    - macos/llama_cpp/vulkan
    prepare.platforms.to_build: null # use test.platform
    test.matbenchmarking.enabled: true
    prepare.llama_cpp.source.repo.version:
    - b6924
    - b6927

  # ---
  micro_bench:
    test.inference_server.benchmark.llama_cpp.backend_ops_perf: true

  benchmark:
    extends: [bench_only]

    test.llm_load_test.args.concurrency: 1
    test.llm_load_test.args.duration: 300

    prepare.brew.capture_dependencies: true

  bench_only:
    test.llm_load_test.enabled: false
    test.inference_server.benchmark.enabled: true

  inference_only:
    test.llm_load_test.enabled: true
    test.inference_server.benchmark.enabled: false

  multi-version:
    test.matbenchmarking.enabled: true
    prepare.llama_cpp.source.repo.version:
    - b4897 # latest - 25-03-17
    - b5076 # latest - 25-04-08

  multi-models:
    test.matbenchmarking.enabled: true
    test.model.name:
    - granite3.3
    - qwen
    - phi
    - llama3.2
    - mistral
    - gemma3
    - gpt-oss

  multi-model-sizes:
    test.matbenchmarking.enabled: true
    test.model.name:
    - ollama://llama3.2:1b
    - ollama://llama3.2:3b
    - ollama://llama3.1:8b
    - ollama://llama2:13b

  multi-quant:
    test.matbenchmarking.enabled: true
    test.model.name:
    - hf://TheBloke/Llama-2-7B-Chat-GGUF:Q4_K_S
    - hf://TheBloke/Llama-2-7B-Chat-GGUF:Q4_0
    - hf://TheBloke/Llama-2-7B-Chat-GGUF:Q4_K_M
    - hf://TheBloke/Llama-2-7B-Chat-GGUF:Q3_K_M
    - hf://TheBloke/Llama-2-7B-Chat-GGUF:Q5_K_S

  multi-users:
    test.matbenchmarking.enabled: true
    test.llm_load_test.args.concurrency: [1, 2, 4]

  # ---

  llama-cpp:
    prepare.platforms.to_build: null # use test.platform
    test.platform:
    - macos/llama_cpp/metal
    - macos/llama_cpp/vulkan
    - podman/llama_cpp/vulkan

  keep_running:
    test.inference_server.unload_on_exit: false
    prepare.podman.stop_on_exit: false

secrets:
  dir:
    name: crc-mac-ai-secret
    env_key: CRC_MAC_AI_SECRET_PATH
  private_key_path: mac_ai__private_key
  hostname: mac_ai__hostname.mac-m4
  username: mac_ai__username.mac-m4
  base_work_dir: mac_ai__base_work_dir.mac-m4
  opensearch_instances: opensearch.yaml
  image_registry: quay-push.yaml

remote_host:
  run_locally: false
  private_key_filename: "@secrets.private_key_path" # in the secret dir
  hostname: "*$@secrets.hostname"
  username: "*$@secrets.username"
  port: 22
  base_work_dir: "*$@secrets.base_work_dir"
  ssh_flags:
  - -oStrictHostKeyChecking=no
  - -oUserKnownHostsFile=/dev/null
  - -o LogLevel=ERROR
  system: darwin
  arch: arm64
  python_bin: python3
  podman_bin: podman # only used if not prepare.podman.repo.enabled
  env:
    PATH: "/opt/homebrew/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin"
  home_is_base_work_dir: true
  verbose_ssh_commands: false
  description: null

foreign_testing:
  llama_cpp:
    repo: prepare.llama_cpp.source.repo.url
    version: prepare.llama_cpp.source.repo.version
    presets:
    - remoting_wip
  ramalama:
    repo: prepare.ramalama.repo.url
    version: prepare.ramalama.repo.version

prepare:
  # the list of platform environments to prepare
  platforms:
    to_build: null

    model_pullers:
      llama_cpp: macos/llama_cpp/upstream_bin
      ollama: macos/ollama
      ramalama: podman/ramalama
      lightspeed: podman/lightspeed

  cleanup_on_exit: false
  prepare_only_inference_server: false

  llm_load_test:
    install_requirements: false

  lightspeed:
    installer:
      image: quay.io/redhat-user-workloads/rhel-lightspeed-tenant/installer
      version: latest
      credentials: "*$@secrets.image_registry"
    runtime:
      container_names:
        model: rhel-cla-llamacpp-model

  llama_cpp:
    iterable_build_fields: # sync with test.matbenchmarking.iterable_test_fields if necessary
    - prepare.llama_cpp.source.repo.version
    - prepare.llama_cpp.source.cmake.parallel
    release:
      repo:
        url: https://github.com/ggml-org/llama.cpp
        version: b7356

      file:
        darwin: llama-{@prepare.llama_cpp.release.repo.version}-bin-macos-{@remote_host.arch}.zip
        linux: llama-{@prepare.llama_cpp.release.repo.version}-bin-ubuntu-vulkan-x64.zip
      tarball: true

    source:
      repo:
        url: https://github.com/ggml-org/llama.cpp
        version: latest
        latest: null # resolved at runtime
      cmake: # native builds only
        parallel: 5
        openmp:
          enabled: true
          flags:
            -DGGML_OPENMP=ON -DOpenMP_C_FLAGS="-I{@prepare.brew._libomp.location}/{@prepare.brew._libomp.version}/include/" -DOpenMP_C_LIB_NAMES=omp -DOpenMP_omp_LIBRARY={@prepare.brew._libomp.location}/{@prepare.brew._libomp.version}/lib/libomp.dylib -DOpenMP_CXX_FLAGS="-I{@prepare.brew._libomp.location}/{@prepare.brew._libomp.version}/include/" -DOpenMP_CXX_LIB_NAMES=omp
        common: -DGGML_CPU_ARM_ARCH=native -DLLAMA_CURL=OFF
        debug:
          enabled: false
          flags: -DCMAKE_BUILD_TYPE=Debug
        flavors:
          metal: -DGGML_NATIVE=ON -DGGML_VULKAN=OFF
          vulkan:
            common: -DGGML_VULKAN=ON -DGGML_NATIVE=OFF -DGGML_METAL=OFF
            darwin: -DVulkan_INCLUDE_DIR=/opt/homebrew/include/ -DVulkan_LIBRARY=/opt/homebrew/lib/libMoltenVK.dylib
          remoting:
            common: -DGGML_NATIVE=OFF -DGGML_REMOTINGBACKEND=ON
            darwin: -DGGML_METAL=ON -DGGML_METAL_MACOSX_VERSION_MIN=14.0 # backend
            linux: -DGGML_VULKAN=ON
      podman:
        build_from: local_container_file
        command: # will be set in prepare_test

        local_container_file:
          path: images/llama_cpp.containerfile
          command: /app/llama.cpp/build/bin/llama-server
          build_args:
            LLAMA_CPP_VERSION: "@prepare.llama_cpp.source.repo.version"
            LLAMA_CPP_REPO: "@prepare.llama_cpp.source.repo.url"
            LLAMA_CPP_CMAKE_BUILD_FLAGS: # set at runtime with --parallel
            # ..flavors.<name> will be appended to this
            LLAMA_CPP_CMAKE_FLAGS: -DLLAMA_CURL=OFF
            BUILD_FLAVOR: # set at runtime with the name of the flavor (one from below)
          flavors:
            vulkan:
              common: -DGGML_VULKAN=ON -DGGML_NATIVE=OFF -DGGML_METAL=OFF
            remoting:
              common: -DGGML_REMOTINGFRONTEND=ON -DGGML_CPU_ARM_ARCH=native

  ollama:
    repo:
      url: https://github.com/ollama/ollama/
      version: v0.5.7
      macos:
        file: ollama-darwin

  ramalama:
    iterable_build_fields: # sync with test.matbenchmarking.iterable_test_fields if necessary
    - prepare.ramalama.repo.version
    repo:
      url: https://github.com/containers/ramalama
      version: latest
    build_image:
      enabled: false
      name: ramalama
      registry_path: localhost
      debug: false
      publish:
        enabled: false
        credentials: "*$@secrets.image_registry"
    remoting:
      backend: vulkan
  brew:
    install_dependencies: false
    capture_dependencies: false
    _libomp:
      location: /opt/homebrew/Cellar/libomp/
      version: "20.1.0"
    dependencies:
    # brew tap slp/krunkit
    - krunkit

    - molten-vk

    - vulkan-headers
    - libkrun-efi
    - virglrenderer
    - molten-vk
    - cmake
    - shaderc
    - libomp
    - glslang

  virglrenderer: # only built when using the `remoting` platform
    enabled: false
    repo:
      url: https://gitlab.freedesktop.org/kpouget/virglrenderer
      branch: main
    build:
      flags:
        common: -Dapir=true
        darwin: -Dc_link_args=-L/opt/homebrew/lib/
    debug:
      enabled: false
      flags: --buildtype=debug
  podman:
    repo:
      enabled: true
      url: https://github.com/containers/podman
      version: v5.6.2
      darwin:
        file: podman-remote-release-{@remote_host.system}_{@remote_host.arch}.zip
        zip: true
    gvisor:
      repo:
        enabled: true
        url: https://github.com/containers/gvisor-tap-vsock/
        version: v0.8.7
        file: "gvproxy-{@remote_host.system}"

    container:
      name: topsail_mac_ai

      image: quay.io/slopezpa/fedora-vgpu:latest # arm64 image
      python_bin: python3
      system: linux
      device: /dev/dri

    stop_on_exit: true

    machine:
      enabled: true
      force_configuration: false
      set_default: true
      name: podman-machine-default
      configuration:
        cpus: 4 # in cores
        memory: 10240 # in MiB
      env:
        CONTAINERS_MACHINE_PROVIDER: libkrun
        CONTAINERS_HELPER_BINARY_DIR: /opt/homebrew/bin/
      remoting_env:
        enabled: false
        ggml_libs:
        - libggml-remotingbackend.dylib # keep the APIR backend first
        - libggml-metal.dylib # keep the GGML backend second
        - libggml-base.dylib
        env:
          APIR_LLAMA_CPP_GGML_LIBRARY_REG: ggml_backend_metal_reg
          APIR_LLAMA_CPP_GGML_LIBRARY_INIT: ggml_backend_metal_init
          VIRGL_APIR_LOG_TO_FILE: /tmp/topsail_apir_virglrenderer.log
          APIR_LLAMA_CPP_LOG_TO_FILE: /tmp/topsail_apir_llama_cpp.log
        env_extra: {}
  remoting:
    podman_desktop_extension:
      enabled: false
      repo:
        url: https://github.com/crc-org/podman-desktop-remoting-ext
        version: v0.1.3
      image:
        containerfile: Containerfile
        dest: quay.io/crcont/podman-desktop-remoting-ext
        publish:
          enabled: false
          credentials: "*$@secrets.image_registry"

    publish: false

test:
  platform: *all_platforms

  platforms_to_skip: []

  inference_server:
    port: 11434
    unload_on_exit: true
    stop_on_exit: true

    benchmark:
      enabled: true
      llama_cpp:
        llama_bench: true
        backend_ops_perf: false
  model:
    name: ollama://granite4
    size: small
    gguf_dir: models

  llm_load_test:
    enabled: true

    args:
      host: 127.0.0.1
      port: "@test.inference_server.port"
      duration: 60
      concurrency: 1
      plugin: openai_plugin
      interface: http
      streaming: true
      endpoint: "/v1/chat/completions"

    dataset_sizes:
      small:
        max_input_tokens: 2047
        max_output_tokens: 1024
        max_sequence_tokens: 2048
      large:
        max_input_tokens: 3500
        max_output_tokens: 800
        max_sequence_tokens: 3500
  matbenchmarking:
    enabled: false
    stop_on_error: true
    iterable_test_fields:
    - test.model.name
    - test.platform
    - test.llm_load_test.args
    - prepare.llama_cpp.release.repo.version
    - prepare.llama_cpp.source.repo.version
    - prepare.ramalama.repo.version

  capture_metrics:
    enabled: true
    gpu:
      # needs this in visudo:
      # $USER ALL = (root) NOPASSWD: /usr/bin/pkill powermetrics
      # $USER ALL = (root) NOPASSWD: /usr/bin/powermetrics *
      enabled: true
      sampler: gpu_power
      rate: 1000
    virtgpu:
      enabled: true

cleanup:
  models:
    gguf: true
    ollama: true
    ramalama: true

  files:
    llama_cpp: true
    ollama: true
    ramalama: true
    llm-load-test: true
    podman: true
    virglrenderer: true
    lightspeed: true

  images:
    llama_cpp: true
    lightspeed: true

  podman_machine:
    delete: true
    reset: true

export_artifacts:
  enabled: false

matbench:
  enabled: true
  preset: null
  workload: projects.mac_ai.visualizations.llm_load_test
  config_file: plots.yaml
  download:
    mode: prefer_cache
    url:
    url_file:
    # if true, copy the results downloaded by `matbench download` into the artifacts directory
    save_to_artifacts: false
  # directory to plot. Set by topsail/testing/visualize.py before launching the visualization
  test_directory: null
  lts:
    generate: false
    opensearch:
      export:
        enabled: false
        enabled_on_replot: false
        fail_test_on_fail: true
      instance: smoke
      index: topsail-mac-ai-cpt
      index_prefix: ""
      build_counter_index: "topsail-mac-ai-builds" # use the generate a unique ID for each build
    regression_analyses:
      enabled: false
      enabled_on_replot: true
      upload_lts_on_regression: true
      # if the regression analyses fail, mark the test as failed
      fail_test_on_regression: true
      notification:
        enabled: true
        title: "Mac-AI CPT"

exec_list:
  _only_: false

  pre_cleanup_ci: null
  prepare_ci: null
  test_ci: null
  post_cleanup_ci: null
  matbench_run: true
  generate_plots_from_pr_args: true

__platform_check:
  system: [macos, podman, linux]
  inference_server: [llama_cpp, ollama, ramalama, lightspeed]

  options:
    no_gpu: no-gpu

  flavors:
    llama_cpp: [metal, vulkan, upstream_bin, remoting]
    ramalama: [remoting, no-gpu, null]
