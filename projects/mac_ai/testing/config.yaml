__platforms: &all_platforms
  - macos/llama_cpp/metal
  - macos/llama_cpp/vulkan
  - macos/llama_cpp/upstream_bin
  - podman/llama_cpp/vulkan
  - macos/ollama
  - podman/ramalama

ci_presets:
  # list of names of presets to apply, or a single name, or null if not preset
  to_apply: []

  # dict of variables to apply
  variable_overrides: {}

  # list of names of presets that have been applied
  names: []

  local_config: null # defined locally in variable_overrides.yaml

  mac4:
    secrets.hostname: mac_ai__hostname.mac4
    secrets.username: mac_ai__username.mac4
    secrets.base_work_dir: mac_ai__base_work_dir.mac4

  mac5:
    secrets.hostname: mac_ai__hostname.mac5
    secrets.username: mac_ai__username.mac5
    secrets.base_work_dir: mac_ai__base_work_dir.mac5

  mac-m4:
    secrets.hostname: mac_ai__hostname.mac-m4
    secrets.username: mac_ai__username.mac-m4
    secrets.base_work_dir: mac_ai__base_work_dir.mac-m4

  # ---

  use_intlab_os:
    matbench.lts.opensearch.index_prefix: "psap-rhoai."
    matbench.lts.opensearch.instance: intlab

  export_kpis:
    matbench.lts.generate: true
    matbench.lts.opensearch.export.enabled: true

  analyze_kpis:
    matbench.lts.generate: true
    matbench.lts.regression_analyses.enabled: true

  # -*-

  cpt_all:
    extends: [cpt_ramalama, cpt_llama_cpp]
    test.model.name: llama3.2/llama3.2
    test.platform:
    - podman/ramalama
    - macos/llama_cpp/vulkan
    - podman/llama_cpp/vulkan
    - macos/llama_cpp/metal

  cpt_ramalama:
    extends: [use_intlab_os, analyze_kpis, export_kpis]
    test.platform: [podman/ramalama]
    test.model.name: llama3.2/llama3.2
    prepare.ramalama.repo.version: latest
    matbench.lts.generate: true
    test.inference_server.benchmark.enabled: false

  cpt_llama_cpp:
    extends: [use_intlab_os, analyze_kpis, export_kpis]
    test.platform:
    - macos/llama_cpp/vulkan
    - macos/llama_cpp/metal
    - podman/llama_cpp/vulkan

    test.model.name: llama3.2/llama3.2
    prepare.llama_cpp.source.repo.version: latest
    matbench.lts.generate: true
    test.inference_server.benchmark.enabled: false

  # ---

  remoting:
    test.platform:
    - podman/llama_cpp/remoting
    - macos/llama_cpp/metal
    - podman/llama_cpp/vulkan
    prepare.llama_cpp.source.repo.url: https://github.com/kpouget/llama.cpp
    prepare.llama_cpp.source.repo.version: main
  llama_cpp:
    test.platform:
    - macos/llama_cpp/metal
    - macos/llama_cpp/vulkan
    - podman/llama_cpp/vulkan

  ramalama:
    test.model.name: ollama://llama3.1:8b
    test.platform:
    - podman/ramalama

  multi-servers:
    test.matbenchmarking.enabled: true
    test.model.name: ollama://llama3.1:8b
    test.platform:
    - podman/ramalama
    - macos/llama_cpp/metal
    - macos/llama_cpp/vulkan
    prepare.platforms.to_build: null # use test.platform

  # ---

  benchmark:
    test.matbenchmarking.enabled: true
    test.platform: *all_platforms
    test.model.name: llama3.2/llama3.2

    test.llm_load_test.args.concurrency: 1
    test.llm_load_test.args.duration: 300

    prepare.brew.capture_dependencies: true

  bench_only:
    test.llm_load_test.enabled: false
    test.inference_server.benchmark.enabled: true

  inference_only:
    test.llm_load_test.enabled: true
    test.inference_server.benchmark.enabled: false

  multi-version:
    test.matbenchmarking.enabled: true
    prepare.llama_cpp.source.repo.version:
    - b4897 # latest - 25-03-17
    - b5076 # latest - 25-04-08

  multi-model:
    test.model.name:
    - ollama://llama3.2:1b
    - ollama://llama3.2:3b
    - ollama://llama3.1:8b
    - ollama://llama2:13b

  multi-quant:
    test.model.name:
    - hf://TheBloke/Llama-2-7B-Chat-GGUF:Q4_K_S
    - hf://TheBloke/Llama-2-7B-Chat-GGUF:Q4_0
    - hf://TheBloke/Llama-2-7B-Chat-GGUF:Q4_K_M
    - hf://TheBloke/Llama-2-7B-Chat-GGUF:Q3_K_M
    - hf://TheBloke/Llama-2-7B-Chat-GGUF:Q5_K_S

  multi-users:
    test.matbenchmarking.enabled: true
    test.llm_load_test.args.concurrency: [1, 2, 4]

  # ---

  llama-cpp:
    prepare.platforms.to_build: null # use test.platform
    test.platform:
    - macos/llama_cpp/metal
    - macos/llama_cpp/vulkan
    - podman/llama_cpp/vulkan

  keep_running:
    test.inference_server.unload_on_exit: false
    prepare.podman.stop_on_exit: false

secrets:
  dir:
    name: crc-mac-ai-secret
    env_key: CRC_MAC_AI_SECRET_PATH
  private_key_path: mac_ai__private_key
  hostname: mac_ai__hostname.mac-m4
  username: mac_ai__username.mac-m4
  base_work_dir: mac_ai__base_work_dir.mac-m4
  opensearch_instances: opensearch.yaml

remote_host:
  run_locally: false
  private_key_filename: "@secrets.private_key_path" # in the secret dir
  hostname: "*$@secrets.hostname"
  username: "*$@secrets.username"
  port: 22
  base_work_dir: "*$@secrets.base_work_dir"
  ssh_flags:
  - -oStrictHostKeyChecking=no
  - -oUserKnownHostsFile=/dev/null
  - -o LogLevel=ERROR
  system: darwin
  arch: arm64
  python_bin: python3
  podman_bin: podman # only used if not prepare.podman.repo.enabled
  env:
    PATH: "/opt/homebrew/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin"
  home_is_base_work_dir: true
  verbose_ssh_commands: true

prepare:
  # the list of platform environments to prepare
  platforms:
    to_build: null

    model_pullers:
      llama_cpp: macos/llama_cpp/upstream_bin
      ollama: macos/ollama
      ramalama: podman/ramalama

  cleanup_on_exit: false
  prepare_only_inference_server: false

  llm_load_test:
    install_requirements: false

  llama_cpp:
    iterable_build_fields: # sync with test.matbenchmarking.iterable_test_fields if necessary
    - prepare.llama_cpp.source.repo.version
    - prepare.llama_cpp.source.cmake.parallel
    release:
      repo:
        url: https://github.com/ggml-org/llama.cpp
        version: b5648
      file: llama-{@prepare.llama_cpp.release.repo.version}-bin-macos-{@remote_host.arch}.zip
      tarball: true

    source:
      repo:
        url: https://github.com/ggml-org/llama.cpp
        version: latest
      cmake: # native builds only
        parallel: 5
        openmp:
          enabled: true
          flags: -DGGML_OPENMP=ON -DOpenMP_C_FLAGS="-I{@prepare.brew._libomp.location}/{@prepare.brew._libomp.version}/include/" -DOpenMP_C_LIB_NAMES=omp -DOpenMP_omp_LIBRARY={@prepare.brew._libomp.location}/{@prepare.brew._libomp.version}/lib/libomp.dylib -DOpenMP_CXX_FLAGS="-I{@prepare.brew._libomp.location}/{@prepare.brew._libomp.version}/include/" -DOpenMP_CXX_LIB_NAMES=omp
        common: -DGGML_CPU_ARM_ARCH=native -DLLAMA_CURL=OFF
        flavors:
          metal: -DGGML_NATIVE=ON -DGGML_VULKAN=OFF
          vulkan: -DGGML_VULKAN=ON -DGGML_NATIVE=OFF -DGGML_METAL=OFF -DVulkan_INCLUDE_DIR=/opt/homebrew/include/ -DVulkan_LIBRARY=/opt/homebrew/lib/libMoltenVK.dylib
          remoting: -DGGML_NATIVE=OFF -DGGML_REMOTINGBACKEND=ON -DGGML_METAL=ON # backend
      podman:
        build_from: local_container_file
        command: # will be set in prepare_test

        local_container_file:
          path: images/llama_cpp.containerfile
          command: /app/llama.cpp/build/bin/llama-server
          build_args:
            LLAMA_CPP_VERSION: "@prepare.llama_cpp.source.repo.version"
            LLAMA_CPP_REPO: "@prepare.llama_cpp.source.repo.url"
            LLAMA_CPP_CMAKE_BUILD_FLAGS: # set at runtime with --parallel
            # ..flavors.<name> will be appended to this
            LLAMA_CPP_CMAKE_FLAGS: -DLLAMA_CURL=OFF
            BUILD_FLAVOR: # set at runtime with the name of the flavor (one from below)
          flavors:
            kompute: -DGGML_KOMPUTE=ON -DGGML_NATIVE=OFF -DGGML_METAL=OFF
            vulkan: -DGGML_VULKAN=ON -DGGML_NATIVE=OFF -DGGML_METAL=OFF
            remoting: -DGGML_REMOTINGFRONTEND=ON -DGGML_CPU_ARM_ARCH=native

  ollama:
    repo:
      url: https://github.com/ollama/ollama/
      version: v0.5.7
      macos:
        file: ollama-darwin

  ramalama:
    iterable_build_fields: # sync with test.matbenchmarking.iterable_test_fields if necessary
    - prepare.ramalama.repo.version
    repo:
      url: https://github.com/containers/ramalama
      version: latest

  brew:
    install_dependencies: false
    capture_dependencies: false
    _libomp:
      location: /opt/homebrew/Cellar/libomp/
      version: "20.1.0"
    dependencies:
    # brew tap slp/krunkit
    - krunkit
    # brew link --overwrite molten-vk-krunkit
    - molten-vk-krunkit

    - vulkan-headers
    - libkrun-efi
    - virglrenderer
    - molten-vk
    - cmake
    - shaderc
    - libomp
    - glslang

  virglrenderer: # only built when using the `remoting` platform
    enabled: true
    repo:
      url: https://gitlab.freedesktop.org/kpouget/virglrenderer
      branch: main
    build:
      flags: -Dvenus=true -Dc_link_args=-L/opt/homebrew/lib/

  podman:
    repo:
      enabled: true
      url: https://github.com/containers/podman
      version: v5.5.0
      darwin:
        file: podman-remote-release-{@remote_host.system}_{@remote_host.arch}.zip
        zip: true
    gvisor:
      repo:
        enabled: true
        url: https://github.com/containers/gvisor-tap-vsock/
        version: v0.8.3
        file: "gvproxy-{@remote_host.system}"

    container:
      name: topsail_mac_ai

      image: quay.io/slopezpa/fedora-vgpu:latest # arm64 image
      python_bin: python3
      system: linux
      device: /dev/dri

    stop_on_exit: true

    machine:
      enabled: true
      force_configuration: false
      set_default: true
      name: podman-machine-default
      configuration:
        cpus: 4 # in cores
        memory: 10240 # in MiB
      env:
        CONTAINERS_MACHINE_PROVIDER: libkrun
        CONTAINERS_HELPER_BINARY_DIR: /opt/homebrew/bin/
      remoting_env:
        enabled: true
        apir_lib:
          name: libggml-remotingbackend.dylib
        ggml_lib:
          name: libggml-metal.dylib
        env:
          APIR_LLAMA_CPP_GGML_LIBRARY_REG: ggml_backend_metal_reg
          APIR_LLAMA_CPP_GGML_LIBRARY_INIT: ggml_backend_metal_init

test:
  platform: *all_platforms

  platforms_to_skip:
  # - macos/llama_cpp/upstream_bin # always built, but no need to test it every time
  - podman/llama_cpp/no-gpu/kompute # slow, not interesting atm

  inference_server:
    port: 11434
    unload_on_exit: true
    stop_on_exit: true

    benchmark:
      enabled: true
      llama_cpp:
        llama_bench: true
        backend_ops_perf: false
  model:
    name: llama3.2/llama3.2
    size: small
    gguf_dir: models

  llm_load_test:
    enabled: true

    args:
      host: localhost
      port: "@test.inference_server.port"
      duration: 60
      concurrency: 1
      plugin: openai_plugin
      interface: http
      streaming: true
      endpoint: "/v1/chat/completions"

    dataset_sizes:
      small:
        max_input_tokens: 2047
        max_output_tokens: 1024
        max_sequence_tokens: 2048
      large:
        max_input_tokens: 3500
        max_output_tokens: 800
        max_sequence_tokens: 3500
  matbenchmarking:
    enabled: false
    stop_on_error: true
    iterable_test_fields:
    - test.model.name
    - test.platform
    - test.llm_load_test.args
    - prepare.llama_cpp.release.repo.version
    - prepare.llama_cpp.source.repo.version
    - prepare.ramalama.repo.version

  capture_metrics:
    enabled: true
    gpu:
      # needs this in visudo:
      # $USER ALL = (root) NOPASSWD: /usr/bin/pkill powermetrics
      # $USER ALL = (root) NOPASSWD: /usr/bin/powermetrics *
      enabled: true
      sampler: gpu_power
      rate: 1000
    virtgpu:
      enabled: true

cleanup:
  models:
    gguf: true
    ollama: true
    ramalama: true

  files:
    llama_cpp: true
    ollama: true
    ramalama: true
    llm-load-test: true
    podman: true
    virglrenderer: true

  images:
    llama_cpp: true

  podman_machine:
    delete: true
    reset: true

export_artifacts:
  enabled: false

matbench:
  enabled: true
  preset: null
  workload: projects.mac_ai.visualizations.llm_load_test
  config_file: plots.yaml
  download:
    mode: prefer_cache
    url:
    url_file:
    # if true, copy the results downloaded by `matbench download` into the artifacts directory
    save_to_artifacts: false
  # directory to plot. Set by topsail/testing/visualize.py before launching the visualization
  test_directory: null
  lts:
    generate: false
    opensearch:
      export:
        enabled: false
        enabled_on_replot: false
        fail_test_on_fail: true
      instance: smoke
      index: topsail-mac-ai-cpt
      index_prefix: ""
    regression_analyses:
      enabled: false
      enabled_on_replot: true
      # if the regression analyses fail, mark the test as failed
      fail_test_on_regression: true

exec_list:
  _only_: false

  pre_cleanup_ci: null
  prepare_ci: null
  test_ci: null
  post_cleanup_ci: null
  matbench_run: true
  generate_plots_from_pr_args: true

__platform_check:
  system: [macos, podman]
  inference_server: [llama_cpp, ollama, ramalama]

  options:
    no_gpu: no-gpu

  flavors:
    llama_cpp: [metal, vulkan, upstream_bin, kompute, remoting]
