---
- name: Ensure that a hostname was passed in parameter
  fail: msg="'benchmarking_run_mlperf_ssd_node_hostname' must be provided"
  when: benchmarking_run_mlperf_ssd_node_hostname | default('', true) | length == 0

- name: Ensure that a valid hostname name was passed in parameter
  command: oc get nodes -l kubernetes.io/hostname={{ benchmarking_run_mlperf_ssd_node_hostname }} -oname

- name: Ensure that the coco dataset PVC exists
  command:
    oc get pvc/{{ benchmarking_run_mlperf_ssd_dataset_pvc_name }}
       -n {{ benchmarking_run_mlperf_ssd_namespace }}

- name: Fetch the coco dataset PVC definition (debug)
  shell:
    oc get pvc/{{ benchmarking_run_mlperf_ssd_pvc_name }}
       -n {{ benchmarking_run_mlperf_ssd_namespace }}
       -oyaml
       > {{ artifact_extra_logs_dir }}/pvc_coco-dataset.yml

- name: Create the entrypoint ConfigMap file
  shell:
    oc create cm {{ benchmarking_run_mlperf_ssd_entrypoint_cm_name }}
       --from-file="{{ benchmarking_mlperf_ssd_entrypoint }}"
       -n {{ benchmarking_run_mlperf_ssd_namespace }}
       --dry-run=client
       -oyaml
       > {{ artifact_extra_logs_dir }}/000_configmap_run-mlperf-ssd_entrypoint.yml

- name: Create the entrypoint ConfigMap resource
  command: oc apply -f {{ artifact_extra_logs_dir }}/000_configmap_run-mlperf-ssd_entrypoint.yml

- name: Apply the Pod template
  template:
    src: "{{ benchmarking_mlperf_ssd_pod }}"
    dest: "{{ artifact_extra_logs_dir }}/001_pod_run-mlperf-ssd.yml"
    mode: '0400'

- name: Delete the Pod, if it exists
  command:
    oc delete -f "{{ artifact_extra_logs_dir }}/001_pod_run-mlperf-ssd.yml"
       --ignore-not-found=true

- name: Deploy the Pod to run the benchmark
  command:
    oc create -f "{{ artifact_extra_logs_dir }}/001_pod_run-mlperf-ssd.yml"

- name: Make sure the benchmark completes
  block:
  - name: Wait for the benchmark completion
    command:
      oc get pod/{{ benchmarking_run_mlperf_ssd_name }}
      --no-headers
      -ocustom-columns=phase:status.phase
      -n {{ benchmarking_run_mlperf_ssd_namespace }}
    register: wait_benchmark_pod_cmd
    until: "'Succeeded' in wait_benchmark_pod_cmd.stdout or 'Failed' in wait_benchmark_pod_cmd.stdout or 'Error' in wait_benchmark_pod_cmd.stdout"
    retries: 60
    delay: 60

  - name: Fail if the pod execution failed
    when: "'Failed' in wait_benchmark_pod_cmd.stdout or 'Error' in wait_benchmark_pod_cmd.stdout"
    fail: msg="The benchmark execution failed"

  always:
  - name: Store the logs of benchmark execution (for post-processing)
    shell:
      oc logs pod/{{ benchmarking_run_mlperf_ssd_name }} -n {{ benchmarking_run_mlperf_ssd_namespace }}
        > "{{ artifact_extra_logs_dir }}/pod_run-mlperf-ssd.log"
    failed_when: false

  - name: Store the status of the benchmark execution (for post-processing)
    shell:
      echo "{{ wait_benchmark_pod_cmd.stdout }}" > "{{ artifact_extra_logs_dir }}/pod_run-mlperf-ssd.status"

  - name: Store the description of benchmark execution (debug)
    shell:
      oc describe pod/{{ benchmarking_run_mlperf_ssd_name }} -n {{ benchmarking_run_mlperf_ssd_namespace }}
        > "{{ artifact_extra_logs_dir }}/pod_run-mlperf-ssd.descr"
    failed_when: false

- name: Get average sample rate
  shell:
    set -o pipefail;
    cat "{{ artifact_extra_logs_dir }}/pod_run-mlperf-ssd.log" | grep avg. | tail -n1 | awk '{ print $NF " samples/sec" }' > "{{ artifact_dir }}/benchmarking_run_ssd_sample_rate.log";
    cp {{ artifact_dir }}/benchmarking_run_ssd_sample_rate.log {{ artifact_extra_logs_dir }}/benchmarking_run_ssd_sample_rate.log

- name: Get time to run benchmark
  shell:
    set -o pipefail;
    cat "{{ artifact_extra_logs_dir }}/pod_run-mlperf-ssd.log" | grep real | awk '{ print $NF }' | awk -Fm '{ print $1 " mins" }' > "{{ artifact_dir }}/benchmarking_run_ssd_bench_duration.log";
    cp {{ artifact_dir }}/benchmarking_run_ssd_bench_duration.log {{ artifact_extra_logs_dir }}/benchmarking_run_ssd_bench_duration.log
