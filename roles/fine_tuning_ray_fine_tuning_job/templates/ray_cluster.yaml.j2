# this resource (.spec) is currently used AS PART of the the RayJob,
# NOT directly.
apiVersion: ray.io/v1
kind: RayCluster
metadata:
  name: {{ fine_tuning_ray_fine_tuning_job_name }}
  namespace: {{ fine_tuning_ray_fine_tuning_job_namespace }}
spec:
  headGroupSpec:
    enableIngress: false
    rayStartParams:
      block: 'true'
      dashboard-host: 0.0.0.0
      num-gpus: "{% if fine_tuning_ray_fine_tuning_job_gpu %}{{ fine_tuning_ray_fine_tuning_job_gpu }}{% else %}0{% endif %}"
      resources: '"{}"'
    serviceType: ClusterIP
    template: &head_template
      metadata:
        annotations:
{% if fine_tuning_ray_fine_tuning_job_use_secondary_nic %}
            k8s.v1.cni.cncf.io/networks:
              '[
{% for secondary_nic in secondary_nics %}
                {
                  "name": "{{ secondary_nic }}",
                  "namespace": "{{ fine_tuning_ray_fine_tuning_job_namespace }}"
                }{% if secondary_nic != secondary_nics[-1] %},{% endif %}
{% endfor %}
              ]'
{% endif %}
        labels:
          ray-anti-affinity: "true"
      spec:
{% if fine_tuning_ray_fine_tuning_job_node_selector_key | length %}
        nodeSelector:
          {{ fine_tuning_ray_fine_tuning_job_node_selector_key }}: "{{ fine_tuning_ray_fine_tuning_job_node_selector_value }}"
{% endif %}
        containers:
        - name: ray-head
          command: ["python3 /mnt/entrypoint/init_ray_container.py"] # the ray command will be executed after that
          ports:
          - containerPort: 6379
            name: gcs
            protocol: TCP
          - containerPort: 8265
            name: dashboard
            protocol: TCP
          - containerPort: 10001
            name: client
            protocol: TCP
          image: &head_image "{{ fine_tuning_ray_fine_tuning_job_container_image }}"
{% if fine_tuning_ray_fine_tuning_job_use_secondary_nic %}
          securityContext:
            runAsUser: 0
            capabilities:
              add:
              - IPC_LOCK
              - SYS_RESOURCE
              - NET_ADMIN
              - NET_RAW
{% endif %}
          env: &head_env
          - name: CONFIG_JSON_PATH
            value: /mnt/config/config.json
          - name: NODE_HOSTNAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
{% if fine_tuning_ray_fine_tuning_job_use_secondary_nic %}
          - name: SECONDARY_SOCKET_IFNAME
            value: "{{ secondary_socket_ifname }}"
{% endif %}
          resources: &head_resources
            requests: &head_request_block
{% if fine_tuning_ray_fine_tuning_job_gpu %}
              nvidia.com/gpu: "{{ fine_tuning_ray_fine_tuning_job_gpu }}"
{% endif %}
              memory: "{{ fine_tuning_ray_fine_tuning_job_memory }}Gi"
              cpu: "{{ fine_tuning_ray_fine_tuning_job_cpu }}"
{% if fine_tuning_ray_fine_tuning_job_request_equals_limits %}
            limits:  *head_request_block
{% else %}
            limits:
{% if fine_tuning_ray_fine_tuning_job_gpu %}
              nvidia.com/gpu: "{{ fine_tuning_ray_fine_tuning_job_gpu }}"
{% endif %}
              cpu: 20
{% endif %}

          volumeMounts: &head_volume_mounts
{% if fine_tuning_ray_fine_tuning_job_pvc_name %}
          - name: storage-volume
            mountPath: /mnt/storage
{% endif %}
          - name: app-volume
            mountPath: /mnt/app
          - name: entrypoint-volume
            mountPath: /mnt/entrypoint
          - name: config-volume
            mountPath: /mnt/config
          - name: output-volume
            mountPath: /mnt/output
{% if fine_tuning_ray_fine_tuning_job_use_secondary_nic %}
          - name: nic-mapping-volume
            mountPath: /mnt/nic-mapping
{% endif %}
        volumes: &head_volumes
{% if fine_tuning_ray_fine_tuning_job_pvc_name %}
        - name: storage-volume
          persistentVolumeClaim:
            claimName: {{ fine_tuning_ray_fine_tuning_job_pvc_name }}
{% endif %}

{% if fine_tuning_ray_fine_tuning_job_use_secondary_nic %}
        - name: nic-mapping-volume
          configMap:
            name: {{ job_name_safe }}-nic-mapping
{% endif %}
        - name: config-volume
          configMap:
            name: {{ job_name_safe }}-config
        - name: entrypoint-volume
          configMap:
            name: {{ job_name_safe }}-entrypoint
        - name: app-volume
          configMap:
            name: {{ job_name_safe }}-app
{% if fine_tuning_ray_fine_tuning_job_ephemeral_output_pvc_size %}
        - name: output-volume
          ephemeral:
            volumeClaimTemplate:
              metadata:
              spec:
                accessModes: [ "ReadWriteOnce" ]
                resources:
                  requests:
                    storage: "{{ fine_tuning_ray_fine_tuning_job_ephemeral_output_pvc_size }}"
{% else %}
        - name: output-volume
          emptyDir: {}
{% endif %}
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: ray-anti-affinity
                    operator: In
                    values:
                    - "true"
                topologyKey: "kubernetes.io/hostname"

  rayVersion: {{ fine_tuning_ray_fine_tuning_job_ray_version }}
  workerGroupSpecs:
  - groupName: {{ fine_tuning_ray_fine_tuning_job_name }}
    maxReplicas: {{ fine_tuning_ray_fine_tuning_job_pod_count -1 }}
    minReplicas: {{ fine_tuning_ray_fine_tuning_job_pod_count -1 }}
    rayStartParams:
      block: "true"
      num-gpus: "{% if fine_tuning_ray_fine_tuning_job_gpu %}{{ fine_tuning_ray_fine_tuning_job_gpu }}{% else %}0{% endif %}"
      resources: '"{}"'
    replicas: {{ fine_tuning_ray_fine_tuning_job_pod_count }}
    template: *head_template
