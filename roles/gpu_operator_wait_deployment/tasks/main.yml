---
- name: Ensure that NFD is deployed
  include_role:
    name: nfd_has_labels

- name: Ensure that there are GPU nodes
  include_role:
    name: nfd_test_wait_gpu

- name: Wait for the ClusterPolicy CRD to be deployed
  command: oc get crds/clusterpolicies.nvidia.com
  register: has_clusterpolicy_crd
  until:
  - has_clusterpolicy_crd.rc == 0
  retries: 10
  delay: 10

- name: Wait for the ClusterPolicy CR to be deployed
  command: oc get ClusterPolicies -oname
  register: has_clusterpolicy
  until:
  - has_clusterpolicy.rc == 0
  retries: 10
  delay: 10

- name: Wait for the GPU Operator namespace (only v1.7.0 .. v1.8.1)
  when: gpu_operator_version is version("1.7.0", ">=") and gpu_operator_version is version("1.8.1", "<=")
  # With the GPU Operator >=v1.7.0, the namespace is created by the
  # operator, so it takes a few seconds before being available after the
  # deployment.
  command: oc get ns/{{ gpu_operator_namespace }}
  register: has_gpu_operator_namespace
  until: has_gpu_operator_namespace.rc == 0
  retries: 10
  delay: 10


- name: Wait for the GPU Operator to validate the driver deployment
  when: gpu_operator_version is version("1.8.0", ">=")
  command:
    oc rsh
       -n {{ gpu_operator_namespace }}
       ds/nvidia-node-status-exporter
       test -f /run/nvidia/validations/driver-ready
  register: has_gpu_operator_driver
  until: has_gpu_operator_driver.rc == 0
  retries: 15
  delay: 60

- name: Wait for the GPU Operator to run its internal validation steps
  when: gpu_operator_version is version("1.7.0", ">=")
  block:
  - name: Ensure that nvidia-operator-validator DS is ready
    command:
      oc get ds/nvidia-operator-validator
         -n {{ gpu_operator_namespace }}
         -oyaml
         -ojsonpath={.status.numberUnavailable}
    register: operator_validator_number_unavailable
    until:
    - operator_validator_number_unavailable.rc == 0
    - not operator_validator_number_unavailable.stdout
    retries: 15
    delay: 60

- name: Wait for the GPU Operator (< v1.7.0) to run its internal validation steps
  when: gpu_operator_version is version("1.7.0", "<")
  block:
  - name: Ensure that nvidia-device-plugin-validation Pod has ran successfully
    command:
      oc get pods
        --field-selector=metadata.name=nvidia-device-plugin-validation,status.phase=Succeeded
        -n {{ gpu_operator_namespace }}
        -oname --no-headers
    register: has_deviceplugin_validation_pod
    until:
    - has_deviceplugin_validation_pod.stdout == "pod/nvidia-device-plugin-validation"
    retries: 15
    delay: 60

- block:
  - name: Wait for the gpu-feature-discovery Pod to label the nodes
    command: oc get nodes -l nvidia.com/gpu.count -oname
    register: has_gpu_feature_discovery_labels
    until:
    - has_gpu_feature_discovery_labels.stdout != ""
    retries: 20
    delay: 30

  rescue:
  - name: Capture the GFD logs (debug)
    shell:
      oc logs ds/gpu-feature-discovery
         -n {{ gpu_operator_namespace }} > {{ artifact_extra_logs_dir }}/gpu_operator_gfd.log
    failed_when: false

  - name: The GFD did not label the nodes
    fail: msg="The GFD did not label the nodes"

- name: Validate the GPU Operator metrics
  include_tasks: metrics.yml
