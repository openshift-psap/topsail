---
- name: Get the content of the variable_overrides file, if it is enabled
  when: local_ci_run_multi_variable_overrides | default('', true) | trim
  command:
    cat "{{ local_ci_run_multi_variable_overrides }}"
  register: local_ci_run_multi_variable_overrides_content_cmd

- name: Ensure that the CI image exists
  command: oc get istag -n "{{ local_ci_run_multi_namespace }}" "{{ local_ci_run_multi_istag }}"

- name: Create the src directory
  file:
    path: "{{ artifact_extra_logs_dir }}/src"
    state: directory
    mode: '0755'

- name: Create the config configmap
  shell:
    set -o nounset;
    set -o pipefail;
    oc create configmap
       {{ local_ci_run_multi_job_name }}-config
       -n {{ local_ci_run_multi_namespace }}
       "--from-file=config.yaml=$TOPSAIL_FROM_CONFIG_FILE"
       --dry-run=client
       -oyaml
       | tee "{{ artifact_extra_logs_dir }}/src/001_config.yaml"
       | oc apply -f-
  when: local_ci_run_multi_use_local_config

- name: Empty the Minio S3 bucket
  shell: |
    oc -c mc -n {{ local_ci_run_multi_minio_namespace }} rsh $(oc get pod -lapp=minio -n {{ local_ci_run_multi_minio_namespace }} -oname) \
       mc --insecure --config-dir /tmp rm minio/{{ local_ci_run_multi_minio_bucket_name }}/ --recursive --force --quiet >/dev/null;
    oc -c mc -n {{ local_ci_run_multi_minio_namespace }} rsh $(oc get pod -lapp=minio -n {{ local_ci_run_multi_minio_namespace }} -oname) \
       rm -rf /artifacts/to_export > /dev/null
    oc -c mc -n {{ local_ci_run_multi_minio_namespace }} rsh $(oc get pod -lapp=minio -n {{ local_ci_run_multi_minio_namespace }} -oname) \
       mc --insecure --config-dir /tmp cp /etc/os-release minio/{{ local_ci_run_multi_minio_bucket_name }}; # without it, cp may fail if the bucket is empty
  failed_when: false
  when: local_ci_run_multi_retrieve_artifacts

- name: Apply the Job template
  template:
    src: "{{ local_ci_run_multi_job_template }}"
    dest: "{{ artifact_extra_logs_dir }}/src/000_job.yaml"
    mode: '0400'

- name: Delete any stalled CI execution Pod
  command: oc delete -f "{{ artifact_extra_logs_dir }}/src/000_job.yaml" --ignore-not-found=true

- name: Cleanup the Prometheus database of the cluster
  include_role:
    name: cluster_prometheus_db
  vars:
    cluster_prometheus_db_mode: reset
  when: local_ci_run_multi_capture_prom_db | bool

- name: Create the CI Job
  command: oc apply -f "{{ artifact_extra_logs_dir }}/src/000_job.yaml"

- name: Inform about the next task
  debug:
    msg: |
      The next task waits for the completion of the local-ci command '{{ local_ci_run_multi_ci_command }}'
      Watch the progress with this command: oc get pods -ljob-name={{ local_ci_run_multi_job_name }} -n {{ local_ci_run_multi_namespace }}

- name: Wait for the CI Job to start
  shell:
    oc get -f "{{ artifact_extra_logs_dir }}/src/000_job.yaml"
       -ojsonpath={.status.startTime}
  register: wait_ci_job_start
  retries: 12
  delay: 5
  until: wait_ci_job_start.stdout

- name: Wait for the daemon pods
  when: local_ci_run_multi_launch_as_daemon
  block:
  - name: Wait for all the pods to become active
    shell:
      set -o pipefail;
      oc get -f "{{ artifact_extra_logs_dir }}/src/000_job.yaml"
      -ojson
      | jq '.status.failed + .status.ready + .status.succeeded'
    register: job_pods_launched
    retries: 30
    delay: 10
    until: job_pods_launched.stdout | int == local_ci_run_multi_user_count

  - name: Capture the state of the Pods
    shell:
      oc get -f "{{ artifact_extra_logs_dir }}/src/000_job.yaml"
      -oyaml > "{{ artifact_extra_logs_dir }}/ci_job.yaml";

      oc get pods -owide
        -ljob-name={{ local_ci_run_multi_job_name }}
        -n {{ local_ci_run_multi_namespace }}
        > "{{ artifact_extra_logs_dir }}/ci_pods.status"
    ignore_errors: true

  - name: Check that no pod failed
    shell:
      set -o pipefail;
      oc get -f "{{ artifact_extra_logs_dir }}/src/000_job.yaml" -ojson
         | jq '.status.failed'
    register: job_pods_failed
    failed_when: job_pods_failed.rc != 0 or job_pods_failed.stdout_lines[0] != "null"

  - name: Terminate the play is launching as a daemon
    meta:
      end_play

- name: Wait for the CI Job to terminate
  command:
    oc get -f "{{ artifact_extra_logs_dir }}/src/000_job.yaml"
       -ojsonpath={.status.active}
  register: wait_ci_job
  retries: 1440
  delay: 30
  until: not wait_ci_job.stdout
  failed_when: false

- name: Get the OpenShift Prometheus database
  include_role:
    name: cluster_prometheus_db
  vars:
    cluster_prometheus_db_mode: dump
    cluster_prometheus_db_dump_name_prefix: prometheus_ocp
  when: local_ci_run_multi_capture_prom_db | bool
  ignore_errors: true

- name: Get the status of the CI Job
  shell:
    oc get -f "{{ artifact_extra_logs_dir }}/src/000_job.yaml"
        > "{{ artifact_extra_logs_dir }}/ci_job.status"
  ignore_errors: true

- name: Get the yaml of the CI Job
  shell:
    oc get -f "{{ artifact_extra_logs_dir }}/src/000_job.yaml"
       -oyaml
       > "{{ artifact_extra_logs_dir }}/ci_job.yaml"
  ignore_errors: true

- name: Get the description of the CI Job
  shell:
    oc describe -f "{{ artifact_extra_logs_dir }}/src/000_job.yaml"
        > "{{ artifact_extra_logs_dir }}/ci_job.desc"
  ignore_errors: true

- name: Get the status of the CI Pods
  shell:
    oc get pods -owide
       -ljob-name={{ local_ci_run_multi_job_name }}
       -n {{ local_ci_run_multi_namespace }}
        > "{{ artifact_extra_logs_dir }}/ci_pods.status"
  ignore_errors: true

- name: Get the yaml of the CI Pods
  shell:
    oc get pods -oyaml
       -ljob-name={{ local_ci_run_multi_job_name }}
       -n {{ local_ci_run_multi_namespace }}
       > "{{ artifact_extra_logs_dir }}/ci_pods.yaml"
  ignore_errors: true

- name: Get the JSON of the CI Pods
  shell:
    oc get pods -ojson
       -ljob-name={{ local_ci_run_multi_job_name }}
       -n {{ local_ci_run_multi_namespace }}
       > "{{ artifact_extra_logs_dir }}/ci_pods.json"
  ignore_errors: true

- name: Get the description of the CI Pods
  shell:
    oc describe pods
       -ljob-name={{ local_ci_run_multi_job_name }}
       -n {{ local_ci_run_multi_namespace }}
       > "{{ artifact_extra_logs_dir }}/ci_pods.desc"
  ignore_errors: true

- name: Create the CI Pod logs directory
  file:
    path: "{{ artifact_extra_logs_dir }}/logs"
    state: directory
    mode: '0755'

- name: Get the names of the CI Pods
  command:
    oc get pods -oname
       -ljob-name={{ local_ci_run_multi_job_name }}
       -n {{ local_ci_run_multi_namespace }}
  register: pod_names_cmd
  ignore_errors: true
  # mute to reduce the log length
  no_log: true

- name: Get the logs of the tester Pod
  shell:
    oc logs {{ item }}
         --all-containers --prefix
         -n {{ local_ci_run_multi_namespace }}
         > "{{ artifact_extra_logs_dir }}/logs/$(basename "{{ item }}").log"
  loop: "{{ pod_names_cmd.stdout_lines }}"
  ignore_errors: true
  # mute to reduce the log length
  no_log: true

- name: Get Minio Pod name
  command:
    oc get pod
      -lapp=minio
      -n {{ local_ci_run_multi_minio_namespace }}
      -ojsonpath={.items[0].metadata.name}
  register: minio_podname_cmd
  ignore_errors: true

- name: Create the artifacts directory
  file:
    path: "{{ artifact_extra_logs_dir }}/artifacts"
    state: directory
    mode: '0755'

- name: Export the data out of the S3 bucket
  command:
    oc -n {{ local_ci_run_multi_minio_namespace }}
       -c mc
       exec "{{ minio_podname_cmd.stdout }}"
       -- bash -c 'mkdir -p /artifacts/to_export && mc --insecure --config-dir /tmp cp minio/{{ local_ci_run_multi_minio_bucket_name }}/ /artifacts/to_export --recursive --quiet > /dev/null'
  ignore_errors: true

- name: Extract the test artifacts from the Minio S3 container
  shell: |
    set -e

    oc -n {{ local_ci_run_multi_minio_namespace }} -c ubi8 \
       rsync "{{ minio_podname_cmd.stdout }}:/artifacts/to_export/" "{{ artifact_extra_logs_dir }}/artifacts" --quiet

    rm -f "{{ artifact_extra_logs_dir }}/artifacts/os-release"
  ignore_errors: true

- name: Count how many Pods succeeded
  shell:
    set -o pipefail;
    (cat "{{ artifact_extra_logs_dir }}"/artifacts/ci-pod-*/test.exit_code || true)
      | (grep '^0$' || true)
      | wc -l
  register: success_count_cmd

- name: Store the success count
  shell:
    set -o pipefail;
    echo "{{ success_count_cmd.stdout_lines[0] }}/{{ local_ci_run_multi_user_count }}"
         | tee "{{ artifact_extra_logs_dir }}/success_count"

# the tasks below will abort the execution in case of problems

- name: Test if the CI job crashed
  command:
    oc get -f "{{ artifact_extra_logs_dir }}/src/000_job.yaml"
       -ojsonpath={.status.failed}
  register: check_ci_job
  failed_when: check_ci_job.stdout | length > 0

- name: Fail if none of the Pods succeeded
  when: success_count_cmd.stdout == "0"
  fail: msg="None of the Pods succeeded"

- name: Fail if not all the Pods succeeded
  when: local_ci_run_multi_need_all_success | bool and success_count_cmd.stdout != local_ci_run_multi_user_count | string
  fail: msg="{{ success_count_cmd.stdout }}/{{ local_ci_run_multi_user_count }} Pods succeeded"
