- name: Create the src artifacts directory
  file:
    path: "{{ artifact_extra_logs_dir }}/src/"
    state: directory
    mode: '0755'

- name: Create the ConfigMap definition
  template:
    src: "{{ config_map_template }}"
    dest: "{{ artifact_extra_logs_dir }}/src/configmap.yaml"
    mode: '0400'

- name: Instanciate the ConfigMap
  command:
    oc apply -f "{{ artifact_extra_logs_dir }}/src/configmap.yaml"
  when: gpu_operator_enable_time_sharing_replicas != 1

- name: Delete the ConfigMap when only 1 replica is requested
  command:
    oc delete -f "{{ artifact_extra_logs_dir }}/src/configmap.yaml" --ignore-not-found
  when: gpu_operator_enable_time_sharing_replicas == 1

- name: Cleanup the ClusterPolicy (to force the update in the next task)
  command: |
    oc patch clusterpolicy/gpu-cluster-policy \
       --type merge \
       -p '{"spec": {"devicePlugin": {"config": {"name": "", "default": "any"}}}}'

- name: Update the ClusterPolicy
  command: |
    oc patch clusterpolicy/gpu-cluster-policy \
       --type merge \
       -p '{"spec": {"devicePlugin": {"config": {"name": "{{ gpu_operator_enable_time_sharing_configmap_name }}", "default": "any"}}}}'
  when: gpu_operator_enable_time_sharing_replicas != 1

- name: Force bounce the Device Plugin and GPU Feature Discovery daemonsets
  command:
    oc delete daemonsets nvidia-device-plugin-daemonset gpu-feature-discovery
       -n {{ gpu_operator_enable_time_sharing_namespace }}
       --ignore-not-found

- name: Wait for the DaemonSets to be all available
  shell:
    set -o pipefail;
    oc get daemonsets
       -o=jsonpath="{range .items[*]}{.metadata.name}{' ='}{.status.numberUnavailable}{'=\n'}{end}"
       -n {{ gpu_operator_enable_time_sharing_namespace }}
       | grep -v "==" || true
  register: daemonsets_not_ready
  until: not daemonsets_not_ready.stdout
  retries: 60
  delay: 10
  failed_when: daemonsets_not_ready.stdout | length > 0

- name: Wait for the number of GPU per node to be updated (on the first node)
  shell:
    set -o pipefail;
    oc get nodes -lnvidia.com/gpu.replicas -ojson
      | jq -r '.items[0].metadata.labels["nvidia.com/gpu.replicas"]'
      | head -1
  register: replicas_count
  until: replicas_count.stdout_lines[0] == gpu_operator_enable_time_sharing_replicas | string
  retries: 30
  delay: 10
  failed_when: replicas_count.stdout_lines[0] != gpu_operator_enable_time_sharing_replicas | string

- name: Get the number of GPUs per node
  shell:
    set -o pipefail;
    oc get node -lnvidia.com/gpu.present -ojson
       | jq '.items[] | .metadata.name + " ==> "+ .status.allocatable["nvidia.com/gpu"] + " GPUs"' -r
