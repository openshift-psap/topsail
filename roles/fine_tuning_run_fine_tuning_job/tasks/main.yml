---
- name: Store the flag for rdma use
  set_fact:
    rdma_nics: "{{ fine_tuning_run_fine_tuning_job_use_secondary_nic and fine_tuning_run_fine_tuning_job_use_secondary_nic | select('search', 'rdma') | list}}"

- name: Create the src directory
  file:
    path: "{{ artifact_extra_logs_dir }}/src/"
    state: directory
    mode: '0755'

- name: Create the artifacts directory
  file:
    path: "{{ artifact_extra_logs_dir }}/artifacts/"
    state: directory
    mode: '0755'
  when: not fine_tuning_run_fine_tuning_job_prepare_only

- name: Make the name k8s safe
  set_fact:
    job_name_safe: "{{ fine_tuning_run_fine_tuning_job_name | replace('.', '-') | replace('_', '-') }}"

- name: Delete the fine-tuning job configmaps, if any
  command:
    oc delete configmap
       -ltopsail.fine-tuning-jobname={{ job_name_safe }}
       --ignore-not-found
       -n {{ fine_tuning_run_fine_tuning_job_namespace }}
  when: fine_tuning_run_fine_tuning_job_delete_other | bool

- name: Prepare the config file template
  template:
    src: "{% if fine_tuning_run_fine_tuning_job_workload == 'fms' -%}
    {{ fine_tuning_job_fms_config_template }}{%-
elif fine_tuning_run_fine_tuning_job_workload == 'ilab' -%}
    {{ fine_tuning_job_ilab_config_template }}
{%- endif %}"
    dest: "{{ artifact_extra_logs_dir }}/src/config_base.yaml"
    mode: '0400'

- name: Save the hyper-parameters overrides into a file
  shell: |
    set -o pipefail;

    cat << EOF | yq -y > "{{ artifact_extra_logs_dir }}/src/config_override.yaml"
    {{ fine_tuning_run_fine_tuning_job_hyper_parameters | to_yaml }}
    EOF

- name: Convert the config to json
  shell:
    set -o pipefail;

    cat "{{ artifact_extra_logs_dir }}/src/config_base.yaml"
    {% if fine_tuning_run_fine_tuning_job_hyper_parameters %}
        "{{ artifact_extra_logs_dir }}/src/config_override.yaml"
    {% endif %}
        | yq
      > "{{ artifact_extra_logs_dir }}/src/config_final.json"

- name: Prepare the config ConfigMap
  shell: |
    set -o pipefail;

    oc create cm {{ job_name_safe }}-config \
       -n {{ fine_tuning_run_fine_tuning_job_namespace }} \
       --from-file=config.json=<(cat "{{ artifact_extra_logs_dir }}/src/config_final.json") \
       --dry-run=client \
       -oyaml \
       | yq -Y '. | .metadata.labels = {"topsail.fine-tuning-jobname": "{{ job_name_safe }}"}' \
       | tee "{{ artifact_extra_logs_dir }}/src/configmap_config.yaml" \
       | oc apply -f-

- name: Prepare the entrypoint ConfigMap
  shell: |
    set -o pipefail;

    oc create cm {{ job_name_safe }}-entrypoint \
       -n {{ fine_tuning_run_fine_tuning_job_namespace }} \
       --from-file=$(find "{{ fine_tuning_job_entrypoint_dir }}" -maxdepth 1 -not -type d | tr '\n' ,)/dev/null \
       --dry-run=client \
       -oyaml \
       | yq -Y '. | .metadata.labels = {"topsail.fine-tuning-jobname": "{{ job_name_safe }}"}' \
       | tee "{{ artifact_extra_logs_dir }}/src/configmap_entrypoint.yaml" \
       | oc apply -f-

- name: Delete the fine-tuning job, if it exists
  command:
    oc delete pytorchjobs.kubeflow.org
       --all
       -n {{ fine_tuning_run_fine_tuning_job_namespace }}
       --ignore-not-found
  when: fine_tuning_run_fine_tuning_job_delete_other | bool

- name: Capture the IP of the secondary NIC of the GPU nodes
  when:
  - fine_tuning_run_fine_tuning_job_use_secondary_nic | default('', true) | length > 0
  - not fine_tuning_run_fine_tuning_job_prepare_only | bool
  block:
  - name: List the GPU nodes
    command:
      oc get nodes -oname -lnvidia.com/gpu.present=true
    register: gpu_node_names_cmd

  - name: Prepare the secondary nic list
    set_fact:
      secondary_nics: "{% if fine_tuning_run_fine_tuning_job_use_secondary_nic is string %}{{ [fine_tuning_run_fine_tuning_job_use_secondary_nic] }}{% else %}{{ fine_tuning_run_fine_tuning_job_use_secondary_nic or [] }}{% endif %}"

  - name: Generate the iface names (net1,net2,...)
    command:
      python -c 'print(",".join(f"net{1+i}" for i in range(len("{{ secondary_nics }}".split(",")))))'
    register: nccl_socket_ifname_cmd

  - name: Prepare the nccl iface names
    set_fact:
      nccl_socket_ifname: "{{ nccl_socket_ifname_cmd.stdout }}"

  - name: Fetch the NetworkAttachmentDefinition
    shell:
      oc get Network-Attachment-Definition/{{ secondary_nic }}
         -oyaml
         -n {{ fine_tuning_run_fine_tuning_job_namespace }}
         > "{{ artifact_extra_logs_dir }}/artifacts/Network-Attachment-Definition_{{ secondary_nic }}.yaml"
    loop: "{{ secondary_nics }}"
    loop_control:
      loop_var: secondary_nic

  - name: Ensure that the NetworkAttachmentDefinition has the expected host-device type
    shell:
      set -o pipefail;

      cat "{{ artifact_extra_logs_dir }}/artifacts/Network-Attachment-Definition_{{ secondary_nic }}.yaml"
          | yq .spec.config -r
          | jq .type -r
    register: secondary_nic_type_cmd
    failed_when: secondary_nic_type_cmd.stdout | trim != "host-device"
    loop: "{{ secondary_nics }}"
    loop_control:
      loop_var: secondary_nic

  - name: Extract the name of the secondary NIC
    shell: |
      set -o pipefail;

      for nad in {{ artifact_extra_logs_dir }}/artifacts/Network-Attachment-Definition_*.yaml; do
        cat "$nad" | yq .spec.config -r
      done | jq .device -r

    register: secondary_nic_names_cmd
    failed_when: not secondary_nic_names_cmd.stdout | trim | length

  - name: Save the NIC name
    set_fact:
      secondary_nic_names: "{{ ' '.join(secondary_nic_names_cmd.stdout_lines) }}"

  - name: Ensure that there is no PyTorchJob Pod running in the namespace
    shell:
      set -o pipefail;

      oc get pod --no-headers
         -ltraining.kubeflow.org/job-name
         -n {{ fine_tuning_run_fine_tuning_job_namespace }}
         | awk '{print $3}' | uniq
    register: namespace_pods_state_cmd
    until:
    - '"Running" not in namespace_pods_state_cmd.stdout_lines'
    - '"Terminating" not in namespace_pods_state_cmd.stdout_lines'
    - '"Init:0/1" not in namespace_pods_state_cmd.stdout_lines'
    retries: 18
    delay: 10

  - name: Save the host/IP mapping in a file
    shell: |
      set -o pipefail;
      route=$(oc debug {{ node_name }} -- bash -c "ip route" 2>/dev/null)
      echo "Route of {{ node_name }}"
      echo "$route"
      nic_idx=1
      for nic_name in {{ secondary_nic_names }}; do
        ip=$(echo "$route" | grep $nic_name | cut -d' ' -f9)
        if [[ -z "$ip" ]]; then
          echo "Couldn't find the IP of NIC/$nic_name on {{ node_name }} :/ "
          exit 1
        fi
        echo "$(echo "{{ node_name }}" | cut -d/ -f2):$nic_name:net$nic_idx:$ip" | tee -a "{{ artifact_extra_logs_dir }}/src/nodename_ip_mapping.yaml"
        nic_idx=$((nic_idx+1))
      done
    loop: "{{ gpu_node_names_cmd.stdout_lines }}"
    loop_control:
      loop_var: node_name

  - name: Create a ConfigMap with the secondary NIC mapping
    shell:
      set -o pipefail;

      oc create cm {{ job_name_safe }}-nic-mapping \
       --from-file="{{ artifact_extra_logs_dir }}/src/nodename_ip_mapping.yaml" \
       --dry-run=client \
       -oyaml \
       | tee "{{ artifact_extra_logs_dir }}/src/configmap_mapping.yaml" \
       | oc apply -f-

- name: Capture the Python package versions of the image
  shell: |
    MINI_POD_SPEC='{"apiVersion": "v1", "kind":"Pod","metadata":{"name":"test"},"spec":{"containers":[{"name":"cnt"}]}}'
    IMAGE="{{ fine_tuning_run_fine_tuning_job_container_image }}"

    oc debug -f <(echo "$MINI_POD_SPEC") \
       --image=${IMAGE} \
       -n {{ fine_tuning_run_fine_tuning_job_namespace }} \
       -- pip freeze > "{{ artifact_extra_logs_dir }}/artifacts/pip-freeze.txt"
  when:
  - fine_tuning_run_fine_tuning_job_capture_artifacts | bool
  - not fine_tuning_run_fine_tuning_job_prepare_only | bool

- name: Prepare the template file
  template:
    src: "{{ fine_tuning_job_template }}"
    dest: "{{ artifact_extra_logs_dir }}/src/pytorchjob_fine_tuning.yaml"
    mode: '0400'

- name: Delete the fine-tuning job, if it exists
  command:
    oc delete -f "{{ artifact_extra_logs_dir }}/src/pytorchjob_fine_tuning.yaml" --ignore-not-found

- name: Exit the play in 'prepare_only' mode
  meta: end_play
  when: fine_tuning_run_fine_tuning_job_prepare_only | bool

- name: Create the fine-tuning job
  command:
    oc create -f "{{ artifact_extra_logs_dir }}/src/pytorchjob_fine_tuning.yaml"

- name: Wait for the job completion
  block:
  - name: Wait for the Pod to start running
    shell:
      set -o pipefail;
      oc get pods -l 'training.kubeflow.org/job-name in ({{ job_name_safe }}), training.kubeflow.org/job-role in (master)'
         -n {{ fine_tuning_run_fine_tuning_job_namespace }}
         --no-headers | awk '{print $3}'
    register: wait_pod_start
    retries: 20
    delay: 5
    until: wait_pod_start.stdout in ["Running", "Error", "Init:Error", "Completed", "NotReady", "CrashLoopBackOff", "ContainerCreating", "ImagePullBackOff"]

  - name: Fail if the Pod did not start successfully
    fail: msg="Pod in error state"
    when: wait_pod_start.stdout in ["Error", "Init:Error", "CrashLoopBackOff", "ImagePullBackOff"]

  - name: Wait for the Pod to fetch the image
    when: wait_pod_start.stdout in ["ContainerCreating"]
    block:
      - name: Wait for the Pod to fetch the image
        shell:
          set -o pipefail;
          oc get pods -l 'training.kubeflow.org/job-name in ({{ job_name_safe }}), training.kubeflow.org/job-role in (master)'
             -n {{ fine_tuning_run_fine_tuning_job_namespace }}
             --no-headers | awk '{print $3}'
        register: wait_pod_fetch
        retries: 720
        delay: 10
        until: wait_pod_fetch.stdout in ["Running", "Error", "Init:Error", "Completed", "NotReady", "CrashLoopBackOff", "ImagePullBackOff"]

      - name: Fail if the Pod did not start successfully
        fail: msg="Pod in error state"
        when: wait_pod_fetch.stdout in ["Error", "Init:Error", "CrashLoopBackOff", "ImagePullBackOff"]

  - name: Finish here if sleeping forever
    when: fine_tuning_run_fine_tuning_job_sleep_forever | bool
    meta: end_play

  - name: Wait for the main container to finish running
    shell:
      set -o pipefail;
      oc get pods -l 'training.kubeflow.org/job-name in ({{ job_name_safe }}), training.kubeflow.org/job-role in (master)'
         -n {{ fine_tuning_run_fine_tuning_job_namespace }}
         --no-headers | awk '{print $3}'
    register: wait_pod_start
    retries: 9999
    delay: 30
    until: wait_pod_start.stdout != "Running"

  - name: Fail if the Pod did not complete properly
    fail: msg="Pod in '{{ wait_pod_start.stdout }}' state"
    when: wait_pod_start.stdout not in ["Completed"]

  always:
  - name: Capture the state of the fine-tuning Pod resource
    shell:
      set -o pipefail;

      oc get pod
         -ltraining.kubeflow.org/job-name={{ job_name_safe }}
         -ojson
         -n {{ fine_tuning_run_fine_tuning_job_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/pod.json;

      oc get pod
         -ltraining.kubeflow.org/job-name={{ job_name_safe }}
         -oyaml
         -n {{ fine_tuning_run_fine_tuning_job_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/pod.yaml;

      oc get pod
         -ltraining.kubeflow.org/job-name={{ job_name_safe }}
         -owide
         -n {{ fine_tuning_run_fine_tuning_job_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/pod.status;

      oc describe pod
         -ltraining.kubeflow.org/job-name={{ job_name_safe }}
         -n {{ fine_tuning_run_fine_tuning_job_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/pod.desc

      oc logs $(
        oc get pod
           -ltraining.kubeflow.org/job-name={{ job_name_safe }}
           -n {{ fine_tuning_run_fine_tuning_job_namespace }}
           -oname | head -1)
        -n {{ fine_tuning_run_fine_tuning_job_namespace }}
        > {{ artifact_extra_logs_dir }}/artifacts/pod.log
    ignore_errors: true
    when: fine_tuning_run_fine_tuning_job_capture_artifacts | bool

  - name: Capture the state of the fine-tuning Pod resource
    shell:
      set -o pipefail;

      oc get pytorchjob/{{ job_name_safe }}
         -oyaml
         -n {{ fine_tuning_run_fine_tuning_job_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/pytorchjob.yaml;

      oc get pytorchjob/{{ job_name_safe }}
         -owide
         -n {{ fine_tuning_run_fine_tuning_job_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/pytorchjob.status;

      oc describe pytorchjob/{{ job_name_safe }}
         -n {{ fine_tuning_run_fine_tuning_job_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/pytorchjob.desc
    ignore_errors: true
    when: fine_tuning_run_fine_tuning_job_capture_artifacts | bool

  - name: Get the name of the job pods
    shell:
      set -o pipefail;

      oc get pods -ltraining.kubeflow.org/job-name={{ job_name_safe }}
         --no-headers
         -oname
         -n {{ fine_tuning_run_fine_tuning_job_namespace }}
    failed_when: false
    register: job_pod_name_cmd

  - name: Save the job pod logs
    shell:
      set -o pipefail;

      oc logs {{ item }} --all-containers
         -n {{ fine_tuning_run_fine_tuning_job_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/pod_{{ item | replace('pod/', '') }}.log
    failed_when: false
    loop: "{{ job_pod_name_cmd.stdout_lines }}"

  - name: Retrieve Pod files to artifacts directory
    when: fine_tuning_run_fine_tuning_job_retrieve_files | bool
    block:
    - name: Create the Pod artifacts directory
      file:
        path: "{{ artifact_extra_logs_dir }}/pod-artifacts/"
        state: directory
        mode: '0755'

    - name: Prepare the retrieval Pod template
      template:
        src: "{{ fine_tuning_job_retrieval_pod_template }}"
        dest: "{{ artifact_extra_logs_dir }}/src/retrieval_pod.yaml"
        mode: '0400'

    - name: Launch fine-tuning-file-retrieval Pod from template
      command:
        oc create -f "{{ artifact_extra_logs_dir }}/src/retrieval_pod.yaml"

    - name: Wait for fine-tuning-file-retrieval Pod to be Running
      shell:
        set -o pipefail;

        oc get --no-headers
           -f "{{ artifact_extra_logs_dir }}/src/retrieval_pod.yaml"
           | awk '{print $3}'
      register: retrieve_pod_state_cmd
      until: '"Running" in retrieve_pod_state_cmd.stdout_lines'
      retries: 18
      delay: 10

    - name: Copy files to the Pod artifacts directory
      command:
        oc cp fine-tuning-file-retrieval:{{ fine_tuning_job_retrieval_output_dir }}
           "{{ artifact_extra_logs_dir }}/pod-artifacts/"
           -n {{ fine_tuning_run_fine_tuning_job_namespace }}

    always:
    - name: Delete fine-tuning-file-retrieval Pod
      shell:
        oc get -f "{{ artifact_extra_logs_dir }}/src/retrieval_pod.yaml"
           > "{{ artifact_extra_logs_dir }}/artifacts/retrieval_pod.status";

        oc get -f "{{ artifact_extra_logs_dir }}/src/retrieval_pod.yaml"
           -oyaml
           > "{{ artifact_extra_logs_dir }}/artifacts/retrieval_pod.yaml";

        oc describe -f "{{ artifact_extra_logs_dir }}/src/retrieval_pod.yaml"
           > "{{ artifact_extra_logs_dir }}/artifacts/retrieval_pod.descr";

        oc delete -f "{{ artifact_extra_logs_dir }}/src/retrieval_pod.yaml";
