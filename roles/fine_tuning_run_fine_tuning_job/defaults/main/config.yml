# Auto-generated file, do not edit manually ...
# Toolbox generate command: repo generate_ansible_default_settings
# Source component: Fine_Tuning.run_fine_tuning_job

# Parameters
# the name of the fine-tuning job to create
# Mandatory value
fine_tuning_run_fine_tuning_job_name:

# the name of the namespace where the scheduler load will be generated
# Mandatory value
fine_tuning_run_fine_tuning_job_namespace:

# the name of the PVC where the model and dataset are stored
# Mandatory value
fine_tuning_run_fine_tuning_job_pvc_name:

# the name of the workload to run inside the container (fms or ilab)
# Mandatory value
fine_tuning_run_fine_tuning_job_workload:

# the name of the model to use inside the /dataset directory of the PVC
# Mandatory value
fine_tuning_run_fine_tuning_job_model_name:

# the name of the dataset to use inside the /model directory of the PVC
# Mandatory value
fine_tuning_run_fine_tuning_job_dataset_name:

# number of replications of the dataset to use, to artificially extend or reduce the fine-tuning effort
fine_tuning_run_fine_tuning_job_dataset_replication: 1

# name of the transformation to apply to the dataset
fine_tuning_run_fine_tuning_job_dataset_transform: null

# if True, and the dataset has to be transformed/duplicated, save and/or load it from the PVC
fine_tuning_run_fine_tuning_job_dataset_prefer_cache: true

# if True, only prepare the dataset cache file and do not run the fine-tuning.
fine_tuning_run_fine_tuning_job_dataset_prepare_cache_only: false

# the delimiter marking the beginning of the response in the dataset samples
fine_tuning_run_fine_tuning_job_dataset_response_template: null

# the image to use for the fine-tuning container
fine_tuning_run_fine_tuning_job_container_image: quay.io/modh/fms-hf-tuning:release-7a8ff0f4114ba43398d34fd976f6b17bb1f665f3

# the number of GPUs to request for the fine-tuning job
fine_tuning_run_fine_tuning_job_gpu: 0

# the number of RAM gigs to request for to the fine-tuning job (in Gigs)
fine_tuning_run_fine_tuning_job_memory: 10

# the number of CPU cores to request for the fine-tuning job (in cores)
fine_tuning_run_fine_tuning_job_cpu: 1

# if True, sets the 'limits' of the job with the same value as the request.
fine_tuning_run_fine_tuning_job_request_equals_limits: false

# amount of shm (in GB) to give to each of the job pods
fine_tuning_run_fine_tuning_job_shared_memory: null

# if True, only prepare the environment but do not run the fine-tuning job.
fine_tuning_run_fine_tuning_job_prepare_only: false

# if True, delete the other PyTorchJobs before running
fine_tuning_run_fine_tuning_job_delete_other: false

# number of Pods to include in the job
fine_tuning_run_fine_tuning_job_pod_count: 1

# dictionnary of hyper-parameters to pass to sft-trainer
fine_tuning_run_fine_tuning_job_hyper_parameters: {}

# if enabled, captures the artifacts that will help post-mortem analyses
fine_tuning_run_fine_tuning_job_capture_artifacts: true

# if true, sleeps forever instead of running the fine-tuning command.
fine_tuning_run_fine_tuning_job_sleep_forever: false

# if a size (with units) is passed, use an ephemeral volume claim for storing the fine-tuning output. Otherwise, use an emptyDir.
fine_tuning_run_fine_tuning_job_ephemeral_output_pvc_size: null

# if enabled, tell NCCL to use the primary NIC. Only taken into account if --use_secondary_nic is passed.
fine_tuning_run_fine_tuning_job_use_primary_nic: true

# if enabled, activates the secondary NIC. Can be a list with the name of multiple NetworkDefinitionAttachements, in the same namespace.
fine_tuning_run_fine_tuning_job_use_secondary_nic: false

# if enabled, activates the host network
fine_tuning_run_fine_tuning_job_use_host_network: false

# if enabled, allows files retrieval from the pod to the artifacts directory.
fine_tuning_run_fine_tuning_job_retrieve_files: true
